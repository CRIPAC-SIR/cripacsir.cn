<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dataset | Smart Individual Recognition</title>
    <link>http://localhost:1313/tag/dataset/</link>
      <atom:link href="http://localhost:1313/tag/dataset/index.xml" rel="self" type="application/rss+xml" />
    <description>Dataset</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 13 Feb 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_3.png</url>
      <title>Dataset</title>
      <link>http://localhost:1313/tag/dataset/</link>
    </image>
    
    <item>
      <title>CASIA-Iris-Africa</title>
      <link>http://localhost:1313/dataset/casia-iris-africa/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Various forms of iris databases had been published that address some specific research problems and are made available to the biometrics research community such as: constraint databases; mobile iris; multispectral; synthetics; iris at a distance; contact lenses; liveness detection; Aging, etc. However, even though these databases have addressed many of the problems mentioned above, most of them contain subjects of primarily Caucasian and Asian docents with very few Africans and many times zero Africans. It is particularly challenging as this has created a research blind spot for African cohorts in these databases. Despite many of the reported investigative studies on racial bias in face biometrics, very few iris racial bias-related studies have been published, which can be due to the insufficient number of other races databases such as the Africans. Recently, face recognition algorithms have been reported to be biased toward specific demographics. Most prominently, many investigative studies have reported higher false-positive rates in African subject cohorts than in other cohorts. Due to the unavailability of the required quantity of African cohorts in the publicly available iris databases, similar studies would be difficult to replicate on iris biometrics. We presents a mainly African dataset that can be useful in addressing some of these challenges.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The iris images were captured in Nigeria, Africa. The capturing exercise was conducted across various locations in multiple sessions over three months. Due to logistical constraints, the capturing exercise was limited to the northern part of Nigeria. Each subject volunteer was asked to sign a consent form authorizing the data to be used for only research purposes. The iris sensor device for capturing the images is the IKUSBE30 iris sensor from IrisKing. The setup for the capturing exercise is shown in Fig. 1. Each volunteer was asked to hold the device, standing or sitting, based on their preference. As such, the iris images were captured at various degrees of head postures.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/1.png&#34; title=&#34;Iris Capturing set-ups (indoor and outdoor)&#34;&gt;
Fig. 1: Iris Capturing set-ups (indoor and outdoor)&lt;/p&gt;
&lt;p&gt;The procedure for image capturing is in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Step 1: Straight Gaze Capture: In this step, the volunteer was asked to hold the sensor and gaze into its lenses while moving the eyes in small ranges to the left, right, up, or down for approximately 3 minutes. During this time, images were periodically captured automatically at constant intervals. The eye movement ensures that the iris position and orientation are highly diversified across the captured images. Samples of the captured images in this step are shown in Fig. 2 (left).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Step 2: Open-Close Capture:  In this step, the volunteer was asked to open and close the eyes while gazing into the device for approximately 3 minutes. The iris images were automatically captured within this period. The essence of the opening and closing of the eyes is to ensure that the images contain irises of various sizes and degrees of occlusion, from fully closed eyes to half open and fully open eyes. This procedure will ultimately improve image diversity. Exemplary samples are shown in Fig. 2 (right).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/2.png&#34; title=&#34;Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&#34;&gt;
Fig. 2: Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&lt;/p&gt;
&lt;p&gt;After the capturing sessions, the stored images were processed. The processing steps comprise automatic image selection to discard images with duplicate content, followed by manual image selection that guarantees the selected images&amp;rsquo; reliability. In the automatic selection, all the images from a single eye of one subject were organized into a matrix, with each column representing an image. The images of the subject&amp;rsquo;s eyes were then processed, and duplicates were removed using this procedure. The remaining images were then manually processed. The manual selection involves one-by-one human inspection of each image to identify damaged images and those images with no irises captured in them. Some sample images of one subject from the generated dataset are shown in Fig. 3, the labelled sampled images in Fig. 4, the summary of the database is presented in Table 1 and the database subjects&amp;rsquo; age distribution is shown in Fig. 5.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/3.png&#34; title=&#34;Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 3: Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 4&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/4.png&#34; title=&#34;Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 4: Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;Tab. 1: Summary of the generated database
&lt;img alt=&#34;Figure 5&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/table.png&#34; title=&#34;Summary of the generated database&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 6&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/5.png&#34; title=&#34;Age distribution of the generated database&#34;&gt;
Fig. 5: Age distribution of the generated database&lt;/p&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image folder: This contain the iris images. Each image is named with SubjectID_Eye_ImageNumber.jpg. That represents:
&lt;ul&gt;
&lt;li&gt;SubjectID: a unique number for each subject&lt;/li&gt;
&lt;li&gt;Eye is a letter that can be either “L” for left eye or “R” for right eye&lt;/li&gt;
&lt;li&gt;ImageNumber is a sequential number for images for the same subject&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iris_edge folder: This contain the corresponding 1-pixel iris edge binary images&lt;/li&gt;
&lt;li&gt;iris_edge_mask folder: This contain the corresponding iris binary masks of the images&lt;/li&gt;
&lt;li&gt;pupil_edge folder: This contain the corresponding 1-pixel pupil edge binary images&lt;/li&gt;
&lt;li&gt;pupil_edge_mask folder: This contain the corresponding pupil binary masks of the images&lt;/li&gt;
&lt;li&gt;params folder: This contain the corresponding ini files that describes radius and orientation both the iris and pupil used in the generation of iris edge, iris mask, pupil edge, pupil and mask images&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-Iris-Africa collected by the Chinese Academy of Sciences’ Institute of Automation (CASIA)” and a reference to “CASIA-Iris-Africa Image Database, 
&lt;a href=&#34;http://biometrics.idealtest.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Polar</title>
      <link>http://localhost:1313/dataset/casia-polar/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-polar/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;CASIA-Polar is a face anti-spoofing dataset based on polarization imaging, which takes advantage of polarization information in material classification to enable robust face anti-spoofing research.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The hardware system used to collect this dataset is shown in Fig.1, consists of a Lucid Phoenix PHX050S-P polarized camera equipped with Sony’s polarization sensor and a Mindvision MVGE501GC-T RGB camera.&lt;/p&gt;
&lt;p&gt;Types of spoofing attacks include printed paper, printed glossy photographs, electronic displays, silicone masks, rubber masks, and customized silicone prosthetic heads. High-quality face samples are first captured by the MVGE501GC-T RGB camera and these high-quality samples are then printed on paper and photographs or displayed on a computer screen to produce artifacts. At the same time, both genuine and spoofing attack face samples were captured when the subjects and presentation attack were standing at or be placed at six distances, i.e. 1m, 1.5m, 2m, 3m, 4m, and 5m.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/1_1.png&#34;&gt;
&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/1_2.png&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset includes 22,174 samples from 121 subjects. All subjects had visible light and polarized images taken. The genuine face has 14,698 samples, while the spoofing attack has about 7,476 samples.&lt;/p&gt;
&lt;p&gt;Figure 2 shows an example of the face images collected in the dataset, with the first row being the visible image and the second row is the corresponding DoLP image.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/2.png&#34; title=&#34;The attacks present in CASIA-Polar&#34;&gt;
Fig.2: The attacks present in CASIA-Polar&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved.&lt;/p&gt;
&lt;p&gt;E-mail: 
&lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Iris-LFLD</title>
      <link>http://localhost:1313/dataset/casia-iris-lfld/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-lfld/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The dataset for iris liveness detection based on light field (LF) imaging was collected by [1], wherein the first author is one of our collaborators. We have got the authority from the authors of [1] and released the LF focal stack data.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The dataset was captured using a 
&lt;a href=&#34;http://cripac.ia.ac.cn/CN/column/item105.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lab-produced microlens based LF camera&lt;/a&gt; and a commercial device 
&lt;a href=&#34;http://www.irisking.com/pron.php?id=523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IKUSB-E30&lt;/a&gt; under near-infrared (NIR) illumination. The setup of dataset collection was shown in Fig.1.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/capture_setup.png&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset contains 504 samples from 14 subjects, consisting of 230 LF images of bona fide iris and 274 LF images of spoofing iris. The respective sample number of the PAIs, i.e. printed papers, printed photos, and electronic display are 18, 122, 134.&lt;/p&gt;
&lt;p&gt;An example of raw LF image containing both eyes printed on photos is shown in Fig.2. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/lf_raw_sample.png&#34; title=&#34;An example of raw LF image containing both eyes printed on photos.&#34;&gt;
Fig.2: An example of raw LF image containing both eyes printed on photos. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;The LF toolbox released by [2] was utilized to decode raw LF images into 4D LF data. The eye regions were cropped from the same location of each sub-aperture image (SAI). The spatial resolution of each SAI after cropping is $128 \times 96$, and the angular resolution is $7 \times 7$. Examples from the same subject&amp;rsquo;s right eye in the dataset are shown in Fig.3.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/dataset_samples.png&#34; title=&#34;Examples from the same subject&amp;#39;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&#34;&gt;
Fig. 3: Examples from the same subject&amp;rsquo;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&lt;/p&gt;
&lt;p&gt;The rendered focal stack via digital refocusing has 145 slices around the best focus plane.&lt;/p&gt;
&lt;p&gt;The details of the adopted database is listed in Table 1.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Table 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/details.png&#34; title=&#34;Details of the adopted database&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the  database are reserved.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] 宋平, 黄玲, 王云龙, 刘菲, 孙哲南. 基于计算光场成像的虹膜活体检测方法. 自动化学报, 2019, 45(9): 1701-1712. (Ping Song, Huang Ling, Wang Yunlong, Liu Fei, and Sun Zhenan. Iris liveness detection based on light field imaging. ACTA AUTOMATICA SINICA, 45(9):1701–1712, 2019.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[2] Donald G Dansereau, Oscar Pizarro, and Stefan B Williams, “Decoding, calibration and rectification for lenselet-based plenoptic cameras,” in Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1027–1034.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Iris-Degradation</title>
      <link>http://localhost:1313/dataset/casia-iris-degradation/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-degradation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This work presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.&lt;/p&gt;
&lt;h2 id=&#34;description-of-casia-iris-degradation&#34;&gt;Description of CASIA-Iris-Degradation&lt;/h2&gt;
&lt;p&gt;CASIA-Iris-Degradation contains 36,539 images from 255 Asian people. All images were collected under NIR illumination and two eyes were captured simultaneously. Details of the proposed database are shown in the table below.
&lt;img alt=&#34;Details of the proposed database&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/Statistics.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;setup-of-image-collection&#34;&gt;Setup of image collection&lt;/h3&gt;
&lt;p&gt;As shown in the figure below, we built a collection room using black-out cloth. The curtain on one side of the room can be opened and closed artificially to control the natural light.
Inside, there were camera (A), light sources (B, C), volunteer (D), and four directional markers (1-4).
Each volunteer was asked to sit down, put their chin on the holder (0.75 m from the camera), keep their head as still as possible, and move their eyes according to instructions.
To simulate the off-angle situation, the subject was required to look along a set of directions indicated by four markers in the visual field, while in other cases look straight ahead (i.e., the midpoint of marker 1 and 4).
&lt;img alt=&#34;environment and equipment&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/newcx1camera.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-proposed-database&#34;&gt;Statistics of the proposed database&lt;/h3&gt;
&lt;p&gt;To simulate real image degradation, the proposed database is divided into four categories, and each of them is separated into three to five subsets as follows:&lt;/p&gt;
&lt;h4 id=&#34;illumination&#34;&gt;Illumination&lt;/h4&gt;
&lt;p&gt;The intensity of the VW light source was adjusted to four levels: Dark (0%), Weak (25%), Medium (50%), Strong (100%) to change the pupil size.
In addition, images under natural light were also collected (with Dark level).
Note that the intensity in other categories was set to the Medium level by default.&lt;/p&gt;
&lt;h4 id=&#34;off-angle&#34;&gt;Off-angle&lt;/h4&gt;
&lt;p&gt;There are four directions: (1) Left, (2) Upper left, (3) Upper right, (4) Right.
The left and right are in the horizontal direction, while the upper left and right angles are both 45 degrees.&lt;/p&gt;
&lt;h4 id=&#34;nonideal-eye-state&#34;&gt;Nonideal eye state&lt;/h4&gt;
&lt;p&gt;Since it is difficult to keep eyes open all the time, we collected images of closed, squinted, and half-open eyes.
Although most images of the former two classes have no effective iris region and are accompanied by blur, they can be used to train eye state detectors for fatigue driving detection or other relevant scenarios.&lt;/p&gt;
&lt;h4 id=&#34;occlusion&#34;&gt;Occlusion&lt;/h4&gt;
&lt;p&gt;For occlusion, volunteers were required to wear glasses, masks and using a hand to cover the mouth and nose. During this section, the NIR light source was randomly moved slightly to generate light spots. Meanwhile, some glasses also had stains on the surface to occlude the iris.
An unexpected observation is that for some elderly volunteers, their eyelids droop naturally, resulting in severe occlusion, which should arouse more attention.&lt;/p&gt;
&lt;p&gt;More samples and annotations are presented below.
&lt;img alt=&#34;More samples&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/supp_images.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA-Iris-Degradation database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Iris-Degradation-V1.0 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Evaluation Benchmark for High-throughput Iris Recognition at a Distance</title>
      <link>http://localhost:1313/dataset/blurred_iris_benchmark/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/blurred_iris_benchmark/</guid>
      <description>&lt;h2 id=&#34;1-introduction&#34;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A key problem of iris recognition at a distance is that a large portion of captured iris images is nonideal because of narrow depth of field (DoF), noncooperative user movement, incongruous exposure time and so on. Current iris recognition systems usually filter out these low-quality images using strict criteria of image quality evaluation (IQA). However, this strategy inevitably leads to a waste of device capacity and low throughput. Therefore, a better and practical solution is to make the utmost of degraded iris images for personal identification. We announce the availability of a long-range captured dataset containing 3,756 iris images of various degradation factors from 98 subjects. An evaluation benchmark is built upon the dataset for a comparative study on preprocessing and recognition of NIR iris images in high-throughput scenarios. The datasets, manual annotations and evaluation toolkit are publicly available.&lt;/p&gt;
&lt;h2 id=&#34;2-descriptions-and-statistics-of-the-database&#34;&gt;&lt;strong&gt;2. Descriptions and Statistics of the Database&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;image-collection&#34;&gt;&lt;strong&gt;Image collection&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- &lt;iframe height=498 width=510 src=&#34;collection_glasses.mp4&#34;&gt; --&gt;
&lt;p&gt;The schematic and setup of blur-varying iris image collection of this database at a distance are shown as following.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Schematic and setup of NIR iris image collection at a distance and in less cooperative environments.&#34; src=&#34;http://localhost:1313/dataset/blurred_iris_benchmark/capture_setting.png&#34; title=&#34;Image Collection&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next generation of CASIA-LR-Cam bundled with NIR illumination at a wavelength of 830 nm was employed as the capturing device. Its standoff distance is approximately 1.2 meters with a DoF of over 20 centimeters. The field of view (FoV) is approximately 20 degrees. The device was placed in an indoor environment under no extra lighting sources. During the process of image collection, the subjects were obliged to move freely in the restricted square area 1.0~1.4 meters away from the device. Specifically, they could casually step forward and backward, left and right.&lt;/p&gt;
&lt;p&gt;While moving inside the restricted area, the subjects were guided by the indication signal on the GUI screen to look at different directions for approximately 30 seconds in a single session. Two separate sessions were launched in the daytime under the same conditions, and the interval was one week. If the subject was wearing glasses, he or she needed to take them off in either of the two sessions (&lt;code&gt;play the video and see&lt;/code&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 1 With Glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_glasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 2 No glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_noglasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;The acquired iris image sequences were captured at 5~10 frames per second. The resolution of each frame was 3840x2748. The frames in which irides were completely invisible caused by blinking or squinting were thrown away. Then evenly spaced images are extracted from the processed sequence every 5 frames. On average, approximately 20 images of each subject were retained.&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-dataset&#34;&gt;&lt;strong&gt;Statistics of the dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attributes&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;The database&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Camera Type&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;CASIA-LR-Cam II&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Illumination&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NIR and natural lighting sources&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total pixel&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3840x2748&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cropped eye region&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;640x480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sessions&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Two separate sessions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Institution of subjects&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Graduate students and staff of CASIA and TAfIRT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standoff distance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.0~1.4m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Working mode&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Step freely within a moderate square area&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Depth of field&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ca. 20cm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of subjects&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Classes&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Images&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3,765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Images per class&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ca. 19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pairs of Images&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;39,418 intraclass and 7,406,312 interclass&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;mannual-annotations&#34;&gt;&lt;strong&gt;Mannual Annotations&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Each image in the dataset is manually annotated with binary maps of iris masks, inner and outer iris boundaries, upper and lower eyelids, and sclera masks shown as below.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Two annotated instances in the dataset.&#34; src=&#34;http://localhost:1313/dataset/blurred_iris_benchmark/annotations.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;evaluation-toolkit&#34;&gt;&lt;strong&gt;Evaluation Toolkit&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;IrisStat_V3.0.rar&#34;&gt;IrisStat_V3.0.rar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The package of evaluation toolkit is organized as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IrisIQA
│
└───config/
│
└───out/ 
│
└───utils/
|     computeMotionblur.m
|     computeSharpness.m
|     ini2struct.m
|     Integral.m
|     progressbar.m
│     struct2ini.m
|
└───MotionBlur_Main.m
│
└───Sharpness_Main.m

 
Segmentation
│
└───config/
│
└───out/ 
│
└───utils/
|    evalSeg.m
|    Hausdorff.m
|    ini2struct.m
|    progressbar.m
|    struct2ini.m 
│
└───IrisSeg_Main.m

Recognition
│
└───config/
│
└───out/ 
│
└───utils/
|     ACC.m
|     Bitshift.m
|     colors.mat
|     compute_iriscode_sim.m
|     compute_om_sim.m
|     compute_vector_sim.m
|     draw_CMC_curve.m
|     draw_DET_curve.m
|     EER.m
|     IdentiACC.m
|     linspecer.m
|     Merge_Multi_CMC_Curve.m
|     Merge_Multi_Det_Curve.m
|     plot_styles.mat
|     progressbar.m
|     VerfiACC.m
│
└───IrisRec_main.m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The main function of iris IQA evaluating sharpness is &lt;code&gt;Sharpness_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris segmentation evaluation is &lt;code&gt;IrisSeg_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris recognition evaluation is &lt;code&gt;IrisRec_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;&lt;strong&gt;Database Organization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database package comprises the following components organized in multiple directories.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|
circle_params
|  │
|  └───xxxL(R)_xx.ini
|
ellipse_params
|  │
|  └───xxxL(R)_xx.ini 
|
image
|  │
|  └───xxxL(R)_xx.jpg
|
iris_edge
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
iris_mask
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
sclera_mask
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
vis_result
|  │
|  └───xxxL(R)_xx.png
|
vis_result_new
|  │
|  └───xxxL(R)_xx.png
|
imgList.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file naming rule is &amp;ldquo;&lt;code&gt;xxxL(R)_xx&lt;/code&gt;&amp;rdquo;, where &amp;ldquo;&lt;code&gt;xxx&lt;/code&gt;&amp;rdquo; denotes the unique identifier of the subject, &amp;ldquo;&lt;code&gt;L&lt;/code&gt;&amp;rdquo; denotes left eye and &amp;ldquo;&lt;code&gt;R&lt;/code&gt;&amp;rdquo; denotes right eye and &amp;ldquo;&lt;code&gt;xx&lt;/code&gt;&amp;rdquo; denotes the index of the image in the class, e.g., &lt;code&gt;001L_01&lt;/code&gt;. All the filenames of the iris images and belonging classes are stored in &lt;code&gt;imgList.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;&lt;strong&gt;Copyright Note and Contacts&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the dataset collected by Smart Iris Recognition (SIR) group from the Chinese Academy of Sciences, Institute of Automation (CASIA)”.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, a non-student researcher must manually sign the 
&lt;a href=&#34;license_agreement.pdf&#34;&gt;License Agreement&lt;/a&gt; and agree to observe the restrictions. The signed document should be digitized and sent through email to: 
&lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Face-Africa</title>
      <link>http://localhost:1313/dataset/casia-face-africa/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-face-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the years, numerous face databases have been published that brought about exciting breakthrough in the facial biometric research field, most especially from the recent trend of deep learning contributions. However, as investigated by researchers, most of these databases are demographically imbalanced and often contain few number of African cohorts. Of those with relatively large number of the Africans, the databases are usually wild (downloaded from the internet or digitalised from printed photographs). Methods that adopt these skewed databases often exhibits some form of performance bias that can result to unintended consequences for real time applications. As such, there is need for more demographically inclusive datasets. CASIA-Face-Africa is developed to provide solution to this problem. It is an all-African database that is made to be used as a complementary database with the existing databases to balance the number of the African cohorts in the published datasets and improve their demographic inclusiveness.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;DESCRIPTION&lt;/h2&gt;
&lt;p&gt;The database images were captured at various locations in Nigeria, Africa. About 1150 volunteers participated in the capturing exercise. The images of each subject were captured concurrently using 3 cameras. Two visible wavelength (VW) cameras and one near-infrared (NIR) camera. The capturing was done in various sessions over a period of 3 months. Some of the subjects have images captured in multiple sessions while majority of the subjects have their images captured in a single session.
&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/1.png&#34;&gt;
&lt;em&gt;Figure 1: The cameras arrangement set-up was made to be same in all the capturing sessions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For each capturing instance, 3 to 10 still images were captured by each camera at a fixed time interval of 1 to 3 seconds. For some subjects, an external illumination light source was used for capturing additional images of that subjects. Also, some subjects were asked to use face accessory such as eye glasses for multiple capturing. The captured images were then organized and their land mark labelled. The organized database comprises a total of 38,546 images from 1,183 subjects. Specifically, 12,063 images captured by VW camera 1 at the resolution of 1332×1080, 13,232 images are captured by VW camera 2 at the resolution of 787 × 962, and 13,251 images are captured by NIR camera at the resolution of 983 × 877. Some samples of the captured images are shown in Figure 2, Figure 3 and Figure 4.
&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/2.png&#34;&gt;
&lt;em&gt;Figure 2: Sample of a single subject images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/3.png&#34;&gt;
&lt;em&gt;Figure 3: Sample of subject Expressions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 4&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/4.png&#34;&gt;
&lt;em&gt;Figure 4: Sample of labelled face images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images folder: This contain the actual face images. Each file is named as SubjectID_ImageNumber.jpg. The SubjectID is a unique number for each subject and the ImageNumber is a sequential number for images of the same subject.&lt;/li&gt;
&lt;li&gt;Subjects folder: This contain the corresponding ini files that describe the unique subjects in the images. Each file is named with the ID of the subject as SubjectID.ini.&lt;/li&gt;
&lt;li&gt;Attributes folder: This contain the corresponding ini files that describes each individual face image. Each file is named with its corresponding face image name as SubjectID_ImageNumber.ini&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Example of the names: ID-V-All-Ep1.ini, ID-I-Split-Ep3.ini, etc. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Face-Africa collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] Muhammad Jawad, Yunlong Wang, Caiyong Wang, Kunbo Zhang, Zhenan Sun. “CASIA-Face-Africa: A Large-scale African Face Image Database,” IEEE Transactions on Information Forensics and Security (TIFS), vol.16, pp. 3634-3646, 2021.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
