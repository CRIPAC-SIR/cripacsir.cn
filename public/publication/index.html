<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="The smart iris research (SIR) is affiliated to the Institute of Automation, Chinese Academy of Sciences (CAS).">

  
  <link rel="alternate" hreflang="en-us" href="https://nlpr-sir.github.io/publication/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Smart Individual Recognition">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://nlpr-sir.github.io/publication/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Smart Individual Recognition">
  <meta property="og:url" content="https://nlpr-sir.github.io/publication/">
  <meta property="og:title" content="Publications | Smart Individual Recognition">
  <meta property="og:description" content="The smart iris research (SIR) is affiliated to the Institute of Automation, Chinese Academy of Sciences (CAS)."><meta property="og:image" content="https://nlpr-sir.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_3.png">
  <meta property="twitter:image" content="https://nlpr-sir.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2024-06-17T00:00:00&#43;00:00">
  

  




  


  





  <title>Publications | Smart Individual Recognition</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  









<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Smart Individual Recognition</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Smart Individual Recognition</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#head"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/dataset"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>Member</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2024">
              2024
            </option>
            
            <option value=".year-2023">
              2023
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            <option value=".year-2017">
              2017
            </option>
            
            <option value=".year-2016">
              2016
            </option>
            
            <option value=".year-2015">
              2015
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/haiqing-li/">Haiqing Li</a></span>, <span ><a href="/author/yixin-zhang/">Yixin Zhang</a></span>, <span ><a href="/author/guangzhe-zhao/">Guangzhe Zhao</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tbiom-2024-1/">
      <img src="/publication/wang-tbiom-2024-1/featured_hu7e8e002390aace999746b102958d34ef_361986_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Sclera-TransFuse: Fusing Vision Transformer and CNN for Accurate Sclera Segmentation and Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tbiom-2024-1/">Sclera-TransFuse: Fusing Vision Transformer and CNN for Accurate Sclera Segmentation and Recognition</a>
  </h3>

  
  <a href="/publication/wang-tbiom-2024-1/" class="summary-link">
    <div class="article-style">
      <p>This paper investigates a deep learning based unified framework for accurate sclera segmentation and recognition, named Sclera-TransFuse. Unlike previous CNN-based methods, our framework incorporates Vision Transformer and CNN to extract complementary feature representations, which are beneficial to both subtasks. Specifically, for sclera segmentation, a novel two-stream hybrid model, referred to as Sclera-TransFuse-Seg, is developed to integrate classical ResNet-34 and recently emerging Swin Transformer encoders in parallel. The dual-encoders firstly extract coarse-and fine-grained feature representations at hierarchical stages, separately. Then a Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual-encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. With the results of sclera segmentation, the sclera ROI image is generated for sclera feature extraction. Additionally, a new sclera recognition model, termed as Sclera-TransFuse-Rec, is proposed by combining lightweight EfficientNet B0 and multi-scale Vision Transformer in sequential to encode local and global sclera vasculature feature representations. Extensive experiments on several publicly available databases suggest that our framework consistently achieves state-of-the-art performance on various sclera segmentation and recognition benchmarks, including the 8th Sclera Segmentation and Recognition Benchmarking Competition (SSRBC 2023). A UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through 
<a href="https://github.com/lhqqq/Sclera-TransFuse" target="_blank" rel="noopener">https://github.com/lhqqq/Sclera-TransFuse</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/10559402" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tbiom-2024-1/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TBIOM.2024.3415484" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/xingyu-gao/">Xingyu Gao</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 19)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wei-tifs-2024_1/">
      <img src="/publication/wei-tifs-2024_1/featured_hu9b1fcfafee637cea6a2bbbbd0a8b05b8_384685_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wei-tifs-2024_1/">Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation</a>
  </h3>

  
  <a href="/publication/wei-tifs-2024_1/" class="summary-link">
    <div class="article-style">
      <p>Accurate iris segmentation, especially around the iris inner and outer boundaries, is still a formidable challenge. Pixels within these areas are difficult to semantically distinguish since they have similar visual characteristics and close spatial positions. To tackle this problem, the paper proposes an iris segmentation graph neural network (ISeGraph) for accurate segmentation. ISeGraph regards individual pixels as nodes within the graph and constructs self-adaptive edges according to multi-faceted knowledge, including visual similarity, positional correlation, and semantic consistency for feature aggregation. Specifically, visual similarity strengthens the connections between nodes sharing similar visual characteristics, while positional correlation assigns weights according to the spatial distance between nodes. In contrast to the above knowledge, semantic consistency maps nodes into a semantic space and learns pseudo-labels to define relationships based on label consistency. ISeGraph leverages multi-faceted knowledge to generate self-adaptive relationships for accurate iris segmentation. Furthermore, a pixel-wise adaptive normalization module is developed to increase the feature discriminability. It takes informative features in the shallow layer as a reference to improve the segmentation features from a statistical perspective. Experimental results on three iris datasets illustrate that the proposed method achieves superior performance in iris segmentation, increasing the segmentation accuracy in areas near the iris boundaries.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/10555436" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wei-tifs-2024_1/cite.bib">
  Cite
</button>























<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2024.3407508" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/kaiduo-zhang/">Kaiduo Zhang</a></span>, <span ><a href="/author/muyi-sun/">Muyi Sun</a></span>, <span ><a href="/author/jianxin-sun/">Jianxin Sun</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2024 International Journal of Computer Vision (IJCV)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/kunbo-ijcv-2024/">
      <img src="/publication/kunbo-ijcv-2024/featured_hu1817680fa772f0fd59b5ac10d40042e9_217231_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Open-Vocabulary Text-Driven Human Image Generation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/kunbo-ijcv-2024/">Open-Vocabulary Text-Driven Human Image Generation</a>
  </h3>

  
  <a href="/publication/kunbo-ijcv-2024/" class="summary-link">
    <div class="article-style">
      <p>Generating human images from open-vocabulary text descriptions is an exciting but challenging task. Previous methods (i.e., Text2Human) face two challenging problems: (1) they cannot well handle the open-vocabulary setting by arbitrary text inputs (i.e., unseen clothing appearances) and heavily rely on limited preset words (i.e., pattern styles of clothing appearances); (2) the generated human image is inaccuracy in open-vocabulary settings. To alleviate these drawbacks, we propose a flexible diffusion-based framework, namely HumanDiffusion, for open-vocabulary text-driven human image generation (HIG). The proposed framework mainly consists of two novel modules: the Stylized Memory Retrieval (SMR) module and the Multi-scale Feature Mapping (MFM) module. Encoded by the vision-language pretrained CLIP model, we obtain coarse features of the local human appearance. Then, the SMR module utilizes an external database that contains clothing texture details to refine the initial coarse features. Through SMR refreshing, we can achieve the HIG task with arbitrary text inputs, and the range of expression styles is greatly expanded. Later, the MFM module embedding in the diffusion backbone can learn fine-grained appearance features, which effectively achieves precise semantic-coherence alignment of different body parts with appearance features and realizes the accurate expression of desired human appearance. The seamless combination of the proposed novel modules in HumanDiffusion realizes the freestyle and high accuracy of text-guided HIG and editing tasks. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SOTA) performance, especially in the open-vocabulary setting.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/article/10.1007/s11263-024-02079-7" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/kunbo-ijcv-2024/cite.bib">
  Cite
</button>























<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/s11263-024-02079-7" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zihui-yan/">Zihui Yan</a></span>, <span ><a href="/author/lingxiao-he/">Lingxiao He</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Machine Intelligence Research(Volume: 21)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/yan-mir-2024-1/">
      <img src="/publication/yan-mir-2024-1/featured_hua487db749e76c04f33b69911c11bf4bf_491947_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/yan-mir-2024-1/">Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation</a>
  </h3>

  
  <a href="/publication/yan-mir-2024-1/" class="summary-link">
    <div class="article-style">
      <p>In the daily application of an iris-recognition-at-a-distance (IAAD) system, many ocular images of low quality are acquired. As the iris part of these images is often not qualified for the recognition requirements, the more accessible periocular regions are a good complement for recognition. To further boost the performance of IAAD systems, a novel end-to-end framework for multi-modal ocular recognition is proposed. The proposed framework mainly consists of iris/periocular feature extraction and matching, unsupervised iris quality assessment, and a score-level adaptive weighted fusion strategy. First, ocular feature reconstruction (OFR) is proposed to sparsely reconstruct each probe image by high-quality gallery images based on proper feature maps. Next, a brand new unsupervised iris quality assessment method based on random multiscale embedding robustness is proposed. Different from the existing iris quality assessment methods, the quality of an iris image is measured by its robustness in the embedding space. At last, the fusion strategy exploits the iris quality score as the fusion weight to coalesce the complementary information from the iris and periocular regions. Extensive experimental results on ocular datasets prove that the proposed method is obviously better than unimodal biometrics, and the fusion strategy can significantly improve the recognition performance.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="#ZgotmplZ" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/yan-mir-2024-1/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Machine Intelligence Research(Volume: 21)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/jawad-mir-2024-2/">
      <img src="/publication/jawad-mir-2024-2/featured_hu688f821f6073bc2b15ddb5d63f99446b_1403311_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="CASIA-Iris-Africa: A Large-scale African Iris Image Database">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/jawad-mir-2024-2/">CASIA-Iris-Africa: A Large-scale African Iris Image Database</a>
  </h3>

  
  <a href="/publication/jawad-mir-2024-2/" class="summary-link">
    <div class="article-style">
      <p>Iris biometrics is a phenotypic biometric trait that has proven to be agnostic to human natural physiological changes. Research on iris biometrics has progressed tremendously, partly due to publicly available iris databases. Various databases have been available to researchers that address pressing iris biometric challenges such as constraint, mobile, multispectral, synthetics, long-distance, contact lenses, liveness detection, etc. However, these databases mostly contain subjects of Caucasian and Asian docents with very few Africans. Despite many investigative studies on racial bias in face biometrics, very few studies on iris biometrics have been published, mainly due to the lack of racially diverse large-scale databases containing sufficient iris samples of Africans in the public domain. Furthermore, most of these databases contain a relatively small number of subjects and labelled images. This paper proposes a large-scale African database named Chinese Academy of Sciences Institute of Automation (CASIA)-Iris-Africa that can be used as a complementary database for the iris recognition community to mediate the effect of racial biases on Africans. The database contains 28 717 images of 1 023 African subjects (2 046 iris classes) with age, gender, and ethnicity attributes that can be useful in demographically sensitive studies of Africans. Sets of specific application protocols are incorporated with the database to ensure the databaseâ€™s variability and scalability. Performance results of some open-source state-of-the-art (SOTA) algorithms on the database are presented, which will serve as baseline performances. The relatively poor performances of the baseline algorithms on the proposed database despite better performance on other databases prove that racial biases exist in these iris recognition algorithms.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/s11633-022-1402-8" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/jawad-mir-2024-2/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.idealtest.org" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/s11633-022-1402-8" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span ><a href="/author/hongwen-zhang/">Hongwen Zhang</a></span>, <span ><a href="/author/zerui-chen/">Zerui Chen</a></span>, <span ><a href="/author/mengcheng-li/">Mengcheng Li</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/yebin-liu/">Yebin Liu</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>AAAI 2024</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hu-aaai-2024/">
      <img src="/publication/hu-aaai-2024/featured_hub53f5ff79be32f862f9eb1fb6c4668b0_192849_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hu-aaai-2024/">Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images</a>
  </h3>

  
  <a href="/publication/hu-aaai-2024/" class="summary-link">
    <div class="article-style">
      <p>Reconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. Our method consists of two components: explicit contact prediction and implicit shape reconstruction. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, our method can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin. Our code is publicly available at 
<a href="https://junxinghu.github.io/projects/hoi.html" target="_blank" rel="noopener">https://junxinghu.github.io/projects/hoi.html</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ojs.aaai.org/index.php/AAAI/article/view/27995" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hu-aaai-2024/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/JunxingHu/CHOI" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://underline.io/lecture/93986-learning-explicit-contact-for-implicit-reconstruction-of-hand-held-objects-from-monocular-images" target="_blank" rel="noopener">
  Video
</a>













<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1609/aaai.v38i3.27995" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/ru-yiwei/">Ru Yiwei</a></span>, <span ><a href="/author/li-peipei/">Li Peipei</a></span>, <span ><a href="/author/sun-muyi/">Sun Muyi</a></span>, <span ><a href="/author/wang-yunlong/">Wang Yunlong</a></span>, <span ><a href="/author/zhang-kunbo/">Zhang Kunbo</a></span>, <span ><a href="/author/li-qi/">Li Qi</a></span>, <span ><a href="/author/he-zhaofeng/">He Zhaofeng</a></span>, <span ><a href="/author/sun-zhenan/">Sun Zhenan</a></span>, <span ><a href="/author/et.al/">Et.al</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2023 ACM International Conference on Multimedia</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ru-mm-2023/">
      <img src="/publication/ru-mm-2023/featured_hu5c2027b5f62b123982bf52fb2e9c4cf7_109589_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ru-mm-2023/">Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence</a>
  </h3>

  
  <a href="/publication/ru-mm-2023/" class="summary-link">
    <div class="article-style">
      <p>Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a &ldquo;small yet powerful&rdquo; physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at 
<a href="https://remap-dataset.github.io/ReMAP" target="_blank" rel="noopener">https://remap-dataset.github.io/ReMAP</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://dl.acm.org/doi/abs/10.1145/3581783.3611754" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ru-mm-2023/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1145/3581783.3611754" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/li-haiqing/">Li Haiqing</a></span>, <span ><a href="/author/wang-caiyong/">Wang Caiyong</a></span>, <span ><a href="/author/zhao-guangzhe/">Zhao Guangzhe</a></span>, <span ><a href="/author/he-zhaofeng/">He Zhaofeng</a></span>, <span ><a href="/author/wang-yunlong/">Wang Yunlong</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2023 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/haiqing-ijcb-2023/">
      <img src="/publication/haiqing-ijcb-2023/featured_hu3d830cdf43d1558ff01e3a83a335f2b9_299687_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/haiqing-ijcb-2023/">Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation</a>
  </h3>

  
  <a href="/publication/haiqing-ijcb-2023/" class="summary-link">
    <div class="article-style">
      <p>Sclera segmentation is a crucial step in sclera recognition, which has been greatly advanced by Convolutional Neural Networks (CNNs). However, when dealing with non-ideal eye images, many existing CNN-based approaches are still prone to failure. One major reason is that due to the limited range of receptive fields, CNNs are difficult to effectively model global semantic relevance and thus robustly resist noise interference. To solve this problem, this paper proposes a novel two-stream hybrid model, named Sclera-TransFuse, to integrate classical ResNet-34 and recently emerging Swin Transformer encoders. Specially, the self-attentive Swin Transformer has shown a strong ability in capturing long-range spatial dependencies and has a hierarchical structure similar to CNNs. The dual encoders firstly extract coarse- and fine-grained feature representations at hierarchical stages, separately. Then a novel Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. Experimental results show that Sclera-TransFuse achieves state-of-the-art performance on various sclera segmentation benchmarks. Additionally, a UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through 
<a href="https://github.com/Ihqqq/Sclera-TransFuse" target="_blank" rel="noopener">https://github.com/Ihqqq/Sclera-TransFuse</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/10448814" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/haiqing-ijcb-2023/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/haiqing-ijcb-2023/IJCB2023-2-poster.pdf" target="_blank" rel="noopener">
  Poster
</a>







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ijcb2023.ieee-biometrics.org/award-winners/" target="_blank" rel="noopener">
  Awards
</a>











<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB57857.2023.10448814" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span ><a href="/author/hongwen-zhang/">Hongwen Zhang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Circuits and Systems for Video Technology ( Volume: 34)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hu-tcsvt-2023-2/">
      <img src="/publication/hu-tcsvt-2023-2/featured_hubecde92cf2d55ab0e32d8a5b200c1089_988750_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Personalized Graph Generation for Monocular 3D Human Pose and Shape Estimation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hu-tcsvt-2023-2/">Personalized Graph Generation for Monocular 3D Human Pose and Shape Estimation</a>
  </h3>

  
  <a href="/publication/hu-tcsvt-2023-2/" class="summary-link">
    <div class="article-style">
      <p>3D human pose and shape estimation from a single RGB image is an appealing yet challenging task. Due to the graph-like nature of human parametric models, a growing number of graph neural network-based approaches have been proposed and achieved promising results. However, existing methods build graphs for different instances based on the same template SMPL mesh, neglecting the geometric perception of individual properties. In this work, we propose an end-to-end method named Personalized Graph Generation (PGG) to construct the geometry-aware graph from an intermediate predicted human mesh. Specifically, a convolutional module initially regresses a coarse SMPL mesh tailored for each sample. Guided by the 3D structure of this personalized mesh, PGG extracts the local features from the 2D feature map. Then, these geometry-aware features are integrated with the specific coarse SMPL parameters as vertex features. Furthermore, a body-oriented adjacency matrix is adaptively generated according to the coarse mesh. It considers individual full-body relations between vertices, enhancing the perception of body geometry. Finally, a graph attentional module is utilized to predict the residuals to get the final results. Quantitative experiments across four benchmarks and qualitative comparisons on more datasets show that the proposed method outperforms state-of-the-art approaches for 3D human pose and shape estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/10236465" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hu-tcsvt-2023-2/cite.bib">
  Cite
</button>























<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484357" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/yuhao-zhu/">Yuhao Zhu</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-tpami-2023/">
      <img src="/publication/ren-tpami-2023/featured_hu304eae620bd4594a032f390cfc132492_29805_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-tpami-2023/">Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions</a>
  </h3>

  
  <a href="/publication/ren-tpami-2023/" class="summary-link">
    <div class="article-style">
      <p>Occlusion is a common problem with biometric recognition in the wild. The generalization ability of CNNs greatly decreases due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrating the merits of both CNNs and graph models to overcome occlusion problems in biometric recognition, called multiscale dynamic graph representation (MS-DGR). More specifically, a group of deep features reflected on certain subregions is recrafted into a feature graph (FG). Each node inside the FG is deemed to characterize a specific local region of the input sample, and the edges imply the co-occurrence of non-occluded regions. By analyzing the similarities of the node representations and measuring the topological structures stored in the adjacent matrix, the proposed framework leverages dynamic graph matching to judiciously discard the nodes corresponding to the occluded parts. The multiscale strategy is further incorporated to attain more diverse nodes representing regions of various sizes. Furthermore, the proposed framework exhibits a more illustrative and reasonable inference by showing the paired nodes. Extensive experiments demonstrate the superiority of the proposed framework, which boosts the accuracy in both natural and occlusion-simulated cases by a large margin compared with that of baseline methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10193782" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-tpami-2023/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/RenMin1991/Dyamic-Graph-Representation" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TPAMI.2023.3298836" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security(Volume:18)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/jawad-tifs-2023-1/">
      <img src="/publication/jawad-tifs-2023-1/featured_hu9f78aaa3f06c1513d6e06bef4ab9c32a_798158_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/jawad-tifs-2023-1/">Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics</a>
  </h3>

  
  <a href="/publication/jawad-tifs-2023-1/" class="summary-link">
    <div class="article-style">
      <p>In recent years, unconstrained iris biometrics has become more prevalent due to its wide range of user applications. However, it also presents numerous challenges to the Iris pre-processing task of Localization and Segmentation (ILS). Many ILS techniques have been proposed to address these challenges, among which the most effective is the CNN-based methods. Training the CNN is data-intensive, and most of the existing CNN-based ILS approaches do not incorporate iris-specific features that can reduce their data dependence, despite the limited labelled iris data in the available databases. These trained CNN models built upon these databases can be sub-optimal. Hence, this paper proposes a guided CNN-based ILS approach IrisGuideNet. IrisGuideNet involves incorporating novel iris-specific heuristics named Iris Regularization Term (IRT), deep supervision technique, and hybrid loss functions in the training pipeline, which guides the network and reduces the model data dependence. A novel Iris Infusion Module (IIM) that utilizes the geometrical relationships between the ILS outputs to refine the predicted outputs is introduced at network inference. The proposed model is trained and evaluated with various datasets. Experimental results show that IrisGuideNet has outperformed most models across all the database categories. The codes implementation of the proposed IrisGuideNet will be available at: 
<a href="https://github.com/mohdjawadi/IrisGuidenet" target="_blank" rel="noopener">https://github.com/mohdjawadi/IrisGuidenet</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/10105641" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/jawad-tifs-2023-1/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/mohdjawadi/IrisGuidenet" target="_blank" rel="noopener">
  Code
</a>






















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2023.3268504" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/tianhao-lu/">Tianhao Lu</a></span>, <span ><a href="/author/gaosheng-wu/">Gaosheng Wu</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2022 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/caiyong-ijcb-2022/">
      <img src="/publication/caiyong-ijcb-2022/featured_hu26cc22a4bb0f14968b6f10e3262787d3_387094_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="D-ESRGAN: A Dual-Encoder GAN with Residual CNN and Vision Transformer for Iris Image Super-Resolution">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/caiyong-ijcb-2022/">D-ESRGAN: A Dual-Encoder GAN with Residual CNN and Vision Transformer for Iris Image Super-Resolution</a>
  </h3>

  
  <a href="/publication/caiyong-ijcb-2022/" class="summary-link">
    <div class="article-style">
      <p>Iris images captured in less-constrained environments, especially at long distances often suffer from the interference of low resolution, resulting in the loss of much valid iris texture information for iris recognition. In this paper, we propose a dual-encoder super-resolution generative adversarial network (D-ESRGAN) for compensating texture lost of the raw image meanwhile maintaining the newly generated textures more natural. Specifically, the proposed D-ESRGAN not only integrates the residual CNN encoder to extract local features, but also employs an emerging vision transformer encoder to capture global associative information. The local and global features from two encoders are further fused for the subsequent reconstruction of high-resolution features. During the training, we develop a three-stage strategy to alleviate the problem that generative adversarial networks are prone to collapse. Moreover, to boost the iris recognition performance, we introduce a triplet loss to push away the distance of super-resolved iris images with different IDs, and pull the distance of super-resolved iris images with the same ID much closer. Experimental results on the public CASIA-Iris-distance and CASIA-Iris-M1 datasets show that D-ESRGAN archives better performance than state-of-the-art baselines in terms of both super-resolution image quality metrics and iris recognition metric.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10007938" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/caiyong-ijcb-2022/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.idealtest.org/dbDetailForUser.do?id=4#/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB54206.2022.10007938" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/shubo-zhou/">Shubo Zhou</a></span>, <span ><a href="/author/liang-hu/">Liang Hu</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/xue-qin-jiang/">Xue-Qin Jiang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Circuits and Systems for Video Technology ( Early Access )</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zhow-tcsvt-2023/">
      <img src="/publication/zhow-tcsvt-2023/featured_hucf170d5d591dd9ccda77799ec1902e24_451849_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zhow-tcsvt-2023/">AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus</a>
  </h3>

  
  <a href="/publication/zhow-tcsvt-2023/" class="summary-link">
    <div class="article-style">
      <p>As an aperture-divided computational imaging system, microlens array (MLA) -based light field (LF) imaging is playing an increasingly important role in computer vision. As the trade-off between the spatial and angular resolutions, deep learning (DL) -based image super-resolution (SR) methods have been applied to enhance the spatial resolution. However, in existing DL-based methods, the depth-varying defocus is not considered both in dataset development and algorithm design, which restricts many applications such as depth estimation and object recognition. To overcome this shortcoming, a super-resolution task that reconstructs all-in-focus high-resolution (HR) LF images from low-resolution (LR) LF images is proposed by designing a large dataset and proposing a convolutional neural network (CNN) -based SR method. The dataset is constructed by using Blender software, consisting of 150 light field images used as training data, and 15 light field images used as validation and testing data. The proposed network is designed by proposing the dilated deformable convolutional network (DCN) -based feature extraction block and the LF subaperture image (SAI) Deblur-SR block. The experimental results demonstrate that the proposed method achieves more appealing results both quantitatively and qualitatively.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10018388" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zhow-tcsvt-2023/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/qingpu1988/AllfocusNet" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/qingpu1988/AllfocusNet" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TCSVT.2023.3237593" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/mengmeng-cui/">Mengmeng Cui</a></span>, <span ><a href="/author/wei-wang/">Wei Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/liang-wang/">Liang Wang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2022 IEEE Transactions on Image Processing (TIP)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/kunbo-itip-2022/">
      <img src="/publication/kunbo-itip-2022/featured_hu2ae8d9e4826771bfe51fd36c98f90159_196004_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Pose-Appearance Relational Modeling for Video Action Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/kunbo-itip-2022/">Pose-Appearance Relational Modeling for Video Action Recognition</a>
  </h3>

  
  <a href="/publication/kunbo-itip-2022/" class="summary-link">
    <div class="article-style">
      <p>Recent studies of video action recognition can be classified into two categories: the appearance-based methods and the pose-based methods. The appearance-based methods generally cannot model temporal dynamics of large motion well by virtue of optical flow estimation, while the pose-based methods ignore the visual context information such as typical scenes and objects, which are also important cues for action understanding. In this paper, we tackle these problems by proposing a Pose-Appearance Relational Network (PARNet), which models the correlation between human pose and image appearance, and combines the benefits of these two modalities to improve the robustness towards unconstrained real-world videos. There are three network streams in our model, namely pose stream, appearance stream and relation stream. For the pose stream, a Temporal Multi-Pose RNN module is constructed to obtain the dynamic representations through temporal modeling of 2D poses. For the appearance stream, a Spatial Appearance CNN module is employed to extract the global appearance representation of the video sequence. For the relation stream, a Pose-Aware RNN module is built to connect pose and appearance streams by modeling action-sensitive visual context information. Through jointly optimizing the three modules, PARNet achieves superior performances compared with the state-of-the-arts on both the pose-complete datasets (KTH, Penn-Action, UCF11) and the challenging pose-incomplete datasets (UCF101, HMDB51, JHMDB), demonstrating its robustness towards complex environments and noisy skeletons. Its effectiveness on NTU-RGBD dataset is also validated even compared with 3D skeleton-based methods. Furthermore, an appearance-enhanced PARNet equipped with a RGB-based I3D stream is proposed, which outperforms the Kinetics pre-trained competitors on UCF101 and HMDB51. The better experimental results verify the potentials of our framework by integrating various modules.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/9986038" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/kunbo-itip-2022/cite.bib">
  Cite
</button>























<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIP.2022.3228156" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/huaibo-huang/">Huaibo Huang</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/xingyu-gao/">Xingyu Gao</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 18)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wei-tifs-2022/">
      <img src="/publication/wei-tifs-2022/featured_hu7acd92694d5d398e2d3b543ae6843b2a_265921_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Contextual Measures for Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wei-tifs-2022/">Contextual Measures for Iris Recognition</a>
  </h3>

  
  <a href="/publication/wei-tifs-2022/" class="summary-link">
    <div class="article-style">
      <p>The iris patterns of the human contain a large amount of randomly distributed and irregularly shaped microstructures. These microstructures make the human iris informative biometric traits. To learn identity representation from them, this paper regards each iris region as a potential microstructure and proposes contextual measures (CM) to model the correlations between them. CM adopts two parallel branches to learn global and local contexts in iris image. The first one is the globally contextual measure branch. It measures the global context involving the relationships between all regions for feature aggregation and is robust to local occlusions. Besides, we improve its spatial perception considering the positional randomness of the microstructures. The other one is the locally contextual measure branch. This branch considers the role of local details in the phenotypic distinctiveness of iris patterns and learns a series of relationship atoms to capture contextual information from a local perspective. In addition, we develop the perturbation bottleneck to make sure that the two branches learn divergent contexts. It introduces perturbation to limit the information flow from input images to identity features, forcing CM to learn discriminative contextual information for iris recognition. Experimental results suggest that global and local contexts are two different clues critical for accurate iris recognition. The superior performance on four benchmark iris datasets demonstrates the effectiveness of the proposed approach in within-database and cross-database scenarios.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9947055" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wei-tifs-2022/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/reborn20200813/Contextual-Measures" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2022.3221897" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      *Chinese Conference on Biometric Recognition(CCBR 2022) *
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/jawad-ccbr-2022/">
      <img src="/publication/jawad-ccbr-2022/featured_hu181483092694761bece1521f7742e05c_362008_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="An Empirical Comparative Analysis of Africans with Asians Using DCNN Facial Biometric Models">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/jawad-ccbr-2022/">An Empirical Comparative Analysis of Africans with Asians Using DCNN Facial Biometric Models</a>
  </h3>

  
  <a href="/publication/jawad-ccbr-2022/" class="summary-link">
    <div class="article-style">
      <p>Recently, the problem of racial bias in facial biometric systems has generated considerable attention from the media and biometric community. Many investigative studies have been published on estimating the bias between Caucasians and Asians, Caucasians and Africans, and other racial comparisons. These studies have reported inferior performances of both Asians and Africans when compared to other races. However, very few studies have highlighted the comparative differences in performance as a function of race between Africans and Asians. More so, those previous studies were mainly concentrated on a single aspect of facial biometrics and were usually conducted with images potentially captured with multiple camera sensors, thereby compounding their findings. This paper presents a comparative racial bias study of Asians with Africans on various facial biometric tasks. The images used were captured with the same camera sensor and under controlled conditions. We examine the performances of many DCNN-based models on face detection, facial landmark detection, quality assessment, verification, and identification. The results suggested higher performance on the Asians compared to the Africans by most algorithms under the same imaging and testing conditions.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/978-3-031-20233-9_14" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/jawad-ccbr-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/978-3-031-20233-9_14" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/matej-vitek/">Matej Vitek</a></span>, <span ><a href="/author/abhijit-das/">Abhijit Das</a></span>, <span ><a href="/author/diego-rafael-lucio/">Diego Rafael Lucio</a></span>, <span ><a href="/author/et.al/">Et.al</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 18)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/matej-tifs-2022/">
      <img src="/publication/matej-tifs-2022/featured_hu6994cc1c986cd9fbc5a31436214818ea_735998_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Exploring Bias in Sclera Segmentation Models: A Group Evaluation Approach">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/matej-tifs-2022/">Exploring Bias in Sclera Segmentation Models: A Group Evaluation Approach</a>
  </h3>

  
  <a href="/publication/matej-tifs-2022/" class="summary-link">
    <div class="article-style">
      <p>Bias and fairness of biometric algorithms have been key topics of research in recent years, mainly due to the societal, legal and ethical implications of potentially unfair decisions made by automated decision-making models. A considerable amount of work has been done on this topic across different biometric modalities, aiming at better understanding the main sources of algorithmic bias or devising mitigation measures. In this work, we contribute to these efforts and present the first study investigating bias and fairness of sclera segmentation models. Although sclera segmentation techniques represent a key component of sclera-based biometric systems with a considerable impact on the overall recognition performance, the presence of different types of biases in sclera segmentation methods is still underexplored. To address this limitation, we describe the results of a group evaluation effort (involving seven research groups), organized to explore the performance of recent sclera segmentation models within a common experimental framework and study performance differences (and bias), originating from various demographic as well as environmental factors. Using five diverse datasets, we analyze seven independently developed sclera segmentation models in different experimental configurations. The results of our experiments suggest that there are significant differences in the overall segmentation performance across the seven models and that among the considered factors, ethnicity appears to be the biggest cause of bias. Additionally, we observe that training with representative and balanced data does not necessarily lead to less biased results. Finally, we find that in general there appears to be a negative correlation between the amount of bias observed (due to eye color, ethnicity and acquisition device) and the overall segmentation performance, suggesting that advances in the field of semantic segmentation may also help with mitigating bias.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9926136" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/matej-tifs-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2022.3216468" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/mupei-li/">Mupei Li</a></span>, <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2022 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-ijcb-2022/">
      <img src="/publication/wang-ijcb-2022/featured_huc84d0573213adc0e3df3b7636c4d50fb_189116_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="PDVN: A Patch-based Dual-view Network for Face Liveness Detection using Light Field Focal Stack">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-ijcb-2022/">PDVN: A Patch-based Dual-view Network for Face Liveness Detection using Light Field Focal Stack</a>
  </h3>

  
  <a href="/publication/wang-ijcb-2022/" class="summary-link">
    <div class="article-style">
      <p>Light Field Focal Stack (LFFS) can be efficiently rendered from a light field (LF) image captured by plenoptic cameras. Differences in the 3D surface and texture of biometric samples are internally reflected in the defocus blur and local patterns between the rendered slices of LFFS. This unique property makes LFFS quite appropriate to differentiate presentation attack instruments (PAIs) from bona fide samples. A patch-based dual-view network (PDVN) is proposed in this paper to leverage the merits of LFFS for face presentation attack detection (PAD). First, original LFFS data are divided into various local patches along spatial dimensions, which distracts the model from learning the useless facial semantics and greatly relieve the problem of insufficient samples. The strategy of dual-view branches is innovatively proposed, wherein the original view and microscopic view can simultaneously contribute to liveness detection. Separable 3D convolution on the focal dimension is verified to be more effective than vanilla 3D convolution for extracting discriminative features from LFFS data. The voting mechanism on predictions of patch LFFS samples further strengthens the robustness of the proposed framework. PDVN is compared with other face PAD methods on IST LLFFSD dataset and achieves perfect performance, i.e., ACER drops to 0.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10007998" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-ijcb-2022/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/lmp1265/PDVN-project-for-light-field-liveness-detection.git" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://wylcasia.github.io/papers/IJCB2022-2-poster.pdf" target="_blank" rel="noopener">
  Poster
</a>

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB54206.2022.10007998" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/nianfeng-liu/">Nianfeng Liu</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IET Biometrics 11.5 (2022)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/luo-iet-2022/">
      <img src="/publication/luo-iet-2022/featured_hu14e5d4a37b49e9bcd998978c477d7e2b_586992_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Combining 2D texture and 3D geometry features for Reliable iris presentation attack detection using light field focal stack">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/luo-iet-2022/">Combining 2D texture and 3D geometry features for Reliable iris presentation attack detection using light field focal stack</a>
  </h3>

  
  <a href="/publication/luo-iet-2022/" class="summary-link">
    <div class="article-style">
      <p>Iris presentation attack detection (PAD) is still an unsolved problem mainly due to the various spoof attack strategies and poor generalisation on unseen attackers. In this paper, the merits of both light field (LF) imaging and deep learning (DL) are leveraged to combine 2D texture and 3D geometry features for iris liveness detection. By exploring off-the-shelf deep features of planar-oriented and sequence-oriented deep neural networks (DNNs) on the rendered focal stack, the proposed framework excavates the differences in 3D geometric structure and 2D spatial texture between bona fide and spoofing irises captured by LF cameras. A group of pre-trained DL models are adopted as feature extractor and the parameters of SVM classifiers are optimised on a limited number of samples. Moreover, two-branch feature fusion further strengthens the framework&rsquo;s robustness and reliability against severe motion blur, noise, and other degradation factors. The results of comparative experiments indicate that variants of the proposed framework significantly surpass the PAD methods that take 2D planar images or LF focal stack as input, even recent state-of-the-art (SOTA) methods fined-tuned on the adopted database. Presentation attacks, including printed papers, printed photos, and electronic displays, can be accurately detected without fine-tuning a bulky CNN. In addition, ablation studies validate the effectiveness of fusing geometric structure and spatial texture features. The results of multi-class attack detection experiments also verify the good generalisation ability of the proposed framework on unseen presentation attacks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/bme2.12092" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/luo-iet-2022/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/luozhengquan/LFLD" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cripacsir.cn/dataset/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1049/bme2.12092" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yuhao-zhu/">Yuhao Zhu</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security( Volume: 17)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-tifs-2022/">
      <img src="/publication/ren-tifs-2022/featured_hud869c0865b54296dd31b75fd6a0ac5ab_69225_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Perturbation Inactivation Based Adversarial Defense for Face Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-tifs-2022/">Perturbation Inactivation Based Adversarial Defense for Face Recognition</a>
  </h3>

  
  <a href="/publication/ren-tifs-2022/" class="summary-link">
    <div class="article-style">
      <p>Deep learning-based face recognition models are vulnerable to adversarial attacks. To curb these attacks, most defense methods aim to improve the robustness of recognition models against adversarial perturbations. However, the generalization capacities of these methods are quite limited. In practice, they are still vulnerable to unseen adversarial attacks. Deep learning models are fairly robust to general perturbations, such as Gaussian noises. A straightforward approach is to inactivate the adversarial perturbations so that they can be easily handled as general perturbations. In this paper, a plug-and-play adversarial defense method, named perturbation inactivation (PIN), is proposed to inactivate adversarial perturbations for adversarial defense. We discover that the perturbations in different subspaces have different influences on the recognition model. There should be a subspace, called the immune space, in which the perturbations have fewer adverse impacts on the recognition model than in other subspaces. Hence, our method estimates the immune space and inactivates the adversarial perturbations by restricting them to this subspace. The proposed method can be generalized to unseen adversarial perturbations since it does not rely on a specific kind of adversarial attack method. This approach not only outperforms several state-of-the-art adversarial defense methods but also demonstrates a superior generalization capacity through exhaustive experiments. Moreover, the proposed method can be successfully applied to four commercial APIs without additional training, indicating that it can be easily generalized to existing face recognition systems.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9845464" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-tifs-2022/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/RenMin1991/Perturbation-Inactivate" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2022.3195384" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2022</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/luo-cvprw-2022/">
      <img src="/publication/luo-cvprw-2022/featured_hu2139e86f5b6638ee295457c3a1438d14_74298_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="FedIris: Towards More Accurate and Privacy-Preserving Iris Recognition via Federated Template Communication">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/luo-cvprw-2022/">FedIris: Towards More Accurate and Privacy-Preserving Iris Recognition via Federated Template Communication</a>
  </h3>

  
  <a href="/publication/luo-cvprw-2022/" class="summary-link">
    <div class="article-style">
      <p>As biometric data undergo rapidly growing privacy concerns, building large-scale datasets has become more difficult. Unfortunately, current iris databases are mostly in small scale, e.g., thousands of iris images from hundreds of identities. What&rsquo;s worse, the heterogeneity among decentralized iris datasets hinders the current deep learning (DL) frameworks from obtaining recognition performance with robust generalization. It motivates us to leverage the merits of federated learning (FL) to solve these problems. However, traditional FL algorithms often employ model sharing for knowledge transfer, wherein the simple averaging aggregation lacks interpretability, and divergent optimization directions of clients lead to performance degradation. To overcome this interference, we propose FedIris with solid theoretical foundations, which attempts to employ the iris template as the communication carrier and formulate federated triplet (Fed-Triplet) for knowledge transfer. Furthermore, the massive heterogeneity among iris datasets may induce negative transfer and unstable optimization. The modified Wasserstein distance is embedded into the FedTriplet loss to reweight global aggregation, which drives the clients with similar data distributions to contribute more mutually. Extensive experimental results demonstrate that the proposed FedIris outperforms SOLO training, model-sharing-based FL training, and even centralized training.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Luo_FedIris_Towards_More_Accurate_and_Privacy-Preserving_Iris_Recognition_via_Federated_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/luo-cvprw-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/luo-cvprw-2022/CVPRW2022_FedIris.pptx" target="_blank" rel="noopener">
  Slides
</a>

















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yong-he/">Yong He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/yuhao-zhu/">Yuhao Zhu</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <ul>
<li>ã€Šè®¡ç®—æœºåº”ç”¨ä¸Žè½¯ä»¶ã€‹2022å¹´ç¬¬4æœŸ*</li>
</ul>

    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/he-cas-2022/">
      <img src="/publication/he-cas-2022/featured_hud68813ff569c284c34fb563c95da1c6f_125475_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="EYE LOCATION AND STATE ESTIMATION BASED ON LANDMARKS">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/he-cas-2022/">EYE LOCATION AND STATE ESTIMATION BASED ON LANDMARKS</a>
  </h3>

  
  <a href="/publication/he-cas-2022/" class="summary-link">
    <div class="article-style">
      <p>Eye location and state estimation are key steps in the preprocessing of biometrics recognition such as iris, sclera and periocular. Eye images captured in the non-cooperative environments often suffer from serious occlusions and complex backgrounds. To solve this problem, this paper proposes a robust and accurate single-stage framework based on eye landmarks to detect eye key points and estimate the left, right and open and closed states of eyes. In order to train and evaluate the proposed model, a new OCE-1000 dataset was created and manually labeled with eight key points, open and close state for left and right eyes of each image. Experimental results show that the proposed model achieves 98% accuracy of landmark location and 97% accuracy of eye state estimation on OCE-1000 dataset.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2022&amp;filename=JYRJ202204029&amp;uniplatform=NZKPT&amp;v=Ik-EGuNOCdZmVpb6PiDWGWbI2gE6b3Jwd96F5wORyLXFwZzmLinXuv78PmSX_pbh" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/he-cas-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the 39 th International Conference on Machine Learning (ICML 2022)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/luo-icml-2022/">
      <img src="/publication/luo-icml-2022/featured_huef6251e865d48097aa44da5adc7c5b02_354791_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/luo-icml-2022/">Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring</a>
  </h3>

  
  <a href="/publication/luo-icml-2022/" class="summary-link">
    <div class="article-style">
      <p>Attributes skew hinders the current federated learning (FL) frameworks from consistent optimization directions among the clients, which inevitably leads to performance reduction and unstable convergence. The core problems lie in that: 1) Domain-specific attributes, which are non-causal and only locally valid, are indeliberately mixed into global aggregation. 2) The one-stage optimizations of entangled attributes cannot simultaneously satisfy two conflicting objectives, i.e., generalization and personalization. To cope with these, we proposed disentangled federated learning (DFL) to disentangle the domain-specific and cross-invariant attributes into two complementary branches, which are trained by the proposed alternating local-global optimization independently. Importantly, convergence analysis proves that the FL system can be stably converged even if incomplete client models participate in the global aggregation, which greatly expands the application scope of FL. Extensive experiments verify that DFL facilitates FL with higher performance, better interpretability, and faster convergence rate, compared with SOTA FL methods on both manually synthesized and realistic attributes skew datasets.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2206.06818" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/luo-icml-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.48550/arXiv.2206.06818" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Machine Intelligence Research volume 19, 2022</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-mir-2022/">
      <img src="/publication/ren-mir-2022/featured_hubc4e2dab0ba57f39c7afeacb8745506f_352909_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Towards Interpretable Defense Against Adversarial Attacks via Causal Inference">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-mir-2022/">Towards Interpretable Defense Against Adversarial Attacks via Causal Inference</a>
  </h3>

  
  <a href="/publication/ren-mir-2022/" class="summary-link">
    <div class="article-style">
      <p>Deep learning-based models are vulnerable to adversarial attacks. Defense against adversarial attacks is essential for sensitive and safety-critical scenarios. However, deep learning methods still lack effective and efficient defense mechanisms against adversarial attacks. Most of the existing methods are just stopgaps for specific adversarial samples. The main obstacle is that how adversarial samples fool the deep learning models is still unclear. The underlying working mechanism of adversarial samples has not been well explored, and it is the bottleneck of adversarial attack defense. In this paper, we build a causal model to interpret the generation and performance of adversarial samples. The self-attention/transformer is adopted as a powerful tool in this causal model. Compared to existing methods, causality enables us to analyze adversarial samples more naturally and intrinsically. Based on this causal model, the working mechanism of adversarial samples is revealed, and instructive analysis is provided. Then, we propose simple and effective adversarial sample detection and recognition methods according to the revealed working mechanism. The causal insights enable us to detect and recognize adversarial samples without any extra model or training. Extensive experiments are conducted to demonstrate the effectiveness of the proposed methods. Our methods outperform the state-of-the-art defense methods under various adversarial attacks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/article/10.1007/s11633-022-1330-7" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-mir-2022/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/s11633-022-1330-7" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/%E4%BD%95%E5%8B%87/">ä½•å‹‡</a></span>, <span ><a href="/author/%E5%AD%99%E5%93%B2%E5%8D%97/">å­™å“²å—</a></span>, <span ><a href="/author/%E7%8E%8B%E8%B4%A2%E5%8B%87/">çŽ‹è´¢å‹‡</a></span>, <span ><a href="/author/%E7%8E%8B%E4%BA%91%E9%BE%99/">çŽ‹äº‘é¾™</a></span>, <span ><a href="/author/%E6%9C%B1%E5%AE%87%E8%B1%AA/">æœ±å®‡è±ª</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2022 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/heyong-caas-2022-new/">
      <img src="/publication/heyong-caas-2022-new/featured_hu474a8c99839954b04c3bb74f56f126d1_63113_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="åŸºäºŽå…³é”®ç‚¹çš„çœ¼ç›å®šä½å’ŒçŠ¶æ€ä¼°è®¡">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/heyong-caas-2022-new/">åŸºäºŽå…³é”®ç‚¹çš„çœ¼ç›å®šä½å’ŒçŠ¶æ€ä¼°è®¡</a>
  </h3>

  
  <a href="/publication/heyong-caas-2022-new/" class="summary-link">
    <div class="article-style">
      <p>çœ¼ç›å®šä½å’ŒçŠ¶æ€ä¼°è®¡æ˜¯è™¹è†œã€å·©è†œã€çœ¼å‘¨ç­‰ç”Ÿç‰©ç‰¹å¾è¯†åˆ«ä¸­é‡è¦çš„é¢„å¤„ç†è¿‡ç¨‹ã€‚éžåˆä½œçŽ¯å¢ƒä¸‹æ•èŽ·çš„ çœ¼ç›å›¾åƒç»å¸¸é¢ä¸´ä¸¥é‡çš„é®æŒ¡å’Œå¤æ‚çš„èƒŒæ™¯ã€‚ä¸ºæ­¤æå‡ºä¸€ç§é²æ£’è€Œå‡†ç¡®çš„åŸºäºŽçœ¼ç›å…³é”®ç‚¹çš„å•é˜¶æ®µæ–¹æ³•åŽ»å®šä½çœ¼ç›çš„ä½ç½®å¹¶ä¼°è®¡çœ¼ç›çš„å·¦å³å’Œå¼€é—­çŠ¶æ€ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æå‡ºçš„æ¨¡åž‹ï¼Œæ‰‹å·¥æ ‡æ³¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†OCE-1000ï¼Œæ¯å¹…å›¾åƒæ ‡æ³¨å·¦å³ä¸¤åªçœ¼ç›å…± ï¼˜ä¸ªå…³é”®ç‚¹ï¼Œä»¥åŠå·¦å³çœ¼çš„å¼€é—­çŠ¶æ€ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæå‡ºçš„æ¨¡åž‹åœ¨OCE-1000æ•°æ®é›†ä¸Šè¾¾åˆ°äº†98ï¼…çš„å…³é”®ç‚¹å®šä½å‡†ç¡®çŽ‡ï¼Œçœ¼ç›çŠ¶æ€ä¼°è®¡çš„å‡†ç¡®çŽ‡ä¸º97ï¼…ã€‚</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://kns.cnki.net/KXReader/Detail?invoice=IJ9ayHNH3mTsBLeWeQ2nBPFpBohDSUw4y502yDN3p2W%2FsB6d0mBPz3hpqqIZ4emRrQj5hTzi4H%2BHJzbWySoRAe9KKc1bwPbfFHZ73PuB%2FWKcS78uc2e5SsI9r7K1myNTxqFdW3HZv%2F4YZTnhLA5EqWrYfC7HBm%2FSkEFYL6vXRNs%3D&amp;DBCODE=CJFD&amp;FileName=JYRJ202204029&amp;TABLEName=cjfdlast2022&amp;nonce=9325586410534000850FFD74CD4A4CF5&amp;uid=&amp;TIMESTAMP=1667543903248" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/heyong-caas-2022-new/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/periocular-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/huaibo-huang/">Huaibo Huang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 17)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wei-tifs-2022_2/">
      <img src="/publication/wei-tifs-2022_2/featured_hu0305b6a52acfa099ede95ec984e671e7_262122_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wei-tifs-2022_2/">Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors</a>
  </h3>

  
  <a href="/publication/wei-tifs-2022_2/" class="summary-link">
    <div class="article-style">
      <p>The uncontrollable acquisition process limits the performance of iris recognition. In the acquisition process, various inevitable factors, including eyes, devices, and environment, hinder the iris recognition system from learning a discriminative identity representation. This leads to severe performance degradation. In this paper, we explore uncertain acquisition factors and propose uncertainty embedding (UE) and uncertainty-guided curriculum learning (UGCL) to mitigate the influence of acquisition factors. UE represents an iris image using a probabilistic distribution rather than a deterministic point (binary template or feature vector) that is widely adopted in iris recognition methods. Specifically, UE learns identity and uncertainty features from the input image, and encodes them as two independent components of the distribution, mean and variance. Based on this representation, an input image can be regarded as an instantiated feature sampled from the UE, and we can also generate various virtual features through sampling. UGCL is constructed by imitating the progressive learning process of newborns. Particularly, it selects virtual features to train the model in an easy-to-hard order at different training stages according to their uncertainty. In addition, an instance-level enhancement method is developed by utilizing local and global statistics to mitigate the data uncertainty from image noise and acquisition conditions in the pixel-level space. The experimental results on six benchmark iris datasets verify the effectiveness and generalization ability of the proposed method on same-sensor and cross-sensor recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9722888" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wei-tifs-2022_2/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/reborn20200813/uncertainty" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2022.3154240" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/tianhao-lu/">Tianhao Lu</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Electronic Imaging (February 2022).</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/lu-jei-2022/">
      <img src="/publication/lu-jei-2022/featured_huec0ad3cf33e61e3925f12b83b81e42ed_108182_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Multitask deep active contour-based iris segmentation for off-angle iris images">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/lu-jei-2022/">Multitask deep active contour-based iris segmentation for off-angle iris images</a>
  </h3>

  
  <a href="/publication/lu-jei-2022/" class="summary-link">
    <div class="article-style">
      <p>Iris recognition has been considered as a secure and reliable biometric technology. However, iris images are prone to off-angle or are partially occluded when captured with fewer user cooperations. As a consequence, iris recognition especially iris segmentation suffers a serious performance drop. To solve this problem, we propose a multitask deep active contour model for off-angle iris image segmentation. Specifically, the proposed approach combines the coarse and fine localization results. The coarse localization detects the approximate position of the iris area and further initializes the iris contours through a series of robust preprocessing operations. Then, iris contours are represented by 40 ordered isometric sampling polar points and thus their corresponding offset vectors are regressed via a convolutional neural network for multiple times to obtain the precise inner and outer boundaries of the iris. Next, the predicted iris boundary results are regarded as a constraint to limit the segmentation range of noise-free iris mask. Besides, an efficient channel attention module is introduced in the mask prediction to make the network focus on the valid iris region. A differentiable, fast, and efficient SoftPool operation is also used in place of traditional pooling to keep more details for more accurate pixel classification. Finally, the proposed iris segmentation approach is combined with off-the-shelf iris feature extraction models including traditional OM and deep learning-based FeatNet for iris recognition. The experimental results on two NIR datasets CASIA-Iris-off-angle, CASIA-Iris-Africa, and a VIS dataset SBVPI show that the proposed approach achieves a significant performance improvement in the segmentation and recognition for both regular and off-angle iris images.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-31/issue-4/041211/Multitask-deep-active-contour-based-iris-segmentation-for-off-angle/10.1117/1.JEI.31.4.041211.short?SSO=1&amp;tab=ArticleLink" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/lu-jei-2022/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/lutianhao/IrisGazeSeg" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1117/1.JEI.31.4.041211" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/lingxiao-he/">Lingxiao He</a></span>, <span ><a href="/author/xingyu-liao/">Xingyu Liao</a></span>, <span ><a href="/author/wu-liu/">Wu Liu</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>CVF International Conference on Computer Vision (ICCV) 2021 (ICCV2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-iccv-2021/">
      <img src="/publication/ren-iccv-2021/featured_hu9b621205828ce8b0bc8bcd5ebeea1374_93219_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Learning Instance-level Spatial-Temporal Patterns for Person Re-identification">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-iccv-2021/">Learning Instance-level Spatial-Temporal Patterns for Person Re-identification</a>
  </h3>

  
  <a href="/publication/ren-iccv-2021/" class="summary-link">
    <div class="article-style">
      <p>Person re-identification (Re-ID) aims to match pedestrians under dis-joint cameras. Most Re-ID methods formulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space. Spatial-temporal information has been proven to be efficient to filter irrelevant negative samples and significantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufficiently. In this paper, we propose a novel instance-level and spatial-temporal disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, personalized information such as moving direction is explicitly considered to further narrow down the search space. Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribution, so that outliers can also be well modeled. Abundant experimental analyses on two datasets are presented, which demonstrates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively. Besides, in order to provide a better benchmark for person re-identification, we release a cleaned data list of DukeMTMC-reID with this paper: 
<a href="https://github.com/RenMin1991/cleaned-DukeMTMC-reID" target="_blank" rel="noopener">https://github.com/RenMin1991/cleaned-DukeMTMC-reID</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2108.00171" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-iccv-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/yi-li/">Yi Li</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <ul>
<li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)*</li>
</ul>

    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wei-tcsvt-2021/">
      <img src="/publication/wei-tcsvt-2021/featured_hu27fd3dd24f2f86ee68cf5d123df6d4c5_103717_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Cross-spectral Iris Recognition by Learning Device-specific Band">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wei-tcsvt-2021/">Cross-spectral Iris Recognition by Learning Device-specific Band</a>
  </h3>

  
  <a href="/publication/wei-tcsvt-2021/" class="summary-link">
    <div class="article-style">
      <p>Cross-spectral recognition is still an open challenge in iris recognition. In cross-spectral iris recognition, there exist distinct device-specific bands between near-infrared (NIR) and visible (VIS) images, resulting in the distribution gap between samples from different spectra and thus severe degradation in recognition performance. To tackle this problem, we propose a new cross-spectral iris recognition method to learn spectral-invariant features by estimating device-specific bands. In the proposed method, Gabor Trident Network (GTN) first utilizes the Gabor functionâ€™s priors to perceive iris textures under different spectra, and then codes the device-specific band as the residual component to assist the generation of spectral-invariant features. By investigating the device-specific band, GTN effectively reduces the impact of device-specific bands on identity features. Besides, we make three efforts to further reduce the distribution gap. First, Spectral Adversarial Network (SAN) adopts a class-level adversarial strategy to align feature distributions. Second, Sample-Anchor (SA) loss upgrades triplet loss by pulling samples to their class center and pushing away from other class centers. Third, we develop a higher-order alignment loss to measures the distribution gap according to space bases and distribution shapes. Extensive experiments on five iris datasets demonstrate the efficacy of our proposed method for cross-spectral iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9557317" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wei-tcsvt-2021/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/weijianze/CSINv2" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TCSVT.2021.3117291" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/haiqing-li/">Haiqing Li</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Chinese Conference on Biometric Recognition 2021 (CCBR2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/luo-ccbr-2021/">
      <img src="/publication/luo-ccbr-2021/featured_hu742e3d2d6b65ebc2f6f53555ca73af42_137747_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Iris Normalization Beyond Appr-Circular Parameter Estimation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/luo-ccbr-2021/">Iris Normalization Beyond Appr-Circular Parameter Estimation</a>
  </h3>

  
  <a href="/publication/luo-ccbr-2021/" class="summary-link">
    <div class="article-style">
      <p>The requirement to recognize the iris image of low-quality is rapidly increasing with the practical application of iris recognition, especially the urgent need for high-throughput or applications in covert situations. The appr-circle fitting can not meet the needs due to the high time cost and non-accurate boundary estimation during the normalization process. Furthermore, the appr-circular hypothesis of iris and pupil is not entirely established due to the squint and occlusion in non-cooperative environments. To mitigate this problem, a multi-mask normalization without appr-circular parameter estimation is proposed to make full use of the segmented masks, which provide robust pixel-level iris boundaries. It bridges the segmentation and feature extraction to recognize the low-quality iris, which is thrown directly by the traditional methods. Thus, the complex samples with no appr-circular iris or massive occlusions can be recognized correctly. The extensive experiments are conducted on the representative and challenging databases to verify the generalization and the accuracy of the proposed iris normalization method. Besides, the throughput rate is significantly improved.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-030-86608-2_35" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/luo-ccbr-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1007/978-3-030-86608-2_35" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zihui-yan/">Zihui Yan</a></span>, <span ><a href="/author/lingxiao-he/">Lingxiao He</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Biometrics, Behavior, and Identity Science(TBIOM)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zihui-yan-tbiom-2021/">
      <img src="/publication/zihui-yan-tbiom-2021/featured_huc8c1200ac3b6675ff9a82a81398570bf_125674_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Flexible Iris Matching Based on Spatial Feature Reconstruction">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zihui-yan-tbiom-2021/">Flexible Iris Matching Based on Spatial Feature Reconstruction</a>
  </h3>

  
  <a href="/publication/zihui-yan-tbiom-2021/" class="summary-link">
    <div class="article-style">
      <p>In an iris recognition-at-a-distance (IAAD) system used in surveillance scenarios, the camera usually captures a large number of low-quality images. These images exhibit partial occlusions due to eyelids and eyelashes, specular reflections, and severe deformations caused by pupil dilations and contractions. Recognizing these low-quality images is a challenging yet dominant problem in IAAD. To mitigate this issue, current iris recognition systems mostly filter out low-quality images by using strict criteria based on image quality evaluation. This strategy, however, wastes device capabilities and produces low throughput of subjects. Other systems require highly cooperative users. In this work, we propose a novel occlusion-robust, deformation-robust, and alignment-free framework for low-quality iris matching, which integrates the merits of deep features and sparse representation in an end-to-end learning process known as iris spatial feature reconstruction (ISFR). Here each probe image can be sparsely reconstructed on the basis of appropriate feature maps from gallery high-quality images. ISFR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two iris images, which naturally avoids the time-consuming alignment step. In summary, the distinctiveness of deep features, the robustness of sparse reconstruction, and the flexibility of multiscale matching strategy are unified in a general framework to attain more accurate and reasonable iris matching. Extensive experimental results on four public iris image databases demonstrate that the proposed method significantly outperforms both traditional and deep learning-based iris recognition methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9524810" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zihui-yan-tbiom-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TBIOM.2021.3108559" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/zhengquan-luo/">Zhengquan Luo</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2021 (IJCB2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hu-ijcb-2021/">
      <img src="/publication/hu-ijcb-2021/featured_hu4cd597d88fce49e8401c97159f357c56_376298_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A Large-scale Database for Less Cooperative Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hu-ijcb-2021/">A Large-scale Database for Less Cooperative Iris Recognition</a>
  </h3>

  
  <a href="/publication/hu-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This paper presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9484357" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hu-ijcb-2021/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cripacsir.cn/dataset/casia-iris-degradation/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484357" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/tianhao-lu/">Tianhao Lu</a></span>, <span ><a href="/author/qi-zhang/">Qi Zhang</a></span>, <span ><a href="/author/qichuan-tian/">Qichuan Tian</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2021 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/kunbo-itifs-2023-ne/">
      <img src="/publication/kunbo-itifs-2023-ne/featured_huf970873a01bd873be3cdada0c826e820_235789_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/kunbo-itifs-2023-ne/">NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization</a>
  </h3>

  
  <a href="/publication/kunbo-itifs-2023-ne/" class="summary-link">
    <div class="article-style">
      <p>For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and also been introduced in iris biometrics, especially iris segmentation. To investigate recent developments and attract more interest of researchers in the iris segmentation method, we organized the 2021 NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021) at the 2021 International Joint Conference on Biometrics (IJCB 2021). The challenge was used as a public platform to assess the performance of iris segmentation and localization methods on Asian and African NIR iris images captured in non-cooperative environments. The three best-performing entries achieved solid and satisfactory iris segmentation and localization results in most cases, and their code and models have been made publicly available for reproducibility research.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9484336" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/kunbo-itifs-2023-ne/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cripacsir.cn/dataset/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484336" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/tianhao-lu/">Tianhao Lu</a></span>, <span ><a href="/author/qi-zhang/">Qi Zhang</a></span>, <span ><a href="/author/qichuan-tian/">Qichuan Tian</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/et.al/">Et.al</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2021 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/caiyong-ijcb-2021/">
      <img src="/publication/caiyong-ijcb-2021/featured_hu5c2027b5f62b123982bf52fb2e9c4cf7_109589_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/caiyong-ijcb-2021/">NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization</a>
  </h3>

  
  <a href="/publication/caiyong-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and also been introduced in iris biometrics, especially iris segmentation. To investigate recent developments and attract more interest of researchers in the iris segmentation method, we organized the 2021 NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021) at the 2021 International Joint Conference on Biometrics (IJCB 2021). The challenge was used as a public platform to assess the performance of iris segmentation and localization methods on Asian and African NIR iris images captured in non-cooperative environments. The three best-performing entries achieved solid and satisfactory iris segmentation and localization results in most cases, and their code and models have been made publicly available for reproducibility research.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9484336" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/caiyong-ijcb-2021/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cripacsir.cn/dataset/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  














  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2109.00162" target="_blank" rel="noopener">
  Applications
</a>






<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484336" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/jawad-tifs-2021/">
      <img src="/publication/jawad-tifs-2021/featured_hu9df0dec1e9f62dd117ce5a647277ce52_355407_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="CASIA-Face-Africa: A Large-Scale African Face Image Database">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/jawad-tifs-2021/">CASIA-Face-Africa: A Large-Scale African Face Image Database</a>
  </h3>

  
  <a href="/publication/jawad-tifs-2021/" class="summary-link">
    <div class="article-style">
      <p>Face recognition is a popular and well-studied area with wide applications in our society. However, racial bias had been proven to be inherent in most State Of The Art (SOTA) face recognition systems. Many investigative studies on face recognition algorithms have reported higher false positive rates of African subjects cohorts than the other cohorts. Lack of large-scale African face image databases in public domain is one of the main restrictions in studying the racial bias problem of face recognition. To this end, we collect a face image database namely CASIA-Face-Africa which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The performances of SOTA face recognition algorithms without re-training are reported as baselines. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc. The database can be downloaded from our website.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9456939" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/jawad-tifs-2021/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cripacsir.cn/dataset/casia-face-africa/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2021.3080496" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/%E5%AD%99%E5%93%B2%E5%8D%97/">å­™å“²å—</a></span>, <span ><a href="/author/%E8%B5%AB%E7%84%B6/">èµ«ç„¶</a></span>, <span ><a href="/author/%E7%8E%8B%E4%BA%AE/">çŽ‹äº®</a></span>, <span ><a href="/author/%E9%98%9A%E7%BE%8E%E5%A8%9C/">é˜šç¾Žå¨œ</a></span>, <span ><a href="/author/%E5%86%AF%E5%BB%BA%E6%B1%9F/">å†¯å»ºæ±Ÿ</a></span>, <span ><a href="/author/%E9%83%91%E6%96%B9/">éƒ‘æ–¹</a></span>, <span ><a href="/author/%E9%83%91%E4%BC%9F%E8%AF%97/">éƒ‘ä¼Ÿè¯—</a></span>, <span ><a href="/author/%E5%B7%A6%E6%97%BA%E5%AD%9F/">å·¦æ—ºå­Ÿ</a></span>, <span ><a href="/author/%E5%BA%B7%E6%96%87%E9%9B%84/">åº·æ–‡é›„</a></span>, <span ><a href="/author/%E9%82%93%E4%BC%9F%E6%B4%AA/">é‚“ä¼Ÿæ´ª</a></span>, <span ><a href="/author/%E5%BC%A0%E6%9D%B0/">å¼ æ°</a></span>, <span ><a href="/author/%E9%9F%A9%E7%90%A5/">éŸ©ç¥</a></span>, <span ><a href="/author/%E5%B1%B1%E4%B8%96%E5%85%89/">å±±ä¸–å…‰</a></span>, <span ><a href="/author/%E7%8E%8B%E4%BA%91%E9%BE%99/">çŽ‹äº‘é¾™</a></span>, <span ><a href="/author/%E8%8C%B9%E4%B8%80%E4%BC%9F/">èŒ¹ä¸€ä¼Ÿ</a></span>, <span ><a href="/author/%E6%9C%B1%E5%AE%87%E8%B1%AA/">æœ±å®‡è±ª</a></span>, <span ><a href="/author/%E5%88%98%E4%BA%91%E5%B8%86/">åˆ˜äº‘å¸†</a></span>, <span ><a href="/author/%E4%BD%95%E5%8B%87/">ä½•å‹‡</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zhenan-joiag-2021/">
      <img src="/publication/zhenan-joiag-2021/featured_hu29c72ae87cc4156e5c5d52c76416078d_94852_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="ç”Ÿç‰©ç‰¹å¾è¯†åˆ«å­¦ç§‘å‘å±•æŠ¥å‘Š">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zhenan-joiag-2021/">ç”Ÿç‰©ç‰¹å¾è¯†åˆ«å­¦ç§‘å‘å±•æŠ¥å‘Š</a>
  </h3>

  
  <a href="/publication/zhenan-joiag-2021/" class="summary-link">
    <div class="article-style">
      <p>ä»Žæ‰‹æœºè§£é”ã€å°åŒºé—¨ç¦åˆ°é¤åŽ…åƒé¥­ã€è¶…å¸‚æ”¶é“¶ï¼Œå†åˆ°é«˜é“è¿›ç«™ã€æœºåœºå®‰æ£€ä»¥åŠåŒ»é™¢çœ‹ç—…ï¼Œäººè„¸ã€è™¹è†œå’ŒæŒ‡çº¹ç­‰ç”Ÿç‰©ç‰¹å¾å·²æˆä¸ºäººä»¬è¿›å…¥ä¸‡ç‰©äº’è”ä¸–ç•Œçš„æ•°å­—èº«ä»½è¯ã€‚ç”Ÿç‰©ç‰¹å¾è¯†åˆ«èµ‹äºˆæœºå™¨è‡ªåŠ¨æŽ¢æµ‹ã€æ•èŽ·ã€å¤„ç†ã€åˆ†æžå’Œè¯†åˆ«æ•°å­—åŒ–ç”Ÿç†æˆ–è¡Œä¸ºä¿¡å·çš„é«˜çº§æ™ºèƒ½ï¼Œæ˜¯ä¸€ä¸ªå…¸åž‹è€Œåˆå¤æ‚çš„æ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œä¸€ç›´å¤„äºŽäººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å‰æ²¿ï¼Œåœ¨æ–°ä¸€ä»£äººå·¥æ™ºèƒ½è§„åˆ’ã€â€œäº’è”ç½‘+â€è¡ŒåŠ¨è®¡åˆ’ç­‰å›½å®¶æˆ˜ç•¥ä¸­å…·æœ‰é‡è¦åœ°ä½ã€‚ç”±äºŽç”Ÿç‰©ç‰¹å¾è¯†åˆ«æ¶‰åŠå…¬ä¼—åˆ©ç›Šæ”¸å…³çš„éšç§ã€é“å¾·å’Œæ³•å¾‹ç­‰é—®é¢˜ï¼Œè¿‘æœŸä¹Ÿå¼•èµ·äº†å¹¿æ³›çš„ç¤¾ä¼šå…³æ³¨ã€‚æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†ç”Ÿç‰©ç‰¹å¾è¯†åˆ«å­¦ç§‘å‘å±•çŽ°çŠ¶ã€æ–°å…´æ–¹å‘ã€å­˜åœ¨é—®é¢˜å’Œå¯è¡Œæ€è·¯ï¼Œæ·±å…¥æ¢³ç†äº†äººè„¸ã€è™¹è†œã€æŒ‡çº¹ã€æŽŒçº¹ã€é™è„‰ã€å£°çº¹ã€æ­¥æ€ã€è¡Œäººé‡è¯†åˆ«ä»¥åŠå¤šæ¨¡æ€èžåˆè¯†åˆ«çš„ç ”ç©¶è¿›å±•ï¼Œä»¥äººè„¸ä¸ºä¾‹é‡ç‚¹ä»‹ç»äº†ç”Ÿç‰©ç‰¹å¾è¯†åˆ«é¢†åŸŸè¿‘äº›å¹´å—åˆ°å…³æ³¨çš„æ–°æ–¹å‘â€”â€”å¯¹æŠ—æ”»å‡»å’Œé˜²å¾¡ã€æ·±åº¦ä¼ªé€ å’Œåä¼ªé€ ï¼Œæœ€åŽå‰–æžæ€»ç»“äº†ç”Ÿç‰©ç‰¹å¾è¯†åˆ«é¢†åŸŸå­˜åœ¨çš„3å¤§æŒ‘æˆ˜é—®é¢˜â€”â€”â€œæ„ŸçŸ¥ç›²åŒºâ€ã€â€œå†³ç­–è¯¯åŒºâ€å’Œâ€œå®‰å…¨çº¢åŒºâ€ã€‚æœ¬æ–‡è®¤ä¸ºå¿…é¡»å˜é©å’Œåˆ›æ–°ç”Ÿç‰©ç‰¹å¾çš„ä¼ æ„Ÿã€è®¤çŸ¥å’Œå®‰å…¨æœºåˆ¶ï¼Œæ‰æœ‰å¯èƒ½å–å¾—å¤æ‚åœºæ™¯ç”Ÿç‰©è¯†åˆ«å­¦æœ¯ç ”ç©¶å’ŒæŠ€æœ¯åº”ç”¨çš„æ ¹æœ¬æ€§çªç ´ï¼Œç ´é™¤çŽ°æœ‰ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯çš„å¼Šç«¯ï¼Œæœç€â€œå¯æ„Ÿâ€ã€â€œå¯çŸ¥â€å’Œâ€œå¯ä¿¡â€çš„æ–°ä¸€ä»£ç”Ÿç‰©ç‰¹å¾è¯†åˆ«æ€»ä½“ç›®æ ‡å‘å±•ã€‚</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.11834/jig.210078" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zhenan-joiag-2021/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.idealtest.org/dbDetailForUser.do?id=4#/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  










  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/zhenan-joiag-2021/JIG2024.png" target="_blank" rel="noopener">
  Awards1
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/zhenan-joiag-2021/JIG2023.jpg" target="_blank" rel="noopener">
  Awards2
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/zhenan-joiag-2021/JIG2021.png" target="_blank" rel="noopener">
  Awards3
</a>








<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.11834/jig.210078" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2021 (IJCB2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-ijcb-2021/">
      <img src="/publication/wang-ijcb-2021/featured_hu3831a95f94345853610cf66420f793ae_161338_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="An End-to-End Autofocus Camera for Iris on the Move">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-ijcb-2021/">An End-to-End Autofocus Camera for Iris on the Move</a>
  </h3>

  
  <a href="/publication/wang-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>For distant iris recognition, a long focal length lens is generally used to ensure the resolution of iris images, which reduces the depth of field and leads to potential defocus blur. To accommodate users at different distances, it is necessary to control focus quickly and accurately. While for users in motion, it is expected to maintain the correct focus on the iris area continuously. In this paper, we introduced a novel rapid autofocus camera for active refocusing of the iris area of the moving objects using a focus-tunable lens. Our end-to-end computational algorithm can predict the best focus position from one single blurred image and generate the proper lens diopter control signal automatically. This scene-based active manipulation method enables real-time focus tracking of the iris area of a moving object. We built a testing bench to collect real-world focal stacks for evaluation of the autofocus methods. Our camera has reached an autofocus speed of over 50 fps. The results demonstrate the advantages of our proposed camera for biometric perception in static and dynamic scenes. The code is available at 
<a href="https://github.com/Debatrix/AquulaCam" target="_blank" rel="noopener">https://github.com/Debatrix/AquulaCam</a></p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://arxiv.org/abs/2106.15069" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-ijcb-2021/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/Debatrix/AquulaCam" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-ijcb-2021/Supplementary.zip" target="_blank" rel="noopener">
  Video
</a>















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2021 (IJCB2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wei-ijcb-2021/">
      <img src="/publication/wei-ijcb-2021/featured_hu45ff9a0038e6c94623a6a1a5bd6c85b7_73136_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wei-ijcb-2021/">Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples</a>
  </h3>

  
  <a href="/publication/wei-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>Cross-database recognition is still an unavoidable challenge when deploying an iris recognition system to a new environment. In the paper, we present a compromise problem that resembles the real-world scenario, named iris recognition with insufficient labeled samples. This new problem aims to improve the recognition performance by utilizing partially-or un-labeled data. To address the problem, we propose Contrastive Uncertainty Learning (CUL) by integrating the merits of uncertainty learning and contrastive self-supervised learning. CUL makes two efforts to learn a discriminative and robust feature representation. On the one hand, CUL explores the uncertain acquisition factors and adopts a probabilistic embedding to represent the iris image. In the probabilistic representation, the identity information and acquisition factors are disentangled into the mean and variance, avoiding the impact of uncertain acquisition factors on the identity information. On the other hand, CUL utilizes probabilistic embeddings to generate virtual positive and negative pairs. Then CUL builds its contrastive loss to group the similar samples closely and push the dissimilar samples apart. The experimental results demonstrate the effectiveness of the proposed CUL for iris recognition with insufficient labeled samples.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9484388" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wei-ijcb-2021/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/reborn20200813/uncertainty" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484388" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yiwei-ru/">Yiwei Ru</a></span>, <span ><a href="/author/wanting-zhou/">Wanting Zhou</a></span>, <span ><a href="/author/yunfan-liu/">Yunfan Liu</a></span>, <span ><a href="/author/jianxin-sun/">Jianxin Sun</a></span>, <span ><a href="/author/qi-li/">Qi Li</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2021 (IJCB2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/yiwei-ru-ijcb-2021/">
      <img src="/publication/yiwei-ru-ijcb-2021/featured_hu4f5e0127d7f92a92098d703c2a0d0b57_191244_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/yiwei-ru-ijcb-2021/">Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection</a>
  </h3>

  
  <a href="/publication/yiwei-ru-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>Deep forgery detection on video data has attracted remarkable research attention in recent years due to its potential in defending forgery attacks. However, existing methods either only focus on the visual evidence within individual images, or are too sensitive to fluctuations across frames. To address these issues, this paper propose a novel model, named Bita-Net, to detect forgery faces in video data. The network design of Bita-Net is inspired by the mechanism of how human beings detect forgery data, i.e. browsing and scrutinizing, which is reflected by the two-pathway architecture of Bita-Net. Concretely, the browsing pathway scans the entire video at a high frame rate to check the temporal consistency, while the scrutinizing pathway focuses on analyzing key frames of the video at a lower frame rate. Furthermore, an attention branch is introduced to improve the forgery detection ability of the scrutinizing pathway. Extensive experiment results demonstrate the effectiveness and generalization ability of Bita-Net on various popular face forensics detection datasets, including FaceForensics++, CelebDF, DeepfakeTIMIT and UADFV.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9484408" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/yiwei-ru-ijcb-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484408" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yu-tian/">Yu Tian</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/chong-zhang/">Chong Zhang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2021 (IJCB2021)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/yu-tian-ijcb-2021/">
      <img src="/publication/yu-tian-ijcb-2021/featured_hu28407e8c58f4879d48d2c94eda83f309_92750_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/yu-tian-ijcb-2021/">Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method</a>
  </h3>

  
  <a href="/publication/yu-tian-ijcb-2021/" class="summary-link">
    <div class="article-style">
      <p>Spectacles reflection removal is a challenging problem in iris recognition research. The reflection of the spectacles usually contaminates the iris image acquired under infrared illumination. The intense light reflection caused by the active light source makes reflection removal more challenging than normal scenes since important iris texture features are entirely obscured. Eliminating unnecessary reflections can effectively improve iris recognition system performance. This paper proposes a spectacle reflection removal algorithm based on ray coding and ray tracking to remove spectacle reflection in iris images. By decoding the light sourceâ€™s encoded light beam, the iris imaging device eliminates most of the stray light. Our binocular imaging device tracks the light path to obtain parallax information and realizes reflected light spot removal through image fusion. We designed a prototype system to verify our proposed method in this paper. This method can effectively eliminate reflections without changing iris texture and improve iris recognition in complex scenarios.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/yu-tian-ijcb-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB52358.2021.9484402" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/he-ran/">He Ran</a></span>, <span ><a href="/author/wang-liang/">Wang Liang</a></span>, <span ><a href="/author/kan-meina/">Kan Meina</a></span>, <span ><a href="/author/feng-jianjiang/">Feng Jianjiang</a></span>, <span ><a href="/author/zheng-fang/">Zheng Fang</a></span>, <span ><a href="/author/zheng-weishi/">Zheng Weishi</a></span>, <span ><a href="/author/zuo-wangmeng/">Zuo Wangmeng</a></span>, <span ><a href="/author/kang-wenxiong/">Kang Wenxiong</a></span>, <span ><a href="/author/deng-weihong/">Deng Weihong</a></span>, <span ><a href="/author/zhang-jie/">Zhang Jie</a></span>, <span ><a href="/author/han-hu/">Han Hu</a></span>, <span ><a href="/author/shan-shiguang/">Shan Shiguang</a></span>, <span ><a href="/author/wang-yunlong/">Wang Yunlong</a></span>, <span ><a href="/author/ru-yiwei/">Ru Yiwei</a></span>, <span ><a href="/author/zhu-yuhao/">Zhu Yuhao</a></span>, <span ><a href="/author/liu-yunfan/">Liu Yunfan</a></span>, <span ><a href="/author/he-yong/">He Yong</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Image and Graphics</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/sun-jig-2021/">
      <img src="/publication/sun-jig-2021/featured_hu0fe44507e604ec5c5cc5aba33241259c_195484_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Overview of biometrics research">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/sun-jig-2021/">Overview of biometrics research</a>
  </h3>

  
  <a href="/publication/sun-jig-2021/" class="summary-link">
    <div class="article-style">
      <p>Biometrics, such as face, iris, and fingerprint recognition, have become digital identity proof for people to enter the â€œInternet of Everythingâ€ . For example, one may be asked to present the biometric identifier for unlocking mobile, passing access control at airports, rail stations, and paying at supermarkets or restaurants. Biometric recognition empowers a machine to automatically detect, capture, process, analyze, and recognize digital physiological or behavioral signals with advanced intelligence. Thus, biometrics requires interdisciplinary research of science and technology involving optical engineering, mechanical engineering, electronic engineering, machine learning, pattern recognition, computer vision, digital image processing, signal analysis, cognitive science, neuroscience, human-computer interaction, and information security. Biometrics is a typical and complex pattern recognition problem, which is a frontier research direction of artificial intelligence. In addition, biometric identification is a key development area of Chinese strategies, such as the Development Plan on the New Generation of Artificial Intelligence and the â€œInternet Plusâ€ Action Plan. The development of biometric identification involves public interest, privacy, ethics, and law issues; thus, it has also attracted widespread attention from the society. This article systematically reviews the development status, emerging directions, existing problems, and feasible ideas of biometrics and comprehensively summarizes the research progress of face, iris, fingerprint, palm print, finger / palm vein, voiceprint, gait recognition, person reidentification, and multimodal biometric fusion. The overview of face recognition includes face detection, facial landmark localization, 2D face feature extraction and recognition, 3D face feature extraction and recognition, facial liveness detection, and face video based biological signal measurement. The overview of iris recognition includes iris image acquisition, iris segmentation and localization, iris liveness detection, iris image quality assessment, iris feature extraction, heterogeneous iris recognition, fusion of iris and other modalities, security problems of iris biometrics, and future trends of iris recognition. The overview of fingerprint recognition includes latent fingerprint recognition, fingerprint liveness detection, distorted fingerprint recognition, 3D fingerprint capturing, and challenges and trends of fingerprint biometrics. The overview of palm print recognition mainly introduces databases, feature models, matching strategies, and open problems of palm print biometrics. The overview of vein biometrics introduces main datasets and algorithms for finger vein, dorsal hand vein, and palm vein, and then points out the remaining unsolved problems and development trend of vein recognition. The overview of gait recognition introduces model-based and model-free methods for gait feature extraction and matching. The overview of person reidentification introduces research progress of new methods under supervised, unsupervised and weakly supervised conditions, gait database virtualization, generative gait models, and new problems, such as clothes changing, black clothes, and partial occlusions. The overview of voiceprint recognition introduces the history of speaker recognition, robustness of voiceprint, spoofing attacks, and antispoofing methods. The overview of multibiometrics introduces image-level, feature-level, score-level, and decision-level information fusion methods and deep learning based fusion approaches. Taking face as the exemplar biometric modality, new research directions that have received great attentions in the field of biometric recognition in recent years, i. e. , adversarial attack and defense as well as Deepfake and anti-Deepfake, are also introduced. Finally, we analyze and summarize the three major challenges in the field of biometric recognitionâ€”â€”â€” â€œ the blind spot of biometric sensorsâ€, â€œ the decision errors of biometric algorithmsâ€ and â€œthe red zone of biometric securityâ€ . Therefore, the sensing, cognition, and security mechanisms of biometrics are necessary to achieve a fundamental breakthrough in the academic research and technologies applications of biometrics in complex scenarios to address the shortcomings of the existing biometric technologies and to move towards the overall goal of developing a new generation of â€œ perceptible, â€œ robustâ€, and â€œ trustworthyâ€ biometric identification technology.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=20210605&amp;flag=1" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/sun-jig-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.%2011834%20/%20jig.%20210078" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/m.-vitek/">M. Vitek</a></span>, <span ><a href="/author/a.-das/">A. Das</a></span>, <span ><a href="/author/y.-pourcenoux/">Y. Pourcenoux</a></span>, <span ><a href="/author/et.al/">Et.al</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2020 IEEE International Joint Conference on Biometrics (IJCB)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/vitek-ijcb-2020/">
      <img src="/publication/vitek-ijcb-2020/featured_hu76d05ac8a054f2dcdfca489501a81a28_415938_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="SSBC 2020: Sclera Segmentation Benchmarking Competition in the Mobile Environment">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/vitek-ijcb-2020/">SSBC 2020: Sclera Segmentation Benchmarking Competition in the Mobile Environment</a>
  </h3>

  
  <a href="/publication/vitek-ijcb-2020/" class="summary-link">
    <div class="article-style">
      <p>The paper presents a summary of the 2020 Sclera Segmentation Benchmarking Competition (SSBC), the 7th in the series of group benchmarking efforts centred around the problem of sclera segmentation. Different from previous editions, the goal of SSBC 2020 was to evaluate the performance of sclera-segmentation models on images captured with mobile devices. The competition was used as a platform to assess the sensitivity of existing models to i) differences in mobile devices used for image capture and ii) changes in the ambient acquisition conditions. 26 research groups registered for SSBC 2020, out of which 13 took part in the final round and submitted a total of 16 segmentation models for scoring. These included a wide variety of deep-learning solutions as well as one approach based on standard image processing techniques. Experiments were conducted with three recent datasets. Most of the segmentation models achieved relatively consistent performance across images captured with different mobile devices (with slight differences across devices), but struggled most with low-quality images captured in challenging ambient conditions, i.e., in an indoor environment and with poor lighting.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9304881" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/vitek-ijcb-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/IJCB48548.2020.9304881" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>ICB</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/afdr-iris-recognition/afdr-iris-recognition/">Alignment Free and Distortion Robust Iris Recognition</a>
  </h3>

  
  <a href="/publication/afdr-iris-recognition/afdr-iris-recognition/" class="summary-link">
    <div class="article-style">
      <p>Alignment Free, Iris Recognition, Preprocessing.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8987369" target="_blank" rel="noopener">
  PDF
</a>







  
    
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/ICB45273.2019.8987369" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>25th International Conference on Pattern Recognition (ICPR2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-icpr-2020/">
      <img src="/publication/wang-icpr-2020/featured_hub6bc0afe46e9e9022c0e1013b8bc7e51_322063_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-icpr-2020/">A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition</a>
  </h3>

  
  <a href="/publication/wang-icpr-2020/" class="summary-link">
    <div class="article-style">
      <p>The primitive basis of image based material recognition builds upon the fact that discrepancies in the reflectances of distinct materials lead to imaging differences under multiple viewpoints. LF cameras possess coherent abilities to capture multiple sub-aperture views (SAIs) within one exposure, which can provide appropriate multi-view sources for material recognition. In this paper, a unified Factorize-Connect-Merge (FCM) deep-learning pipeline is proposed to solve problems of light field image based material recognition. 4D light-field data as input is initially decomposed into consecutive 3D light-field slices. Shallow CNN is leveraged to extract low-level visual features of each view inside these slices. As to establish correspondences between these SAIs, Bidirectional Long-Short Term Memory (Bi-LSTM) network is built upon these low-level features to model the imaging differences. After feature selection including concatenation and dimension reduction, effective and robust feature representations for material recognition can be extracted from 4D light-field data. Experimental results indicate that the proposed pipeline can obtain remarkable performances on both tasks of single-pixel material classification and whole-image material segmentation. In addition, the proposed pipeline can potentially benefit and inspire other researchers who may also take LF images as input and need to extract 4D light-field representations for computer vision tasks such as object classification, semantic segmentation and edge detection.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-icpr-2020/wang-icpr-2020.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-icpr-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/fei-liu/">Fei Liu</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Computational Imaging</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tci-2020/">
      <img src="/publication/wang-tci-2020/featured_hu34f7cca5134d38775f830752b4ff7dee_172015_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tci-2020/">High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN</a>
  </h3>

  
  <a href="/publication/wang-tci-2020/" class="summary-link">
    <div class="article-style">
      <p>Multi-view properties of light field (LF) imaging enable exciting applications such as auto-refocusing, depth estimation and 3D reconstruction. However, limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards more practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. We have proposed an end-to-end deep learning framework named Pseudo 4DCNN to solve these problems in a conference paper. Rethinking on the overall paradigm, we further extend pseudo 4DCNN and propose a novel loss function which is applicable for all tasks of light field reconstruction i.e. EPI Structure Preserving (ESP) loss function. This loss function is proposed to attenuate the blurry edges and artifacts caused by averaging effect of L2 norm based loss function. Furthermore, the extended Pseudo 4DCNN is compared with recent state-of-the-art (SOTA) approaches on more publicly available light field databases, as well as self-captured light field biometrics and microscopy datasets. Experimental results demonstrate that the proposed framework can achieve better performances than vanilla Pseudo 4DCNN and other SOTA methods, especially in the terms of visual quality under occlusions. The source codes and self-collected datasets for reproducibility are available online.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/9061053" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/ExtendedP4DCNN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TCI.2020.2986092" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/boqiang-xu/"> Boqiang Xu</a></span>, <span ><a href="/author/yong-he/">Yong He</a></span>, <span ><a href="/author/zhiwei-dong/"> Zhiwei Dong</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-icassp-2020/">
      <img src="/publication/wang-icassp-2020/featured_hueecacb9230f66f40590959ac2039f403_102294_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-icassp-2020/">A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics</a>
  </h3>

  
  <a href="/publication/wang-icassp-2020/" class="summary-link">
    <div class="article-style">
      <p>This paper proposes a novel, lightweight deep convolutional neural network specifically designed for iris segmentation of noisy images acquired by mobile devices. Unlike previous studies, which only focused on improving the accuracy of segmentation mask using the popular CNN technology, our method is a complete end-to-end iris segmentation solution, i.e., segmentation mask and parameterized pupillary and limbic boundaries of the iris are obtained simultaneously, which further enables CNN-based iris segmentation to be applied in any regular iris recognition systems. By introducing an intermediate pictorial boundary representation, predictions of iris boundaries and segmentation mask have collectively formed a multi-label semantic segmentation problem, which could be well solved by a carefully adapted stacked hourglass network. Experimental results show that our method achieves competitive or state-of-the-art performance in both iris segmentation and localization on two challenging mobile iris databases.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9054353" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-icassp-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/ICASSP40776.2020.9054353" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yu-tian/">Yu Tian</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2020 Computer Vision and Pattern Recognition(CVPR)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/kunbo-cvpr-2020/">
      <img src="/publication/kunbo-cvpr-2020/featured_hubf1e47e8dfa88e31c4a8fc2dd03402f3_45752_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/kunbo-cvpr-2020/">Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario</a>
  </h3>

  
  <a href="/publication/kunbo-cvpr-2020/" class="summary-link">
    <div class="article-style">
      <p>Face anti-spoofing is the key to preventing security breaches in biometric recognition applications. Existing software-based and hardwarebased face liveness detection methods are effective in constrained environments or designated datasets only. Deep learning method using RGB and infrared images demands a large amount of training data for new attacks. In this paper, we present a face anti-spoofing method in a realworld scenario by automatic learning the physical characteristics in polarization images of a real face compared to a deceptive attack.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2003.08024" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/kunbo-cvpr-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.48550/arXiv.2003.08024" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 15)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tifs-2021/">
      <img src="/publication/wang-tifs-2021/featured_hu999f382f3bb4634cc2d4dc8939764d4d_130172_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tifs-2021/">Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition</a>
  </h3>

  
  <a href="/publication/wang-tifs-2021/" class="summary-link">
    <div class="article-style">
      <p>Iris images captured in non-cooperative environments often suffer from adverse noise, which challenges many existing iris segmentation methods. To address this problem, this paper proposes a high-efficiency deep learning based iris segmentation approach, named IrisParseNet. Different from many previous CNN-based iris segmentation methods, which only focus on predicting accurate iris masks by following popular semantic segmentation frameworks, the proposed approach is a complete iris segmentation solution, i.e., iris mask and parameterized inner and outer iris boundaries are jointly achieved by actively modeling them into a unified multi-task network. Moreover, an elaborately designed attention module is incorporated into it to improve the segmentation performance. To train and evaluate the proposed approach, we manually label three representative and challenging iris databases, i.e., CASIA.v4-distance, UBIRIS.v2, and MICHE-I, which involve multiple illumination (NIR, VIS) and imaging sensors (long-range and mobile iris cameras), along with various types of noises. Additionally, several unified evaluation protocols are built for fair comparisons. Extensive experiments are conducted on these newly annotated databases, and results show that the proposed approach achieves state-of-the-art performance on various benchmarks. Further, as a general drop-in replacement, the proposed iris segmentation method can be used for any iris recognition methodology, and would significantly improve the performance of non-cooperative iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9036930" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tifs-2021/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2020.2980791" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/jawad-muhammad/">Jawad Muhammad</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Information Forensics and Security ( Volume: 15)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tifs-2020/">
      <img src="/publication/wang-tifs-2020/featured_hu999f382f3bb4634cc2d4dc8939764d4d_130172_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tifs-2020/">Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition</a>
  </h3>

  
  <a href="/publication/wang-tifs-2020/" class="summary-link">
    <div class="article-style">
      <p>Iris images captured in non-cooperative environments often suffer from adverse noise, which challenges many existing iris segmentation methods. To address this problem, this paper proposes a high-efficiency deep learning based iris segmentation approach, named IrisParseNet. Different from many previous CNN-based iris segmentation methods, which only focus on predicting accurate iris masks by following popular semantic segmentation frameworks, the proposed approach is a complete iris segmentation solution, i.e., iris mask and parameterized inner and outer iris boundaries are jointly achieved by actively modeling them into a unified multi-task network. Moreover, an elaborately designed attention module is incorporated into it to improve the segmentation performance. To train and evaluate the proposed approach, we manually label three representative and challenging iris databases, i.e., CASIA.v4-distance, UBIRIS.v2, and MICHE-I, which involve multiple illumination (NIR, VIS) and imaging sensors (long-range and mobile iris cameras), along with various types of noises. Additionally, several unified evaluation protocols are built for fair comparisons. Extensive experiments are conducted on these newly annotated databases, and results show that the proposed approach achieves state-of-the-art performance on various benchmarks. Further, as a general drop-in replacement, the proposed iris segmentation method can be used for any iris recognition methodology, and would significantly improve the performance of non-cooperative iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9036930" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tifs-2020/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xiamenwcy/IrisParseNet" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIFS.2020.2980791" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/ping-song/">Ping Song</a></span>, <span ><a href="/author/ling-huang/">Ling Huang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/fei-liu/">Fei Liu</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Acta Automatica Sinica
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/song-automatica2020/">
      <img src="/publication/song-automatica2020/featured_hua6cf0c9f82465dfbb2af54a0f5684fb0_1059934_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="Iris Liveness Detection Based on Light Field Imaging">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/song-automatica2020/">Iris Liveness Detection Based on Light Field Imaging</a>
  </h3>

  
  <a href="/publication/song-automatica2020/" class="summary-link">
    <div class="article-style">
      <p>Light-field (LF) imaging is a new method to capture both intensity and direction information of visual objects, providing promising solutions to biometrics. Iris recognition is a reliable personal identification method, however it is also vulnerable to spoofing attacks, such as iris patterns printed on contact lens or paper. Therefore iris liveness detection is an important module in iris recognition systems. In this paper, an iris liveness detection approach is proposed to take full advantages of intrinsic characteristics in light-field iris imaging. LF iris images are captured by using lab-made LF cameras, based on which the geometric features as well as the texture features are extracted using the LF digital refocusing technology. These features are combined for genuine and fake iris image classification. Experiments were carried out based on the self-collected near-infrared LF iris database, and the average classification error rate (ACER) of the proposed method is 3.69%, which is 5.94% lower than the best state-of-the-art method. Experimental results indicate the proposed method is able to work effectively and accurately to prevent spoofing attacks such as printed and screen-displayed iris input attacks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/song-automatica2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.16383/j.aas.c180213" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2020 (IJCB2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-ijcb-2020/">
      <img src="/publication/wang-ijcb-2020/featured_huf62c02a8356c55152bbb2cab3f6b47df_86627_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Recognition Oriented Iris Image Quality Assessment in the Feature Space">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-ijcb-2020/">Recognition Oriented Iris Image Quality Assessment in the Feature Space</a>
  </h3>

  
  <a href="/publication/wang-ijcb-2020/" class="summary-link">
    <div class="article-style">
      <p>A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2009.00294" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-ijcb-2020/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/Debatrix/DFSNet" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/dataset/casia-iris-complex" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-ijcb-2020/slide.pdf" target="_blank" rel="noopener">
  Slides
</a>

















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenteng-shen/">Zhenteng Shen</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2020 (IJCB2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zhang-ijcb2020/">
      <img src="/publication/zhang-ijcb2020/featured_hu50b7bae0b7c33630b81634a1cb415849_403047_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="All-in-Focus Iris Camera With a Great Capture Volume">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zhang-ijcb2020/">All-in-Focus Iris Camera With a Great Capture Volume</a>
  </h3>

  
  <a href="/publication/zhang-ijcb2020/" class="summary-link">
    <div class="article-style">
      <p>Imaging volume of an iris recognition system has been restricting the throughput and cooperation convenience in biometric applications. Numerous improvement trials are still impractical to supersede the dominant fixed-focus lens in stand-off iris recognition due to incremental performance increase and complicated optical design. In this study, we develop a novel all-in-focus iris imaging system using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth of field extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed. In addition, the motorized reflection mirror adaptively steers the light beam to extend the horizontal and vertical field of views in an active manner. The proposed all-in-focus iris camera increases the depth of field up to 3.9 m which is a factor of 37.5 compared with conventional long focal lens. We also experimentally demonstrate the capability of this 3D light beam steering imaging system in real-time multi-person iris refocusing using dynamic focal stacks and the potential of continuous iris recognition for moving participants.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/9304932" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zhang-ijcb2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  









  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieee-biometrics.org/ijcb2020/Program.html" target="_blank" rel="noopener">
  Awards
</a>













  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/yunfan-liu/">Yunfan Liu</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tbiom-2020/">
      <img src="/publication/wang-tbiom-2020/featured_huc6676f3f0515d4a98830b7b111a2a6bf_1212330_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tbiom-2020/">ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation</a>
  </h3>

  
  <a href="/publication/wang-tbiom-2020/" class="summary-link">
    <div class="article-style">
      <p>Accurate sclera segmentation is critical for successful sclera recognition. However, studies on sclera segmentation algorithms are still limited in the literature. In this paper, we propose a novel sclera segmentation method based on the improved U-Net model, named as ScleraSegNet. We perform in-depth analysis regarding the structure of U-Net model, and propose to embed an attention module into the central bottleneck part between the contracting path and the expansive path of U-Net to strengthen the ability of learning discriminative representations. We compare different attention modules and find that channel-wise attention is the most effective in improving the performance of the segmentation network. Besides, we evaluate the effectiveness of data augmentation process in improving the generalization ability of the segmentation network. Experiment results show that the best performing configuration of the proposed method achieves state-of-the-art performance with F-measure values of 91.43%, 89.54% on UBIRIS.v2 and MICHE, respectively.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8987270" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tbiom-2020/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/xiamenwcy/IrisSegBenchmark" target="_blank" rel="noopener">
  Code
</a>




  
    
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TBIOM.2019.2962190" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-2020-dynamic/">
      <img src="/publication/ren-2020-dynamic/featured_hu5a7c5f350ab04416a3b81176db425b79_598277_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="Dynamic Graph Representation for Occlusion Handling in Biometrics">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-2020-dynamic/">Dynamic Graph Representation for Occlusion Handling in Biometrics</a>
  </h3>

  
  <a href="/publication/ren-2020-dynamic/" class="summary-link">
    <div class="article-style">
      <p>The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-2020-dynamic/ren-2020-dynamic.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-2020-dynamic/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/RenMin1991/Dyamic-Graph-Representation" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/fei-liu/">Fei Liu</a></span>, <span ><a href="/author/shubo-zhou/">Shubo Zhou</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/guangqi-hou/">Guangqi Hou</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Image Processing</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/liu-tip-2020/">
      <img src="/publication/liu-tip-2020/featured_hu38c5c190bc09914eb7aa3bcdf8d34db0_176628_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/liu-tip-2020/">Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application</a>
  </h3>

  
  <a href="/publication/liu-tip-2020/" class="summary-link">
    <div class="article-style">
      <p>Binocular stereo vision (SV) has been widely used to reconstruct the depth information, but it is quite vulnerable to scenes with strong occlusions. As an emerging computational photography technology, light-field (LF) imaging brings about a novel solution to passive depth perception by recording multiple angular views in a single exposure. In this paper, we explore binocular SV and LF imaging to form the binocular-LF imaging system. An imaging theory is derived by modeling the imaging process and analyzing disparity properties based on the geometrical optics theory. Then an accurate occlusion-robust depth estimation algorithm is proposed by exploiting multibaseline stereo matching cues and defocus cues. The occlusions caused by binocular SV and LF imaging are detected and handled to eliminate the matching ambiguities and outliers. Finally, we develop a binocular-LF database and capture realworld scenes by our binocular-LF system to test the accuracy and robustness. The experimental results demonstrate that the proposed algorithm definitely recovers high quality depth maps with smooth surfaces and precise geometric shapes, which tackles the drawbacks of binocular SV and LF imaging simultaneously.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8851410/" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/liu-tip-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIP.2019.2943019" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/%E5%AE%8B%E5%B9%B3/">å®‹å¹³</a></span>, <span ><a href="/author/%E9%BB%84%E7%8E%B2/">é»„çŽ²</a></span>, <span ><a href="/author/%E7%8E%8B%E4%BA%91%E9%BE%99/">çŽ‹äº‘é¾™</a></span>, <span ><a href="/author/%E5%88%98%E8%8F%B2/">åˆ˜è²</a></span>, <span ><a href="/author/%E5%AD%99%E5%93%B2%E5%8D%97/">å­™å“²å—</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>è‡ªåŠ¨åŒ–å­¦æŠ¥</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/songping-aas-2019/">
      <img src="/publication/songping-aas-2019/featured_hu54cbaee045b2fd91aa1eeeede7811df2_98359_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="åŸºäºŽè®¡ç®—å…‰åœºæˆåƒçš„è™¹è†œæ´»ä½“æ£€æµ‹æ–¹æ³•">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/songping-aas-2019/">åŸºäºŽè®¡ç®—å…‰åœºæˆåƒçš„è™¹è†œæ´»ä½“æ£€æµ‹æ–¹æ³•</a>
  </h3>

  
  <a href="/publication/songping-aas-2019/" class="summary-link">
    <div class="article-style">
      <p>å…‰åœºæˆåƒç›¸å¯¹ä¼ ç»Ÿå…‰å­¦æˆåƒæ˜¯ä¸€æ¬¡é‡å¤§æŠ€æœ¯é©æ–°,é«˜ç»´å…‰åœºä¿¡æ¯ä¸ºç”Ÿç‰©ç‰¹å¾è¯†åˆ«çš„å‘å±•ä¸Žåˆ›æ–°å¸¦æ¥äº†æ–°æœºé‡.è™¹è†œèº«ä»½è¯†åˆ«æŠ€æœ¯ä»¥å…¶å”¯ä¸€æ€§ã€ç¨³å®šæ€§ã€é«˜ç²¾åº¦ç­‰ä¼˜åŠ¿å¹¿æ³›åº”ç”¨äºŽå›½é˜²ã€æ•™è‚²ã€é‡‘èžç­‰å„ä¸ªé¢†åŸŸ,ä½†æ˜¯çŽ°æœ‰çš„è™¹è†œè¯†åˆ«ç³»ç»Ÿå®¹æ˜“è¢«äººé€ å‡ä½“è™¹è†œæ ·æœ¬æ¬ºéª—å¯¼è‡´è¯¯è¯†åˆ«.å› æ­¤,è™¹è†œæ´»ä½“æ£€æµ‹æ˜¯å½“å‰è™¹è†œè¯†åˆ«ç ”ç©¶äºŸå¾…è§£å†³çš„å…³é”®é—®é¢˜.æœ¬æ–‡æå‡ºä¸€ç§åŸºäºŽè®¡ç®—å…‰åœºæˆåƒçš„è™¹è†œæ´»ä½“æ£€æµ‹æ–¹æ³•,é€šè¿‡è½¯ç¡¬ä»¶ç»“åˆçš„æ–¹å¼,å……åˆ†æŒ–æŽ˜å››ç»´å…‰åœºæ•°æ®çš„ä¿¡æ¯.æœ¬æ–¹æ³•ä½¿ç”¨å®žéªŒå®¤è‡ªä¸»ç ”å‘çš„å…‰åœºç›¸æœºé‡‡é›†å…‰åœºè™¹è†œå›¾åƒ,åˆ©ç”¨å…‰åœºæ•°å­—é‡å¯¹ç„¦æŠ€æœ¯æå–çœ¼å‘¨åŒºåŸŸçš„ç«‹ä½“ç»“æž„ç‰¹å¾å’Œè™¹è†œå›¾åƒçš„çº¹ç†ç‰¹å¾,è¿›è¡Œç‰¹å¾èžåˆä¸Žè™¹è†œåˆ†ç±».åœ¨è‡ªä¸»é‡‡é›†çš„è¿‘çº¢å¤–å…‰åœºè™¹è†œæ´»ä½“æ£€æµ‹æ•°æ®åº“ä¸Šè¿›è¡Œå®žéªŒ,æœ¬æ–¹æ³•çš„å¹³å‡åˆ†ç±»é”™è¯¯çŽ‡(Average classification error rate,ACER)ä¸º3.69%,åœ¨çŽ°æœ‰æœ€ä½³æ–¹æ³•çš„åŸºç¡€ä¸Šé™ä½Ž5.94%.å®žéªŒç»“æžœè¡¨æ˜Žæœ¬æ–¹æ³•å¯ä»¥å‡†ç¡®æœ‰æ•ˆåœ°æ£€æµ‹å¹¶é˜»æ­¢æ‰“å°è™¹è†œå’Œå±æ˜¾è™¹è†œå¯¹ç³»ç»Ÿçš„æ”»å‡».</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.cnki.net/kcms/doi/10.16383/j.aas.c180213.html" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/songping-aas-2019/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.16383/j.aas.c180213" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span ><a href="/author/hui-zhang/">Hui Zhang</a></span>, <span ><a href="/author/lihu-xiao/">Lihu Xiao</a></span>, <span ><a href="/author/jing-liu/">Jing Liu</a></span>, <span ><a href="/author/xingguang-li/">Xingguang Li</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng He</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/biometrics/">Biometrics</a>, <a href="/category/computer-vision/">Computer Vision</a></span>
  

</div>

  

  
  
  
  
  <a href="/publication/hu-2019-icb/">
      <img src="/publication/hu-2019-icb/featured_hub20b5cffd4ef0267b382a51e7f712c34_752909_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="Seg-Edge Bilateral Constraint Network for Iris Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hu-2019-icb/">Seg-Edge Bilateral Constraint Network for Iris Segmentation</a>
  </h3>

  
  <a href="/publication/hu-2019-icb/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we present an end-to-end model, namely Seg-Edge bilateral constraint network. The iris edge map generated from rich convolutional layers optimize the iris segmentation by aligning it with the iris boundary. The iris region produced by the coarse segmentation limits the scope. It makes the edge filtering pay more attention to the interesting target. We compress the model while keeping the performance levels almost intact and even better by using l1-norm. The proposed model advances the state-of-the-art iris segmentation accuracies.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/hu-2019-icb/hu-2019-ICB.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hu-2019-icb/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cbsr.ia.ac.cn/china/Iris%20Databases%20CH.asp" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/xiang-wu/">Xiang Wu</a></span>, <span ><a href="/author/zhaofeng-he/">Zhaofeng  He</a></span>, <span ><a href="/author/ran-he/">Ran He</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>10th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS2019)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/csin-btas-2019/">
      <img src="/publication/csin-btas-2019/featured_hu921313c5a5a034189cb86b41e165c587_214059_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="Cross-sensor iris recognition using adversarial strategy and sensor-specific information">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/csin-btas-2019/">Cross-sensor iris recognition using adversarial strategy and sensor-specific information</a>
  </h3>

  
  <a href="/publication/csin-btas-2019/" class="summary-link">
    <div class="article-style">
      <p>Due to the growing demand of iris biometrics, lots of new sensors are being developed for high-quality image acquisition. However, upgrading the sensor and re-enrolling for users is expensive and time-consuming. This leads to a dilemma where enrolling on one type of sensor but recognizing on the others. For this cross-sensor matching, the large gap between distributions of enrolling and recognizing images usually results in degradation in recognition performance. To alleviate this degradation, we propose Cross-sensor iris network (CSIN) by applying the adversarial strategy and weakening interference of sensor-specific information. Specifically, there are three valuable efforts towards learning discriminative iris features. Firstly, the proposed CSIN adds extra feature extractors to generate residual components containing sensor-specific information and then utilizes these components to narrow the distribution gap. Secondly, an adversarial strategy is borrowed from Generative Adversarial Networks to align feature distributions and further reduce the discrepancy of images caused by sensors. Finally, we extend triplet loss and propose instance-anchor loss to pull the instances of the same class together and push away from others. It is worth mentioning that the proposed method doesnâ€™t need pair-same data or triplet, which reduced the cost of data preparation. Experiments on two real-world datasets validate the effectiveness of the proposed method in cross-sensor iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/csin-btas-2019/csin-btas-2019.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/csin-btas-2019/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/weijianze/CSINv2" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Conference on Biometrics (ICB2019)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-2019-alignment/">
      <img src="/publication/ren-2019-alignment/featured_hu007bcafc10ae98032c8ba59182b80711_89939_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Alignment Free and Distortion Robust Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-2019-alignment/">Alignment Free and Distortion Robust Iris Recognition</a>
  </h3>

  
  <a href="/publication/ren-2019-alignment/" class="summary-link">
    <div class="article-style">
      <p>Iris recognition is a reliable personal identification method but there is still much room to improve its accuracy especially in less-constrained situations. For example, free movement of head pose may cause large rotation difference between iris images. And illumination variations may cause irregular distortion of iris texture. To match intra-class iris images with head rotation robustly, the existing soadminlutions usually need a precise alignment operation by exhaustive search within a determined range in iris image preprosessing or brute-force searching the minimum Hamming distance in iris feature matching. In the wild enviroments, iris rotation is of much greater uncertainty than that in constrained situations and exhaustive search within a determined range is impracticable. This paper presents a unified feature-level solution to both alignment free and distortion robust iris recognition in the wild. A new deep learning based method named Alignment Free Iris Network (AFINet) is  proposed, which utilizes a trainable VLAD (Vector of Locally Aggregated Descriptors) encoder called NetVLAD [18] to decouple the correlations between local representations and their spatial positions. And deformable convolution [5] is leveraged to overcome iris texture distortion by dense adaptive sampling. The results of extensive experiments on three public iris image databases and the simulated degradation databases show that AFINet significantly outperforms state-of-art iris recognition methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-2019-alignment/ren-2019-alignment.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-2019-alignment/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/RenMin1991/AF_Net" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/fei-liu/">Fei Liu</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/guangqi-hou/">Guangqi Hou</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Image Processing (TIP)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tip-2018/">
      <img src="/publication/wang-tip-2018/featured_hu42a9c883ceb04f955374599571821e76_1258113_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tip-2018/">LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution</a>
  </h3>

  
  <a href="/publication/wang-tip-2018/" class="summary-link">
    <div class="article-style">
      <p>The low spatial resolution of light-field image poses significant difficulties in exploiting its advantage. To mitigate the dependency of accurate depth or disparity information as priors for light-field image super-resolution, we propose an implicitly multi-scale fusion scheme to accumulate contextual information from multiple scales for super-resolution reconstruction. The implicitly multi-scale fusion scheme is then incorporated into bidirectional recurrent convolutional neural network, which aims to iteratively model spatial relations between horizontally or vertically adjacent sub-aperture images of light-field data. Within the network, the recurrent convolutions are modified to be more effective and flexible in modeling the spatial correlations between neighboring views. A horizontal sub-network and a vertical sub-network of the same network structure are ensembled for final outputs via stacked generalization. Experimental results on synthetic and real-world data sets demonstrate that the proposed method outperforms other state-of-the-art methods by a large margin in peak signal-to-noise ratio and gray-scale structural similarity indexes, which also achieves superior quality for human visual systems. Furthermore, the proposed method can enhance the performance of light field applications such as depth estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-tip-2018/wang-tip-2018.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tip-2018/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/LFNet_Test" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIP.2018.2834819" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zihui-yan/">Zihui Yan</a></span>, <span ><a href="/author/lingxiao-he/">Lingxiao He</a></span>, <span ><a href="/author/man-zhang/">Man Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/yan-2018-liveness/">
      <img src="/publication/yan-2018-liveness/featured_hue4460b8c05352970f1f202eab563cde8_30715_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Hierarchical Multi-class Iris Classification for Liveness Detection">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/yan-2018-liveness/">Hierarchical Multi-class Iris Classification for Liveness Detection</a>
  </h3>

  
  <a href="/publication/yan-2018-liveness/" class="summary-link">
    <div class="article-style">
      <p>In modern society, iris recognition has become increasingly popular. The security risk of iris recognition is increasing rapidly because of the attack by various patterns of fake iris. A German hacker organization called Chaos Computer Club cracked the iris recognition system of Samsung Galaxy S8 recently. In view of these risks, iris liveness detection has shown its significant importance to iris recognition systems. The state-of-the-art algorithms mainly rely on hand-crafted texture features which can only identify fake iris images with single pattern. In this paper, we proposed a Hierarchical Multiclass Iris Classification (HMC) for liveness detection based on CNN. HMC mainly focuses on iris liveness detection of multipattern fake iris. The proposed method learns the features of different fake iris patterns by CNN and classifies the genuine or fake iris images by hierarchical multi-class classification. This classification takes various characteristics of different fake iris patterns into account. All kinds of fake iris patterns are divided into two categories by their fake areas. The process is designed as two steps to identify two categories of fake iris images respectively. Experimental results demonstrate an extremely higher accuracy of iris liveness detection than other state-of-the-art algorithms. The proposed HMC remarkably achieves the best results with nearly 100% accuracy on NDContact, CASIA-Iris-Interval, CASIA-Iris-Syn and LivDetIris-2017-Warsaw datasets. The method also achieves the best results with 100% accuracy on a hybrid dataset which consists of ND-Contact and LivDet-Iris-2017-Warsaw dataset</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/yan-2018-liveness/yan-2018-liveness.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/yan-2018-liveness/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/code_link" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/fei-liu/">Fei Liu</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/guangqi-hou/">Guangqi Hou</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>European Conference On Computer Vision (ECCV2018)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-eccv-2018/">
      <img src="/publication/wang-eccv-2018/featured_hu89b223e321a4ca8216deff9ea011293b_984792_918x517_fill_q90_lanczos_center_3.png" class="article-banner" alt="End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-eccv-2018/">End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN</a>
  </h3>

  
  <a href="/publication/wang-eccv-2018/" class="summary-link">
    <div class="article-style">
      <p>Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2Â dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-eccv-2018/wang-eccv-2018.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-eccv-2018/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/Pseudo4DCNN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/lingxiao-he/">Lingxiao He</a></span>, <span ><a href="/author/haiqing-li/">Haiqing Li</a></span>, <span ><a href="/author/yunfan-liu/">Yunfan Liu</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-ccbr-2017/">
      <img src="/publication/ren-ccbr-2017/featured_hud016b3b807fef695e860c6ba6c857519_259448_918x517_fill_q90_lanczos_smart1_3.PNG" class="article-banner" alt="Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-ccbr-2017/">Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation</a>
  </h3>

  
  <a href="/publication/ren-ccbr-2017/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we study the problem of partial person reidentification (re-id). This problem is more difficult than general person re-identification because the body in probe image is not full. We propose a novel method, similarity-guided sparse representation (SG-SR), as a robust solution to improve the discrimination of the sparse coding. There are three main components in our method. In order to include multi-scale information, a dictionary consisting of features extracted from multiscale patches is established in the first stage. A low rank constraint is then enforced on the dictionary based on the observation that its subspaces of each class should have low dimensions. After that, a classification model is built based on a novel similarity-guided sparse representation which can choose vectors that are more similar to the probe feature vector. The results show that our method outperforms existing partial person re-identification methods significantly and achieves state-of-theart accuracy.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-ccbr-2017/ren-ccbr-2017.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-ccbr-2017/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/code_link" target="_blank" rel="noopener">
  Code
</a>




  
    
  





















  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2016">
          
            





  






<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/guangqi-hou/">Guangqi Hou</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span ><a href="/author/zilei-wang/">Zilei Wang</a></span>, <span ><a href="/author/tieniu-tan/">Tieniu Tan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2016 IEEE International Conference on Image Processing (ICIP2016)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-icip-2016/">
      <img src="/publication/wang-icip-2016/featured_hu3c02e33041bfa332ff03a96bef775598_2084884_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A simple and robust super resolution method for light field images">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-icip-2016/">A simple and robust super resolution method for light field images</a>
  </h3>

  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-icip-2016/wang-icip-2016.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-icip-2016/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/ICIP.2016.7532600" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2016">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/guangqi-hou/">Guangqi Hou</a></span>, <span ><a href="/author/chi-zhang/">Chi Zhang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Smart Photonic and Optoelectronic Integrated Circuits XVIII (SPIE2016)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hou-spie-2016/">
      <img src="/publication/hou-spie-2016/featured_hubf8459d431c9dd9ea3d4bee732584890_509196_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="4D light-field sensing system for people counting">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hou-spie-2016/">4D light-field sensing system for people counting</a>
  </h3>

  
  <a href="/publication/hou-spie-2016/" class="summary-link">
    <div class="article-style">
      <p>Counting the number of people is still an important task in social security applications, and a few methods based on video surveillance have been proposed in recent years. In this paper, we design a novel optical sensing system to directly acquire the depth map of the scene from one light-field camera. The light-field sensing system can count the number of people crossing the passageway, and record the direction and intensity of rays at a snapshot without any assistant light devices. Depth maps are extracted from the raw light-ray sensing data. Our smart sensing system is equipped with a passive imaging sensor, which is able to naturally discern the depth difference between the head and shoulders for each person. Then a human model is built. Through detecting the human model from light-field images, the number of people passing the scene can be counted rapidly. We verify the feasibility of the sensing system as well as the accuracy by capturing real-world scenes passing single and multiple people under natural illumination.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1117/12.2212974" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hou-spie-2016/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  



















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1117/12.2212974" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/%E7%A7%A6%E7%90%B3%E7%90%B3/">ç§¦ç³ç³</a></span>, <span ><a href="/author/%E9%99%86%E6%9E%97%E7%AE%AD/">é™†æž—ç®­</a></span>, <span ><a href="/author/%E7%9F%B3%E6%98%A5/">çŸ³æ˜¥</a></span>, <span ><a href="/author/%E5%90%B4%E5%88%9A/">å´åˆš</a></span>, <span ><a href="/author/%E7%8E%8B%E4%BA%91%E9%BE%99/">çŽ‹äº‘é¾™</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>å†œä¸šæœºæ¢°å­¦æŠ¥</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/qinlinlin-ny-2015/">
      <img src="/publication/qinlinlin-ny-2015/featured_hu9969b5e7d19ae730d4cbdf51394077ba_34758_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="åŸºäºŽç‰©è”ç½‘çš„æ¸©å®¤æ™ºèƒ½ç›‘æŽ§ç³»ç»Ÿè®¾è®¡">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/qinlinlin-ny-2015/">åŸºäºŽç‰©è”ç½‘çš„æ¸©å®¤æ™ºèƒ½ç›‘æŽ§ç³»ç»Ÿè®¾è®¡</a>
  </h3>

  
  <a href="/publication/qinlinlin-ny-2015/" class="summary-link">
    <div class="article-style">
      <p>æ ¹æ®çŽ°ä»£æ¸©å®¤ç›‘æŽ§ä¸Žç®¡ç†éœ€æ±‚ï¼ŒåŸºäºŽç‰©è”ç½‘æŠ€æœ¯æ¡†æž¶ï¼Œè®¾è®¡å¹¶å®žçŽ°äº†ä¸€ç§åŸºäºŽç‰©è”ç½‘çš„æ¸©å®¤æ™ºèƒ½ç›‘æŽ§ç³»ç»Ÿã€‚ç³»ç»Ÿç”±çŽ°åœºç›‘æŽ§å­ç³»ç»Ÿã€è¿œç¨‹ç›‘æŽ§å­ç³»ç»Ÿå’Œæ•°æ®åº“3éƒ¨åˆ†ç»„æˆã€‚é‡‡ç”¨åŸºäºŽåˆ†å¸ƒå¼CANæ€»çº¿çš„ç¡¬ä»¶ç³»ç»Ÿå®žçŽ°çŽ¯å¢ƒæ•°æ®çš„å®žæ—¶é‡‡é›†ä¸Žè®¾å¤‡æŽ§åˆ¶ï¼Œå°†åˆ†å¸ƒå›¾æ³•åº”ç”¨äºŽé‡‡é›†ç³»ç»Ÿç¦»å¼‚æ•°æ®çš„åœ¨çº¿æ£€æµ‹ã€‚ä¸ºäº†æé«˜è¿œç¨‹ç›‘æŽ§å­ç³»ç»Ÿçš„å“åº”é€Ÿåº¦ä¸Žäº¤äº’æ€§ï¼Œé‡‡ç”¨äº†åŸºäºŽå¼‚æ­¥JavaScriptå’ŒXMLæŠ€æœ¯ï¼ˆAjaxï¼‰çš„Webæ•°æ®äº¤äº’æ–¹å¼ã€‚ç»“åˆæ¸©å®¤çŽ¯å¢ƒè°ƒæŽ§çš„ç‰¹ç‚¹ï¼Œå°†åŸºäºŽæ··æ‚è‡ªåŠ¨æœºæ¨¡åž‹çš„æ¸©å®¤æ¸©åº¦ç³»ç»Ÿæ™ºèƒ½æŽ§åˆ¶ç®—æ³•åº”ç”¨äºŽå®žé™…ç³»ç»Ÿï¼Œå®žçŽ°äº†æ¸©å®¤çŽ¯å¢ƒçš„è‡ªåŠ¨è°ƒæŽ§ã€‚ä¸ºä¿è¯è®¾å¤‡æŽ§åˆ¶çš„å®‰å…¨æ€§ï¼Œé‡‡ç”¨è½®è¯¢æ³•å®žçŽ°äº†çŽ°åœºç›‘æŽ§å­ç³»ç»Ÿå’Œè¿œç¨‹ç›‘æŽ§å­ç³»ç»Ÿä¸­è®¾å¤‡çŠ¶æ€çš„åŒæ­¥ï¼Œå¹¶å°†åŸºäºŽZernikeçŸ©çš„å›¾åƒè¯†åˆ«æŠ€æœ¯åº”ç”¨äºŽåŒå‘åž‹è®¾å¤‡çš„çŠ¶æ€æ£€æµ‹ï¼Œå®žçŽ°è®¾å¤‡çš„è‡ªåŠ¨æ ¡å‡†ã€‚è¯•éªŒè¡¨æ˜Žç³»ç»Ÿæ•°æ®ä¼ è¾“ç¨³å®šï¼ŒçŽ¯å¢ƒè°ƒæŽ§å¯é ï¼Œæ»¡è¶³çŽ°ä»£æ¸©å®¤æ™ºèƒ½ç›‘æŽ§çš„éœ€æ±‚ã€‚</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDTEMP&amp;filename=NYJX201503038" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/qinlinlin-ny-2015/cite.bib">
  Cite
</button>














  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/qinlinlin-ny-2015/CSAM2015.pdf" target="_blank" rel="noopener">
  Awards1
</a>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/qinlinlin-ny-2015/CSAM2015.png" target="_blank" rel="noopener">
  Awards2
</a>









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.6041/j.issn.1000-1298.2015.03.038" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.401ba81ed535f2e599b326a63fb33682.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic Website Builder</a>
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>

  <p class="powered-by">
    Welcome visitors
    <a href=" " target="_blank">
      <img src="https://counter.digits.net/?counter={bd8be506-bb7a-bac4-35b6-69103d06bd08}&template=simple"
        alt="Hit Counter by Digits" style="margin: 0 auto;" />
    </a>
  </p>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
