<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Databases for Download | Smart Individual Recognition</title>
    <link>http://localhost:1313/dataset/</link>
      <atom:link href="http://localhost:1313/dataset/index.xml" rel="self" type="application/rss+xml" />
    <description>Databases for Download</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 13 Feb 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_3.png</url>
      <title>Databases for Download</title>
      <link>http://localhost:1313/dataset/</link>
    </image>
    
    <item>
      <title>CASIA-Iris-Africa</title>
      <link>http://localhost:1313/dataset/casia-iris-africa/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Various forms of iris databases had been published that address some specific research problems and are made available to the biometrics research community such as: constraint databases; mobile iris; multispectral; synthetics; iris at a distance; contact lenses; liveness detection; Aging, etc. However, even though these databases have addressed many of the problems mentioned above, most of them contain subjects of primarily Caucasian and Asian docents with very few Africans and many times zero Africans. It is particularly challenging as this has created a research blind spot for African cohorts in these databases. Despite many of the reported investigative studies on racial bias in face biometrics, very few iris racial bias-related studies have been published, which can be due to the insufficient number of other races databases such as the Africans. Recently, face recognition algorithms have been reported to be biased toward specific demographics. Most prominently, many investigative studies have reported higher false-positive rates in African subject cohorts than in other cohorts. Due to the unavailability of the required quantity of African cohorts in the publicly available iris databases, similar studies would be difficult to replicate on iris biometrics. We presents a mainly African dataset that can be useful in addressing some of these challenges.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The iris images were captured in Nigeria, Africa. The capturing exercise was conducted across various locations in multiple sessions over three months. Due to logistical constraints, the capturing exercise was limited to the northern part of Nigeria. Each subject volunteer was asked to sign a consent form authorizing the data to be used for only research purposes. The iris sensor device for capturing the images is the IKUSBE30 iris sensor from IrisKing. The setup for the capturing exercise is shown in Fig. 1. Each volunteer was asked to hold the device, standing or sitting, based on their preference. As such, the iris images were captured at various degrees of head postures.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/1.png&#34; title=&#34;Iris Capturing set-ups (indoor and outdoor)&#34;&gt;
Fig. 1: Iris Capturing set-ups (indoor and outdoor)&lt;/p&gt;
&lt;p&gt;The procedure for image capturing is in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Step 1: Straight Gaze Capture: In this step, the volunteer was asked to hold the sensor and gaze into its lenses while moving the eyes in small ranges to the left, right, up, or down for approximately 3 minutes. During this time, images were periodically captured automatically at constant intervals. The eye movement ensures that the iris position and orientation are highly diversified across the captured images. Samples of the captured images in this step are shown in Fig. 2 (left).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Step 2: Open-Close Capture:  In this step, the volunteer was asked to open and close the eyes while gazing into the device for approximately 3 minutes. The iris images were automatically captured within this period. The essence of the opening and closing of the eyes is to ensure that the images contain irises of various sizes and degrees of occlusion, from fully closed eyes to half open and fully open eyes. This procedure will ultimately improve image diversity. Exemplary samples are shown in Fig. 2 (right).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/2.png&#34; title=&#34;Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&#34;&gt;
Fig. 2: Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&lt;/p&gt;
&lt;p&gt;After the capturing sessions, the stored images were processed. The processing steps comprise automatic image selection to discard images with duplicate content, followed by manual image selection that guarantees the selected images&amp;rsquo; reliability. In the automatic selection, all the images from a single eye of one subject were organized into a matrix, with each column representing an image. The images of the subject&amp;rsquo;s eyes were then processed, and duplicates were removed using this procedure. The remaining images were then manually processed. The manual selection involves one-by-one human inspection of each image to identify damaged images and those images with no irises captured in them. Some sample images of one subject from the generated dataset are shown in Fig. 3, the labelled sampled images in Fig. 4, the summary of the database is presented in Table 1 and the database subjects&amp;rsquo; age distribution is shown in Fig. 5.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/3.png&#34; title=&#34;Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 3: Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 4&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/4.png&#34; title=&#34;Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 4: Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;Tab. 1: Summary of the generated database
&lt;img alt=&#34;Figure 5&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/table.png&#34; title=&#34;Summary of the generated database&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 6&#34; src=&#34;http://localhost:1313/dataset/casia-iris-africa/figures/5.png&#34; title=&#34;Age distribution of the generated database&#34;&gt;
Fig. 5: Age distribution of the generated database&lt;/p&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image folder: This contain the iris images. Each image is named with SubjectID_Eye_ImageNumber.jpg. That represents:
&lt;ul&gt;
&lt;li&gt;SubjectID: a unique number for each subject&lt;/li&gt;
&lt;li&gt;Eye is a letter that can be either “L” for left eye or “R” for right eye&lt;/li&gt;
&lt;li&gt;ImageNumber is a sequential number for images for the same subject&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iris_edge folder: This contain the corresponding 1-pixel iris edge binary images&lt;/li&gt;
&lt;li&gt;iris_edge_mask folder: This contain the corresponding iris binary masks of the images&lt;/li&gt;
&lt;li&gt;pupil_edge folder: This contain the corresponding 1-pixel pupil edge binary images&lt;/li&gt;
&lt;li&gt;pupil_edge_mask folder: This contain the corresponding pupil binary masks of the images&lt;/li&gt;
&lt;li&gt;params folder: This contain the corresponding ini files that describes radius and orientation both the iris and pupil used in the generation of iris edge, iris mask, pupil edge, pupil and mask images&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-Iris-Africa collected by the Chinese Academy of Sciences’ Institute of Automation (CASIA)” and a reference to “CASIA-Iris-Africa Image Database, 
&lt;a href=&#34;http://biometrics.idealtest.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Polar</title>
      <link>http://localhost:1313/dataset/casia-polar/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-polar/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;CASIA-Polar is a face anti-spoofing dataset based on polarization imaging, which takes advantage of polarization information in material classification to enable robust face anti-spoofing research.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The hardware system used to collect this dataset is shown in Fig.1, consists of a Lucid Phoenix PHX050S-P polarized camera equipped with Sony’s polarization sensor and a Mindvision MVGE501GC-T RGB camera.&lt;/p&gt;
&lt;p&gt;Types of spoofing attacks include printed paper, printed glossy photographs, electronic displays, silicone masks, rubber masks, and customized silicone prosthetic heads. High-quality face samples are first captured by the MVGE501GC-T RGB camera and these high-quality samples are then printed on paper and photographs or displayed on a computer screen to produce artifacts. At the same time, both genuine and spoofing attack face samples were captured when the subjects and presentation attack were standing at or be placed at six distances, i.e. 1m, 1.5m, 2m, 3m, 4m, and 5m.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/1_1.png&#34;&gt;
&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/1_2.png&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset includes 22,174 samples from 121 subjects. All subjects had visible light and polarized images taken. The genuine face has 14,698 samples, while the spoofing attack has about 7,476 samples.&lt;/p&gt;
&lt;p&gt;Figure 2 shows an example of the face images collected in the dataset, with the first row being the visible image and the second row is the corresponding DoLP image.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-polar/figures/2.png&#34; title=&#34;The attacks present in CASIA-Polar&#34;&gt;
Fig.2: The attacks present in CASIA-Polar&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved.&lt;/p&gt;
&lt;p&gt;E-mail: 
&lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Iris-LFLD</title>
      <link>http://localhost:1313/dataset/casia-iris-lfld/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-lfld/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The dataset for iris liveness detection based on light field (LF) imaging was collected by [1], wherein the first author is one of our collaborators. We have got the authority from the authors of [1] and released the LF focal stack data.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The dataset was captured using a 
&lt;a href=&#34;http://cripac.ia.ac.cn/CN/column/item105.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lab-produced microlens based LF camera&lt;/a&gt; and a commercial device 
&lt;a href=&#34;http://www.irisking.com/pron.php?id=523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IKUSB-E30&lt;/a&gt; under near-infrared (NIR) illumination. The setup of dataset collection was shown in Fig.1.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/capture_setup.png&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset contains 504 samples from 14 subjects, consisting of 230 LF images of bona fide iris and 274 LF images of spoofing iris. The respective sample number of the PAIs, i.e. printed papers, printed photos, and electronic display are 18, 122, 134.&lt;/p&gt;
&lt;p&gt;An example of raw LF image containing both eyes printed on photos is shown in Fig.2. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/lf_raw_sample.png&#34; title=&#34;An example of raw LF image containing both eyes printed on photos.&#34;&gt;
Fig.2: An example of raw LF image containing both eyes printed on photos. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;The LF toolbox released by [2] was utilized to decode raw LF images into 4D LF data. The eye regions were cropped from the same location of each sub-aperture image (SAI). The spatial resolution of each SAI after cropping is $128 \times 96$, and the angular resolution is $7 \times 7$. Examples from the same subject&amp;rsquo;s right eye in the dataset are shown in Fig.3.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/dataset_samples.png&#34; title=&#34;Examples from the same subject&amp;#39;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&#34;&gt;
Fig. 3: Examples from the same subject&amp;rsquo;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&lt;/p&gt;
&lt;p&gt;The rendered focal stack via digital refocusing has 145 slices around the best focus plane.&lt;/p&gt;
&lt;p&gt;The details of the adopted database is listed in Table 1.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Table 1&#34; src=&#34;http://localhost:1313/dataset/casia-iris-lfld/figures/details.png&#34; title=&#34;Details of the adopted database&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the  database are reserved.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] 宋平, 黄玲, 王云龙, 刘菲, 孙哲南. 基于计算光场成像的虹膜活体检测方法. 自动化学报, 2019, 45(9): 1701-1712. (Ping Song, Huang Ling, Wang Yunlong, Liu Fei, and Sun Zhenan. Iris liveness detection based on light field imaging. ACTA AUTOMATICA SINICA, 45(9):1701–1712, 2019.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[2] Donald G Dansereau, Oscar Pizarro, and Stefan B Williams, “Decoding, calibration and rectification for lenselet-based plenoptic cameras,” in Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1027–1034.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Iris-Degradation</title>
      <link>http://localhost:1313/dataset/casia-iris-degradation/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-iris-degradation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This work presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.&lt;/p&gt;
&lt;h2 id=&#34;description-of-casia-iris-degradation&#34;&gt;Description of CASIA-Iris-Degradation&lt;/h2&gt;
&lt;p&gt;CASIA-Iris-Degradation contains 36,539 images from 255 Asian people. All images were collected under NIR illumination and two eyes were captured simultaneously. Details of the proposed database are shown in the table below.
&lt;img alt=&#34;Details of the proposed database&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/Statistics.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;setup-of-image-collection&#34;&gt;Setup of image collection&lt;/h3&gt;
&lt;p&gt;As shown in the figure below, we built a collection room using black-out cloth. The curtain on one side of the room can be opened and closed artificially to control the natural light.
Inside, there were camera (A), light sources (B, C), volunteer (D), and four directional markers (1-4).
Each volunteer was asked to sit down, put their chin on the holder (0.75 m from the camera), keep their head as still as possible, and move their eyes according to instructions.
To simulate the off-angle situation, the subject was required to look along a set of directions indicated by four markers in the visual field, while in other cases look straight ahead (i.e., the midpoint of marker 1 and 4).
&lt;img alt=&#34;environment and equipment&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/newcx1camera.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-proposed-database&#34;&gt;Statistics of the proposed database&lt;/h3&gt;
&lt;p&gt;To simulate real image degradation, the proposed database is divided into four categories, and each of them is separated into three to five subsets as follows:&lt;/p&gt;
&lt;h4 id=&#34;illumination&#34;&gt;Illumination&lt;/h4&gt;
&lt;p&gt;The intensity of the VW light source was adjusted to four levels: Dark (0%), Weak (25%), Medium (50%), Strong (100%) to change the pupil size.
In addition, images under natural light were also collected (with Dark level).
Note that the intensity in other categories was set to the Medium level by default.&lt;/p&gt;
&lt;h4 id=&#34;off-angle&#34;&gt;Off-angle&lt;/h4&gt;
&lt;p&gt;There are four directions: (1) Left, (2) Upper left, (3) Upper right, (4) Right.
The left and right are in the horizontal direction, while the upper left and right angles are both 45 degrees.&lt;/p&gt;
&lt;h4 id=&#34;nonideal-eye-state&#34;&gt;Nonideal eye state&lt;/h4&gt;
&lt;p&gt;Since it is difficult to keep eyes open all the time, we collected images of closed, squinted, and half-open eyes.
Although most images of the former two classes have no effective iris region and are accompanied by blur, they can be used to train eye state detectors for fatigue driving detection or other relevant scenarios.&lt;/p&gt;
&lt;h4 id=&#34;occlusion&#34;&gt;Occlusion&lt;/h4&gt;
&lt;p&gt;For occlusion, volunteers were required to wear glasses, masks and using a hand to cover the mouth and nose. During this section, the NIR light source was randomly moved slightly to generate light spots. Meanwhile, some glasses also had stains on the surface to occlude the iris.
An unexpected observation is that for some elderly volunteers, their eyelids droop naturally, resulting in severe occlusion, which should arouse more attention.&lt;/p&gt;
&lt;p&gt;More samples and annotations are presented below.
&lt;img alt=&#34;More samples&#34; src=&#34;http://localhost:1313/dataset/casia-iris-degradation/supp_images.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA-Iris-Degradation database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Iris-Degradation-V1.0 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Evaluation Benchmark for High-throughput Iris Recognition at a Distance</title>
      <link>http://localhost:1313/dataset/blurred_iris_benchmark/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/blurred_iris_benchmark/</guid>
      <description>&lt;h2 id=&#34;1-introduction&#34;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A key problem of iris recognition at a distance is that a large portion of captured iris images is nonideal because of narrow depth of field (DoF), noncooperative user movement, incongruous exposure time and so on. Current iris recognition systems usually filter out these low-quality images using strict criteria of image quality evaluation (IQA). However, this strategy inevitably leads to a waste of device capacity and low throughput. Therefore, a better and practical solution is to make the utmost of degraded iris images for personal identification. We announce the availability of a long-range captured dataset containing 3,756 iris images of various degradation factors from 98 subjects. An evaluation benchmark is built upon the dataset for a comparative study on preprocessing and recognition of NIR iris images in high-throughput scenarios. The datasets, manual annotations and evaluation toolkit are publicly available.&lt;/p&gt;
&lt;h2 id=&#34;2-descriptions-and-statistics-of-the-database&#34;&gt;&lt;strong&gt;2. Descriptions and Statistics of the Database&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;image-collection&#34;&gt;&lt;strong&gt;Image collection&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- &lt;iframe height=498 width=510 src=&#34;collection_glasses.mp4&#34;&gt; --&gt;
&lt;p&gt;The schematic and setup of blur-varying iris image collection of this database at a distance are shown as following.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Schematic and setup of NIR iris image collection at a distance and in less cooperative environments.&#34; src=&#34;http://localhost:1313/dataset/blurred_iris_benchmark/capture_setting.png&#34; title=&#34;Image Collection&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next generation of CASIA-LR-Cam bundled with NIR illumination at a wavelength of 830 nm was employed as the capturing device. Its standoff distance is approximately 1.2 meters with a DoF of over 20 centimeters. The field of view (FoV) is approximately 20 degrees. The device was placed in an indoor environment under no extra lighting sources. During the process of image collection, the subjects were obliged to move freely in the restricted square area 1.0~1.4 meters away from the device. Specifically, they could casually step forward and backward, left and right.&lt;/p&gt;
&lt;p&gt;While moving inside the restricted area, the subjects were guided by the indication signal on the GUI screen to look at different directions for approximately 30 seconds in a single session. Two separate sessions were launched in the daytime under the same conditions, and the interval was one week. If the subject was wearing glasses, he or she needed to take them off in either of the two sessions (&lt;code&gt;play the video and see&lt;/code&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 1 With Glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_glasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 2 No glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_noglasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;The acquired iris image sequences were captured at 5~10 frames per second. The resolution of each frame was 3840x2748. The frames in which irides were completely invisible caused by blinking or squinting were thrown away. Then evenly spaced images are extracted from the processed sequence every 5 frames. On average, approximately 20 images of each subject were retained.&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-dataset&#34;&gt;&lt;strong&gt;Statistics of the dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attributes&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;The database&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Camera Type&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;CASIA-LR-Cam II&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Illumination&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;NIR and natural lighting sources&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total pixel&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3840x2748&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cropped eye region&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;640x480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sessions&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Two separate sessions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Institution of subjects&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Graduate students and staff of CASIA and TAfIRT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standoff distance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.0~1.4m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Working mode&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Step freely within a moderate square area&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Depth of field&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ca. 20cm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of subjects&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Classes&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Images&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3,765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Images per class&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ca. 19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pairs of Images&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;39,418 intraclass and 7,406,312 interclass&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;mannual-annotations&#34;&gt;&lt;strong&gt;Mannual Annotations&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Each image in the dataset is manually annotated with binary maps of iris masks, inner and outer iris boundaries, upper and lower eyelids, and sclera masks shown as below.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Two annotated instances in the dataset.&#34; src=&#34;http://localhost:1313/dataset/blurred_iris_benchmark/annotations.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;evaluation-toolkit&#34;&gt;&lt;strong&gt;Evaluation Toolkit&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;IrisStat_V3.0.rar&#34;&gt;IrisStat_V3.0.rar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The package of evaluation toolkit is organized as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IrisIQA
│
└───config/
│
└───out/ 
│
└───utils/
|     computeMotionblur.m
|     computeSharpness.m
|     ini2struct.m
|     Integral.m
|     progressbar.m
│     struct2ini.m
|
└───MotionBlur_Main.m
│
└───Sharpness_Main.m

 
Segmentation
│
└───config/
│
└───out/ 
│
└───utils/
|    evalSeg.m
|    Hausdorff.m
|    ini2struct.m
|    progressbar.m
|    struct2ini.m 
│
└───IrisSeg_Main.m

Recognition
│
└───config/
│
└───out/ 
│
└───utils/
|     ACC.m
|     Bitshift.m
|     colors.mat
|     compute_iriscode_sim.m
|     compute_om_sim.m
|     compute_vector_sim.m
|     draw_CMC_curve.m
|     draw_DET_curve.m
|     EER.m
|     IdentiACC.m
|     linspecer.m
|     Merge_Multi_CMC_Curve.m
|     Merge_Multi_Det_Curve.m
|     plot_styles.mat
|     progressbar.m
|     VerfiACC.m
│
└───IrisRec_main.m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The main function of iris IQA evaluating sharpness is &lt;code&gt;Sharpness_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris segmentation evaluation is &lt;code&gt;IrisSeg_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris recognition evaluation is &lt;code&gt;IrisRec_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;&lt;strong&gt;Database Organization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database package comprises the following components organized in multiple directories.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|
circle_params
|  │
|  └───xxxL(R)_xx.ini
|
ellipse_params
|  │
|  └───xxxL(R)_xx.ini 
|
image
|  │
|  └───xxxL(R)_xx.jpg
|
iris_edge
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
iris_mask
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
sclera_mask
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
vis_result
|  │
|  └───xxxL(R)_xx.png
|
vis_result_new
|  │
|  └───xxxL(R)_xx.png
|
imgList.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file naming rule is &amp;ldquo;&lt;code&gt;xxxL(R)_xx&lt;/code&gt;&amp;rdquo;, where &amp;ldquo;&lt;code&gt;xxx&lt;/code&gt;&amp;rdquo; denotes the unique identifier of the subject, &amp;ldquo;&lt;code&gt;L&lt;/code&gt;&amp;rdquo; denotes left eye and &amp;ldquo;&lt;code&gt;R&lt;/code&gt;&amp;rdquo; denotes right eye and &amp;ldquo;&lt;code&gt;xx&lt;/code&gt;&amp;rdquo; denotes the index of the image in the class, e.g., &lt;code&gt;001L_01&lt;/code&gt;. All the filenames of the iris images and belonging classes are stored in &lt;code&gt;imgList.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;&lt;strong&gt;Copyright Note and Contacts&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the dataset collected by Smart Iris Recognition (SIR) group from the Chinese Academy of Sciences, Institute of Automation (CASIA)”.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, a non-student researcher must manually sign the 
&lt;a href=&#34;license_agreement.pdf&#34;&gt;License Agreement&lt;/a&gt; and agree to observe the restrictions. The signed document should be digitized and sent through email to: 
&lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Face-Africa</title>
      <link>http://localhost:1313/dataset/casia-face-africa/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-face-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the years, numerous face databases have been published that brought about exciting breakthrough in the facial biometric research field, most especially from the recent trend of deep learning contributions. However, as investigated by researchers, most of these databases are demographically imbalanced and often contain few number of African cohorts. Of those with relatively large number of the Africans, the databases are usually wild (downloaded from the internet or digitalised from printed photographs). Methods that adopt these skewed databases often exhibits some form of performance bias that can result to unintended consequences for real time applications. As such, there is need for more demographically inclusive datasets. CASIA-Face-Africa is developed to provide solution to this problem. It is an all-African database that is made to be used as a complementary database with the existing databases to balance the number of the African cohorts in the published datasets and improve their demographic inclusiveness.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;DESCRIPTION&lt;/h2&gt;
&lt;p&gt;The database images were captured at various locations in Nigeria, Africa. About 1150 volunteers participated in the capturing exercise. The images of each subject were captured concurrently using 3 cameras. Two visible wavelength (VW) cameras and one near-infrared (NIR) camera. The capturing was done in various sessions over a period of 3 months. Some of the subjects have images captured in multiple sessions while majority of the subjects have their images captured in a single session.
&lt;img alt=&#34;Figure 1&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/1.png&#34;&gt;
&lt;em&gt;Figure 1: The cameras arrangement set-up was made to be same in all the capturing sessions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For each capturing instance, 3 to 10 still images were captured by each camera at a fixed time interval of 1 to 3 seconds. For some subjects, an external illumination light source was used for capturing additional images of that subjects. Also, some subjects were asked to use face accessory such as eye glasses for multiple capturing. The captured images were then organized and their land mark labelled. The organized database comprises a total of 38,546 images from 1,183 subjects. Specifically, 12,063 images captured by VW camera 1 at the resolution of 1332×1080, 13,232 images are captured by VW camera 2 at the resolution of 787 × 962, and 13,251 images are captured by NIR camera at the resolution of 983 × 877. Some samples of the captured images are shown in Figure 2, Figure 3 and Figure 4.
&lt;img alt=&#34;Figure 2&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/2.png&#34;&gt;
&lt;em&gt;Figure 2: Sample of a single subject images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 3&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/3.png&#34;&gt;
&lt;em&gt;Figure 3: Sample of subject Expressions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;Figure 4&#34; src=&#34;http://localhost:1313/dataset/casia-face-africa/4.png&#34;&gt;
&lt;em&gt;Figure 4: Sample of labelled face images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images folder: This contain the actual face images. Each file is named as SubjectID_ImageNumber.jpg. The SubjectID is a unique number for each subject and the ImageNumber is a sequential number for images of the same subject.&lt;/li&gt;
&lt;li&gt;Subjects folder: This contain the corresponding ini files that describe the unique subjects in the images. Each file is named with the ID of the subject as SubjectID.ini.&lt;/li&gt;
&lt;li&gt;Attributes folder: This contain the corresponding ini files that describes each individual face image. Each file is named with its corresponding face image name as SubjectID_ImageNumber.ini&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Example of the names: ID-V-All-Ep1.ini, ID-I-Split-Ep3.ini, etc. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Face-Africa collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] Muhammad Jawad, Yunlong Wang, Caiyong Wang, Kunbo Zhang, Zhenan Sun. “CASIA-Face-Africa: A Large-scale African Face Image Database,” IEEE Transactions on Information Forensics and Security (TIFS), vol.16, pp. 3634-3646, 2021.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-IrisV4</title>
      <link>http://localhost:1313/dataset/casia-irisv4/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/dataset/casia-irisv4/</guid>
      <description>&lt;h2 id=&#34;download-the-whole-database-186gbhttpbiometricsidealtestorgdownloaddbdoid4&#34;&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download the whole database (1.86GB)&lt;/a&gt;&lt;/h2&gt;
&lt;p style=&#34;text-align: center&#34;&gt;
OR&lt;br&gt;
Download the separated subsets below&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Interval (30.9MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Lamp (390MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Twins (60MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Distance(767MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Thousand (490MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Syn (171MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;With the pronounced need for reliable personal identification, iris recognition has become an important enabling technology in our society. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. Automatic iris recognition has to face unpredictable variations of iris images in real-world applications. For example, recognition of iris images of poor quality, nonlinearly deformed iris images, iris images at a distance, iris images on the move, and faked iris images all are open problems in iris recognition. A basic work to solve the problems is to design and develop a high quality iris image database including all these variations. Moreover, a novel iris image database may help identify some frontier problems in iris recognition and leads to a new generation of iris recognition technology.&lt;/p&gt;
&lt;p&gt;CASIA Iris Image Database (CASIA-Iris) developed by our research group has been released to the international biometrics community and updated from CASIA-IrisV1 to CASIA-IrisV3 since 2002. More than 3,000 users from 70 countries or regions have downloaded CASIA-Iris and much excellent work on iris recognition has been done based on these iris image databases. Although great progress of iris recognition has been achieved since 1990s, the rapid growth of iris recognition applications has clearly highlighted two challenges, i.e. usability and scalability.&lt;/p&gt;
&lt;p&gt;Usability is the largest bottleneck of current iris recognition. It is a trend to develop long-range iris image acquisition systems for friendly user authentication. However, iris images captured at a distance are more challenging than traditional close-up iris images. Lack of long-range iris image data in the public domain has hindered the research and development of next-generation iris recognition systems.&lt;/p&gt;
&lt;p&gt;Most current iris recognition methods have been typically evaluated on medium sized iris image databases with a few hundreds of subjects. However, more and more large-scale iris recognition systems are deployed in real-world applications. Many new problems are met in classification and indexing of large-scale iris image databases. So scalability is another challenging issue in iris recognition.&lt;/p&gt;
&lt;p&gt;In order to promote research on long-range and large-scale iris recognition systems,  we are pleased to release to the public domain CASIA Iris Image Database V4.0 (or CASIA-IrisV4 for short).&lt;/p&gt;
&lt;h2 id=&#34;2-brief-descriptions-and-statistics-of-the-database&#34;&gt;2. Brief Descriptions and Statistics of the Database&lt;/h2&gt;
&lt;p&gt;CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn.&lt;/p&gt;
&lt;p&gt;CASIA-IrisV4 contains a total of 54,601 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.&lt;/p&gt;
&lt;h3 id=&#34;21--casia-iris-interval&#34;&gt;2.1  CASIA-Iris-Interval&lt;/h3&gt;
&lt;p&gt;Iris images of CASIA-Iris-Interval were captured with our self-developed close-up iris camera (Fig.1). The most compelling feature of our iris camera is that we have designed a circular NIR LED array, with suitable luminous flux for iris imaging. Because of this novel design, our iris camera can capture very clear iris images (see Fig.2). CASIA-Iris-Interval is well-suited for studying the detailed texture features of iris images.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.1&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.1.jpg&#34;&gt;
Fig.1 The self-developed iris camera used for collection of CASIA-Iris-Interval
&lt;img alt=&#34;Fig.2&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.2.jpg&#34;&gt;
Fig.2 Example iris images in CASIA-Iris-Interval&lt;/p&gt;
&lt;h3 id=&#34;22--casia-iris-lamp&#34;&gt;2.2  CASIA-Iris-Lamp&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Lamp was collected using a hand-held iris sensor produced by OKI (Fig.3). A lamp was turned on/off close to the subject to introduce more intra-class variations when we collected CASIA-Iris-Lamp. Elastic deformation of iris texture (Fig.4) due to pupil expansion and contraction under different illumination conditions is one of the most common and challenging issues in iris recognition. So CASIA-Iris-Lamp is good for studying problems of non-linear iris normalization and robust iris feature representation.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.3&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.3.jpg&#34;&gt;
Fig.3 The hand-held iris camera used for collection of CASIA-Iris-Lamp
&lt;img alt=&#34;Fig.4&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.4.jpg&#34;&gt;
Fig.4 Example iris images in CASIA-Iris-Lamp&lt;/p&gt;
&lt;h3 id=&#34;23--casia-iris-twins&#34;&gt;2.3  CASIA-Iris-Twins&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Twins contains iris images of 100 pairs of twins, which were collected during Annual Twins Festival in Beijing using OKI&amp;rsquo;s IRISPASS-h camera (Fig.5). Although iris is usually regarded as a kind of phenotypic biometric characteristics and even twins have their unique iris patterns, it is interesting to study the dissimilarity and similarity between iris images of twins.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.5&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.5.jpg&#34;&gt;
Fig.5 Example iris images in CASIA-Iris-Twins&lt;/p&gt;
&lt;h3 id=&#34;24--casia-iris-distance&#34;&gt;2.4  CASIA-Iris-Distance&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Distance contains iris images captured using our self-developed long-range multi-modal biometric image acquisition and recognition system (LMBS, Fig.6). The advanced biometric sensor can recognize users from 3 meters away by actively searching iris, face or palmprint patterns in the visual field via an intelligent multi-camera imaging system. The LMBS is human-oriented by fusing computer vision, human computer interaction and multi-camera coordination technologies and improves greatly the usability of current biometric systems. The iris images of CASIA-Iris-Distance were captured by a high resolution camera so both dual-eye iris and face patterns are included in the image region of interest (Fig. 7). And detailed facial features such as skin pattern are also visible for multi-modal biometric information fusion.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.6&#34; src=&#34;./V4Fig.6.jpg&#34;&gt;
Fig.6  The biometric sensor used for collection of CASIA-Iris-Distance
&lt;img alt=&#34;Fig.7&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.7.jpg&#34;&gt;
Fig.7  An example image in CASIA-Iris-Distance&lt;/p&gt;
&lt;h3 id=&#34;25--casia-iris-thousand&#34;&gt;2.5  CASIA-Iris-Thousand&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Fig. 8) produced by 
&lt;a href=&#34;Http://www.irisking.com&#34;&gt;IrisKing&lt;/a&gt;. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.8&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.8.jpg&#34;&gt;
Fig.8 The iris camera used for collection of CASIA-Iris-Thousand
&lt;img alt=&#34;Fig.9&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.9.jpg&#34;&gt;
Fig.9  An example image in CASIA-Iris-Thousand&lt;/p&gt;
&lt;h3 id=&#34;26--casia-iris-syn&#34;&gt;2.6  CASIA-Iris-Syn&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in [1] (Fig. 10). Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in [1] that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Fig.10&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.10.jpg&#34;&gt;
Fig. 10  Flowchart of the iris texture synthesis method for generation of CASIA-Iris-Syn
&lt;img alt=&#34;Fig.11&#34; src=&#34;http://localhost:1313/dataset/casia-irisv4/V4Fig.11.jpg&#34;&gt;
Fig. 11  Example iris images in CASIA-Iris-Syn&lt;/p&gt;
&lt;h2 id=&#34;3-database-organization&#34;&gt;3. Database Organization&lt;/h2&gt;
&lt;p&gt;The file name of each image in CASIA-IrisV4 is unique to each other and denotes some useful properties associated with the image such as subset category, left/right/double, subject ID, class ID, image ID etc. The file naming rules of all six subsets are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Interval are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Interval/YYY/S1YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Lamp are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Lamp/YYY/E/S2YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Twins are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Twins\XX\YE\S3XXYENN.jpg&lt;/p&gt;
&lt;p&gt;XX: the index of family&lt;/p&gt;
&lt;p&gt;Y: the identifier to one of the twins&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Distance are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Distance/YYY/S4YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘D’ denotes dual-eye iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Thousand are stored as:&lt;/p&gt;
&lt;p&gt;$ root path$ /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Syn are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Syn/YYY/S6YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘S’ denotes it is a synthesized iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-copyright-note-and-contacts&#34;&gt;4. Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-IrisV4 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)” and a reference to “CASIA Iris Image Database, 
&lt;a href=&#34;http://biometrics.idealtest.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included. A copy of all reports and papers that are for public or general release that use the CASIA-IrisV4 should be forwarded upon release or publication to:&lt;/p&gt;
&lt;p&gt;Professor Tieniu Tan&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;or send electronic copies to 
&lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Questions regarding this database can be addressed to Dr. Zhenan Sun at&lt;/p&gt;
&lt;p&gt;Dr. Zhenan Sun&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;Tel: +86 10 8261 0278&lt;/p&gt;
&lt;p&gt;Fax: +86 10 6255 1993&lt;/p&gt;
&lt;p&gt;Email: 
&lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;publications&#34;&gt;Publications&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tieniu Tan, Zhaofeng He, Zhenan Sun, &amp;ldquo;Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition&amp;rdquo;, Image and Vision Computing, Vol.28, No. 2, 2010, pp.223-230.&lt;/li&gt;
&lt;li&gt;T. Tan and L. Ma, “Iris Recognition: Recent Progress and Remaining Challenges”, Proc. of SPIE, Vol. 5404, pp. 183-194, 12-13 Apr 2004, Orlando, USA.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, &amp;ldquo;Ordinal Measures for Iris Recognition,&amp;rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 12, 2009, pp. 2211 - 2226.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, &amp;ldquo;Towards Accurate and Fast Iris Segmentation for Iris Biometrics&amp;rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 9, 2009, pp.1670 - 1684.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, Y. Wang and D. Zhang, “Personal Identification Based on Iris Texture Analysis”, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Vol. 25, No. 12, pp.1519-1533, 2003.&lt;/li&gt;
&lt;li&gt;Li Ma, Tieniu Tan, Yunhong Wang and Dexin Zhang, “Efficient Iris Recognition by Characterizing Key Local Variations”, IEEE Trans. on Image Processing, Vol. 13, No.6, pp. 739- 750, 2004.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, D. Zhang and Y. Wang, “Local Intensity Variation Analysis for Iris Recognition, Pattern Recognition”, Vol.37, No.6, pp. 1287-1298, 2004.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, IEEE Transactions on Systems, Man, and Cybernetics-Part Cï¼ŒVolume 35, Issue 3, 2005, pp.435 - 441.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition”, Proceedings of International Workshop on Biometric Authentication (BioAW), Lecture Notes in Computer Science, Vol.3087, 2004, pp. 270-282.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 418-425.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Robust Direction Estimation of Gradient Vector Field for Iris Recognition”, Proceedings of the 17th International Conference on Pattern Recognition, Vol.2, 2004, pp.783-786.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Cascading Statistical And Structural Classifiers For Iris Recognition”, Proceedings of IEEE International Conference on Image Processing, 2004, pp.1261-1264.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Iris Recognition Based on Non-local Comparisons”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 67-77.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, and Xianchao Qiu, &amp;ldquo;Graph Matching Iris Image Blocks with Local Binary Pattern&amp;rdquo;, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 366-372.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, “Robust and Fast Assessment of Iris Image Quality”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 464 - 471.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “An Appearance-Based Method for Iris Detection”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp.1091-1096, 2004, Korea.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Junzhou Huang, Tieniu Tan, Zhenan Sun and Li Ma, “An Iris Image Synthesis Method Based on PCA and Super-Resolution”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 4, pp. 471-474, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “A Fast and Robust Iris Localization Method Based on Texture Segmentation”, Proc. of SPIE, Vol. 5404, pp. 401-408, 2004, USA.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Li Ma, Tieniu Tan and Zhenan Sun, “An Iris Recognition Algorithm Using Local Extreme Points”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 442-449.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Tieniu Tan and Zhenan Sun, “Fast Recursive Mathematical Morphological Transforms”, Proc. of the 3rd International Conference on Image and Graphics (ICIG), pp. 422-425, 2004, Hong Kong.&lt;/li&gt;
&lt;li&gt;Junzhou Huang, Tieniu Tan, Li Ma, and Yunhong Wang, Phase Correlation Based Iris Image Registration Model, Journal of Computer Science and Technology, Vol.20, No.3, pp.419-425, May 2005.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Based on Multichannel Gabor Filtering”, Proc. of the 5th Asian Conference on Computer Vision (ACCV), Vol. I, pp.279-283, Jan 22-25, 2002, Melbourne, Australia.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Using Circular Symmetric Filters”, Proc. of IAPR International Conference on Pattern Recognitionï¼ˆICPRï¼‰, Vol. II, pp. 414-417, August 11-15, 2002, Quebec, Canada.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, T. N. Tan and Y. H. Wang, “Learning-Based Enhancement Model of Iris”, Proc. of British Machine Vision Conference (BMVC), pp. 153-162, 2003.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, and Y. H. Wang and T. N. Tan, “Iris Model Based on Local Orientation Description”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp. 954-959, 2004, Korea.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, T. N. Tan and J. L. Cui, “A New Iris Segmentation Model”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 3, pp. 554-557, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, J. L. Cui and T. N. Tan, “Noise Removal and Impainting Model for Iris Image”, Proc. of IEEE International Conference on Image Processing (ICIP), pp. 869-872, 2004, Singapore.&lt;/li&gt;
&lt;li&gt;Yuqing He, Yangsheng Wang and Tieniu Tan, “Iris Image Capture System Design For Personal Identification”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 546-552.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, &amp;ldquo;Robust and Fast Assessment of Iris Image quality&amp;rdquo;, Proc. of International Conference of Biometrics, pp. 464-471, 2006.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, &amp;ldquo;Nonlinear Iris Deformation Correction Based on Gaussian Model&amp;rdquo;, International Conference of Biometrics, pp 780-789, 2007.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Yufei Han, Zhenan Sun and Tieniu Tan, Palmprint Image Synthesis: A Preliminary Study, Proc. of IEEE International Conference on Image Processing, 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, Synthesis of Large Realistic Iris Databases Using Patch-based Sampling, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Xianchao Qiu, Zhenan Sun and Tieniu Tan, Counterfeit Iris Detection Based on Texture Analysis, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan and Zhenan Sun, “Iris Localization via Pulling and Pushing”, Proc. of the 18th IEEE International Conference on Pattern Recognition (ICPR&#39;06), Vol.4, pp. 366-369, 2006, Hongkong.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun, Xianchao Qiu, Cheng Zhong and Wenbo Dong, Boosting Ordinal Features for Iris Recognition, Proc. of the 26th IEEE International Conference on Computer Vision and Pattern Recognition (CVPR’08) , pp. 1-8, June 23-28, Alaska, USA&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Zhenan Sun, Tieniu Tan and Xianchao Qiu, Enhanced Usability of Iris Recognition via Efficient User Interface and Iris Image Restoration, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, Robust Eyelid, Eyelash and Shadow Localization for Iris Recognition”, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California, Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Zhuoshi Wei, “Efficient Iris Spoof Detection via Boosted Local Binary Patterns”, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1080-1090, 2009.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Coarse Iris Classification by Learned Visual Dictionary&amp;rdquo;, In Proc. of The 2nd International Conference on Biometrics, pp. 770–779, Seoul, Korea, Aug. 2007.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Global Texture Analysis of Iris Images for Ethnic Classification&amp;rdquo;, In Proc. of The 1st International Conference on Biometrics, pp. 411–418, Hong Kong, China. Jan. 2006.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Self-adaptive iris image acquisition system, Proc. SPIE vol. 6944, 1-9, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, How to make iris recognition easier?, Proc. of the 19th International Conference on Pattern Recognition, pp.1-4, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Zhuoshi Wei, &amp;ldquo;Quality-based dynamic threshold for iris matching&amp;rdquo;, In Proceedings of IEEE International Conference on Image Processing, 2009.&lt;/li&gt;
&lt;li&gt;Long Zhang, Zhenan Sun, Tieniu Tan and Shungeng Hu, &amp;ldquo;Robust Biometric Key Extraction Based on Iris Cryptosystem&amp;rdquo;, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1060-1069, 2009.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Contact lens detection based on weighted LBP, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Statistics of Local Surface Curvatures for Mis-Localized Iris Detection, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Texture Removal for Adaptive Level Set based Iris Segmentation&amp;rdquo;, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Hierarchical Fusion of Face and Iris for Personal Identification&amp;rdquo;, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
