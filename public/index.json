[{"authors":["Min Ren"],"categories":null,"content":"Min Ren received the Ph.D. degree from School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received his B.E. degree in mechanical engineering and automation from National University of Defense Technology, China, in 2013. His research focuses on pattern recognition and biometrics.\nHe is working as a post doc at Beijing Normal University(BNU).\n","date":1720426325,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720426325,"objectID":"6b0177bf3e3641546dd08edd5d3b5858","permalink":"http://localhost:1313/author/min-ren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/min-ren/","section":"authors","summary":"Min Ren received the Ph.D. degree from School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Min Ren","type":"authors"},{"authors":["Yunlong Wang"],"categories":null,"content":"Yunlong Wang is currently an Associate Professor with NLPR, MAIS, CASIA, China. He received the B.E. degree and the M.S. degree in Department of Automation, University of Science and Technology of China. His reserach focuses on pattern recognition, machine learning, light field photography, and biometrics.\n","date":1720426325,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720426325,"objectID":"d707692c1920926e70fb31a5d08cf28e","permalink":"http://localhost:1313/author/yunlong-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunlong-wang/","section":"authors","summary":"Yunlong Wang is currently an Associate Professor with NLPR, MAIS, CASIA, China. He received the B.E. degree and the M.S. degree in Department of Automation, University of Science and Technology of China.","tags":null,"title":"Yunlong Wang","type":"authors"},{"authors":["Zhenan Sun"],"categories":null,"content":"Zhenan Sun is currently an Professor with NLPR, MAIS, CASIA, China. He received the B.E. degree in industrial automation from Dalian University of Thchnology, and the M.S. degree in system engineering from Huazhong University of Science and Technology, and the Ph.D. degree in pattern recognition and intelligent system from CASIA in 1999, 2002, and 2006 respectively. His research focuses on biometrics and pattern recognition.\n","date":1720426325,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720426325,"objectID":"51cdf962e6cea34e169b1e05383e506e","permalink":"http://localhost:1313/author/zhenan-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenan-sun/","section":"authors","summary":"Zhenan Sun is currently an Professor with NLPR, MAIS, CASIA, China. He received the B.E. degree in industrial automation from Dalian University of Thchnology, and the M.S. degree in system engineering from Huazhong University of Science and Technology, and the Ph.","tags":null,"title":"Zhenan Sun","type":"authors"},{"authors":["Tieniu Tan"],"categories":null,"content":"Tieniu Tan received the B.Sc. degree in electronic engineering from Xi’an Jiaotong University, China, in 1984, and the M.Sc. and Ph.D. degrees in electronic engineering from Imperial College London, U.K., in 1986 and 1989, respectively. He is currently a Professor with the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA). His current research interests include biometrics, image and video understanding, information hiding, and information forensics. He is a Fellow of IEEE and the IAPR (International Association of Pattern Recognition).\n","date":1719389525,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1719389525,"objectID":"99b8d6894d308cad9c2a786ee12cd352","permalink":"http://localhost:1313/author/tieniu-tan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tieniu-tan/","section":"authors","summary":"Tieniu Tan received the B.Sc. degree in electronic engineering from Xi’an Jiaotong University, China, in 1984, and the M.Sc. and Ph.D. degrees in electronic engineering from Imperial College London, U.K., in 1986 and 1989, respectively.","tags":null,"title":"Tieniu Tan","type":"authors"},{"authors":["Caiyong Wang"],"categories":null,"content":"Caiyong Wang is currently a Lecturer with the School of Electrical and Information Engineering, Beijing University of Civil Engineering and Architecture (BUCEA), China. He received the Ph.D. degree from New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China, in 2020 (supervised by Prof. Zhenan Sun). He received the B.E. degree in applied mathematics from Xinjiang University, China, in 2013, and the M.S. degree in computational mathematics from Xiamen University, China, in 2016. His research interests include biometrics, computer vision, and deep learning.\n","date":1718582400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718582400,"objectID":"2dbc393cff8e8ff4bbce641bbcac1161","permalink":"http://localhost:1313/author/caiyong-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/caiyong-wang/","section":"authors","summary":"Caiyong Wang is currently a Lecturer with the School of Electrical and Information Engineering, Beijing University of Civil Engineering and Architecture (BUCEA), China. He received the Ph.D. degree from New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China, in 2020 (supervised by Prof.","tags":null,"title":"Caiyong Wang","type":"authors"},{"authors":["Jianze Wei"],"categories":null,"content":"Jianze Wei received the Ph.D. degree from the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. and M.S. degrees in communication engineering from the Civil Aviation University of China, Tianjin, China, in 2015 and 2018, respectively. His current research interests include biometrics, machine learning and computer vision.\nHe is working as a post doc at Institute of Microelectronics of the Chinese Academy of Sciences (IMECAS).\n","date":1718323925,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718323925,"objectID":"6562cb9785fb6ba4e80235ccd2944d72","permalink":"http://localhost:1313/author/jianze-wei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianze-wei/","section":"authors","summary":"Jianze Wei received the Ph.D. degree from the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Jianze Wei","type":"authors"},{"authors":["Kunbo Zhang"],"categories":null,"content":"Kunbo Zhang is currnetly an Associate Professor at NLPR, MAIS, CASIA, China. He received the B.E. degree in Automation from Beijing Institute of Technology in 2006, and the M.Sc. and Ph.D. degrees in Mechanical Engineering from State University of New York at Stony Brook, U.S., in 2008 and 2011, repectively. Between 2011 and 2016 he worked as a machine vision engineer of Advanced Manufacturing Engineering group in Nexteer Automotive, Michigan, U.S.. His current resserch interests focus on light-field photography, biometric imaging, robot vision, human-robot interaction, and intelligent manufacturing.\n","date":1715760725,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715760725,"objectID":"571b00afbac8a6388ea006ab6babc12d","permalink":"http://localhost:1313/author/kunbo-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kunbo-zhang/","section":"authors","summary":"Kunbo Zhang is currnetly an Associate Professor at NLPR, MAIS, CASIA, China. He received the B.E. degree in Automation from Beijing Institute of Technology in 2006, and the M.Sc. and Ph.","tags":null,"title":"Kunbo Zhang","type":"authors"},{"authors":["Zihui Yan"],"categories":null,"content":"Zihun Yan received the Ph.D. degree from the School of Artificial Intelligence, University of Chinese Academy of Sciences, and New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received his B.E. degree in automation from Tsinghua University, China, in 2015. His research focuses on pattern recognition and biometrics.\nHe is working as an engineer at CHINA ENERGY INVESTMENT (CHN ENERGY).\n","date":1705276800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705276800,"objectID":"194ed2105ad8d80c82adf41c08b2355e","permalink":"http://localhost:1313/author/zihui-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihui-yan/","section":"authors","summary":"Zihun Yan received the Ph.D. degree from the School of Artificial Intelligence, University of Chinese Academy of Sciences, and New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Zihui Yan","type":"authors"},{"authors":["Jawad Muhammad"],"categories":null,"content":"Jawad Muhammad received the B.E. degree in computer engineering from Bayero University, Kano, Nigeria, in 2010, and the M.S. degree in computer engineering from Selcuk University, Konya, Turkey, in 2014. He received the Ph.D. degree from School of Artificial Intelligence, University of Chinese Academy of Sciences, China, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA). His current research interests include biometrics, computer vision, and robot vision.\nHe is working as an engineer at the Joint European Torus (JET), UK.\n","date":1705047125,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705047125,"objectID":"a3cbbd6c32991f73e52dd0d46e12df95","permalink":"http://localhost:1313/author/jawad-muhammad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jawad-muhammad/","section":"authors","summary":"Jawad Muhammad received the B.E. degree in computer engineering from Bayero University, Kano, Nigeria, in 2010, and the M.S. degree in computer engineering from Selcuk University, Konya, Turkey, in 2014. He received the Ph.","tags":null,"title":"Jawad Muhammad","type":"authors"},{"authors":["Junxing Hu"],"categories":null,"content":"Junxing Hu received the Ph.D. degree from School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree in software engineering from the Dalian University of Technology (DUT), China, in 2017, and the M.S. degree in computer science from the Institute of Software, Chinese Academy of Sciences (ISCAS), China, in 2020. His current research interests include biometrics, computer vision, and deep leaning.\nHe is working as a management trainee at JD.com, Inc.(JD).\n","date":1705047125,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705047125,"objectID":"daf756d7d9916ef86df32c113688593f","permalink":"http://localhost:1313/author/junxing-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junxing-hu/","section":"authors","summary":"Junxing Hu received the Ph.D. degree from School of Artificial Intelligence, University of Chinese Academy of Sciences, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Junxing Hu","type":"authors"},{"authors":["Tianhao Lu"],"categories":null,"content":"Tianhao Lu received the M.S. degree from Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received B.E. degree in Beijing Jiaotong University. His research focuses on pattern recognition.\nHe is working as an engineer at Baidu Inc..\n","date":1673943125,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1673943125,"objectID":"dfbd4d629b89375f297ef2c4cc336568","permalink":"http://localhost:1313/author/tianhao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianhao-lu/","section":"authors","summary":"Tianhao Lu received the M.S. degree from Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Tianhao Lu","type":"authors"},{"authors":["Mengmeng Cui"],"categories":null,"content":"","date":1671005525,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1671005525,"objectID":"d1e0fe4cf4f9c802ccc3e0deabb192e9","permalink":"http://localhost:1313/author/mengmeng-cui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mengmeng-cui/","section":"authors","summary":"","tags":null,"title":"Mengmeng Cui","type":"authors"},{"authors":["Leyuan Wang"],"categories":null,"content":"Leyuan Wang received the Master degree from New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree from Department of Forensic Audio and Video Technology, Criminal Investigation Police University of China in 2018. His research focuses on pattern recognition, Computational Photography, Biometrics and Criminal Law.\nHe is currently pursuing the Ph.D. degree with Beijing University of Posts and Telecommunications (BUPT).\n","date":1667463125,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1667463125,"objectID":"498111336a79b02d59baa003a4fe4d27","permalink":"http://localhost:1313/author/leyuan-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leyuan-wang/","section":"authors","summary":"Leyuan Wang received the Master degree from New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Leyuan Wang","type":"authors"},{"authors":["Mupei Li"],"categories":null,"content":"Mupei Li is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.S. degree in college of automation, Beijing Institude of Technology, China, in 2022. His research focuses on biometrics, computer vision, and deep learning.\n","date":1665648725,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665648725,"objectID":"102feb9259b8b6b49acc5937e551462d","permalink":"http://localhost:1313/author/mupei-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mupei-li/","section":"authors","summary":"Mupei Li is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Mupei Li","type":"authors"},{"authors":["Zhengquan Luo"],"categories":null,"content":"Zhengquan Luo received the Ph.D. degree with Department of Automation, University of Science and Technology of China(USTC), and also with Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree majoring in automation, University of Science and Technology of China(USTC), China in 2018.\nHe is working as a post doc at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI).\n","date":1665648725,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665648725,"objectID":"63da2a2730da1d919f264a90bcd12076","permalink":"http://localhost:1313/author/zhengquan-luo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhengquan-luo/","section":"authors","summary":"Zhengquan Luo received the Ph.D. degree with Department of Automation, University of Science and Technology of China(USTC), and also with Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.","tags":null,"title":"Zhengquan Luo","type":"authors"},{"authors":["Yong He"],"categories":null,"content":"Yong He is currently an Engineer with Artificial Intelligence, University of Chinese Academy of Sciences, China, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA). He received his M.S. degree in Computer Architect from Hunan University of Technology, China, in 2020. His research focuses on pattern recognition and biometrics.\n","date":1655453525,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655453525,"objectID":"1b7e2599977c3660480c3c43c0f9442f","permalink":"http://localhost:1313/author/yong-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yong-he/","section":"authors","summary":"Yong He is currently an Engineer with Artificial Intelligence, University of Chinese Academy of Sciences, China, and the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA).","tags":null,"title":"Yong He","type":"authors"},{"authors":["Wanting Zhou"],"categories":null,"content":"Wanting Zhou finished postdoc work with the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. She received the B.E. degree in Automation Technology, the M.S. degree in Control Engineering and the Ph.D. degree in Detection Technology and Automation Devices from North China Electric Power University, Beijing, China in 2014, 2016 and 2019. She was funded by the China Scholarship Council and did some research as a joint PhD. student in the University of Cambridge in 2018. She received the National Postdoctoral Program for Innovative Talents from China Postdoctoral Science Foundation in 2019. Her current research interests include biometrics, computer vision, and deep leaning.\nShe is working as an associate professor at Beijing University of Posts and Telecommunications (BUPT).\n","date":1617783125,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617783125,"objectID":"11dcc0c8ae6cdc5b962dcbf0a89d8887","permalink":"http://localhost:1313/author/wanting-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wanting-zhou/","section":"authors","summary":"Wanting Zhou finished postdoc work with the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Wanting Zhou","type":"authors"},{"authors":["Hongda Liu"],"categories":null,"content":"Hongda Liu is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.S. degree in college of computer science and technology, Jilin University, China, in 2020. His research focuses on biometrics, computer vision, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5796074cb306c3f5f5a81d5934c966aa","permalink":"http://localhost:1313/author/hongda-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hongda-liu/","section":"authors","summary":"Hongda Liu is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Hongda Liu","type":"authors"},{"authors":["Tao Qin"],"categories":null,"content":"Tao Qin received the M.S. degree from Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received B.E. degree in Automation from the School of Mechanical and Electrical Engineering at Zhoukou Normal University. His research focuses on pattern recognition.\nHe is working as an engineer at Social Touch Co. Ltd. (SocialTouch).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4a8270a272e9fe6cde36a754db74479a","permalink":"http://localhost:1313/author/tao-qin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tao-qin/","section":"authors","summary":"Tao Qin received the M.S. degree from Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Tao Qin","type":"authors"},{"authors":["Xiaofan Wang"],"categories":null,"content":"Xiaofan Wang received the M.S. degree from the Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.S. degree in Shool of Mathematical Sciences, Beihang University, China, in 2021. His research focuses on biometrics, computer vision, and deep learning.\nHe is currently pursing the Ph.D. degree at the Institute of Microelectronics of the Chinese Academy of Sciences (IMECAS).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"798f40775e4d341c11cce5ea6db06bd7","permalink":"http://localhost:1313/author/xiaofan-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaofan-wang/","section":"authors","summary":"Xiaofan Wang received the M.S. degree from the Hunan University of Technology (HUT), jointly with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Xiaofan Wang","type":"authors"},{"authors":["Yanting Wu"],"categories":null,"content":"Yanting is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b54702e194f523f9b8e170332c450aa6","permalink":"http://localhost:1313/author/yanting-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yanting-wu/","section":"authors","summary":"Yanting is currently pursuing the Ph.D. degree with New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Yanting Wu","type":"authors"},{"authors":["Yu Tian"],"categories":null,"content":"Yu Tian is is currently an Engineer at the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA). He received the B.E. degree in Information Counter Technology and the M.S. degree in Information and Communication Engineering from North University of China, China in 2016 and 2019. His research focuses on computational photography, pattern recognition and biometrics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"36763d8d13111cc30c012c934e8db19d","permalink":"http://localhost:1313/author/yu-tian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yu-tian/","section":"authors","summary":"Yu Tian is is currently an Engineer at the New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA).","tags":null,"title":"Yu Tian","type":"authors"},{"authors":["Yuchen Zheng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0ba13bdf78c0db0d89068400fd828707","permalink":"http://localhost:1313/author/yuchen-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuchen-zheng/","section":"authors","summary":"","tags":null,"title":"Yuchen Zheng","type":"authors"},{"authors":["Zixuan Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f28bba436c104952b758c1e86d2fd17c","permalink":"http://localhost:1313/author/zixuan-zhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zixuan-zhu/","section":"authors","summary":"","tags":null,"title":"Zixuan Zhu","type":"authors"},{"authors":null,"categories":null,"content":"Send Email to 1047450724@qq.com ","date":1723075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723075200,"objectID":"f6f6692b1fd72cfa76791af88c447ed3","permalink":"http://localhost:1313/post/welcome/","publishdate":"2024-08-08T00:00:00Z","relpermalink":"/post/welcome/","section":"post","summary":"Send Email to 1047450724@qq.com ","tags":[],"title":"Welcome to Communicate with Us","type":"post"},{"authors":["Min Ren","Yuhao Zhu","Yunlong Wang","Yongzhen Huang","Zhenan Sun"],"categories":[],"content":"","date":1720426325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720426325,"objectID":"cc3e11eea225be2cf3f92c971cc55554","permalink":"http://localhost:1313/publication/ren-tifs-2024/","publishdate":"2022-07-08T08:58:18Z","relpermalink":"/publication/ren-tifs-2024/","section":"publication","summary":"Deep neural networks have proven to be highly effective in the face recognition task, as they can map raw samples into a discriminative high-dimensional representation space. However, understanding this complex space proves to be challenging for human observers. In this paper, we propose a novel approach that interprets deep face recognition models via facial attributes. To achieve this, we introduce a two-stage framework that recovers attributes from the deep face representations. This framework allows us to quantitatively measure the significance of facial attributes in relation to the recognition model. Moreover, this framework enables us to generate sample-specific explanations through counterfactual methodology. These explanations are not only understandable but also quantitative. Through the proposed approach, we are able to acquire a deeper understanding of how the recognition model conceptualizes the notion of identity and understand the reasons behind the error decisions made by the deep models. By utilizing attributes as an interpretable interface, the proposed method marks a paradigm shift in our comprehension of deep face recognition models. It allows a complex model, obtained through gradient backpropagation, to effectively communicate with humans. The source code is available here, or you can visit this website: https://github.com/RenMin1991/Facial-Attribute-Recovery.","tags":["face recognition"],"title":"Understanding Deep Face Representation via Attribute Recovery","type":"publication"},{"authors":null,"categories":null,"content":"1. Understanding Deep Face Representation via Attribute Recovery Min Ren, Yuhao Zhu, Yunlong Wang, Yongzhen Huang, Zhenan Sun. Understanding Deep Face Representation via Attribute Recovery. IEEE Transactions on Information Forensics and Security ( Early Access )\nhttps://ieeexplore.ieee.org/document/10587012\nDeep neural networks have proven to be highly effective in the face recognition task, as they can map raw samples into a discriminative high-dimensional representation space. However, understanding this complex space proves to be challenging for human observers. In this paper, we propose a novel approach that interprets deep face recognition models via facial attributes. To achieve this, we introduce a two-stage framework that recovers attributes from the deep face representations. This framework allows us to quantitatively measure the significance of facial attributes in relation to the recognition model. Moreover, this framework enables us to generate sample-specific explanations through counterfactual methodology. These explanations are not only understandable but also quantitative. Through the proposed approach, we are able to acquire a deeper understanding of how the recognition model conceptualizes the notion of “identity” and understand the reasons behind the error decisions made by the deep models. By utilizing attributes as an interpretable interface, the proposed method marks a paradigm shift in our comprehension of deep face recognition models. It allows a complex model, obtained through gradient backpropagation, to effectively “communicate” with humans. The source code is available here, or you can visit this website: https://github.com/RenMin1991/Facial-Attribute-Recovery .\n","date":1720396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720396800,"objectID":"637b9caa242a73f63dfb22d037c3f705","permalink":"http://localhost:1313/post/recent-published-papers-2407/","publishdate":"2024-07-08T00:00:00Z","relpermalink":"/post/recent-published-papers-2407/","section":"post","summary":"Understanding Deep Face Representation via Attribute Recovery","tags":[],"title":"One paper was accepted to IEEE Transactions on Information Forensics and Security ( Early Access )","type":"post"},{"authors":["Min Ren","Yunlong Wang","Yuhao Zhu","Yongzhen Huang","Zhenan Sun","Qi Li","Tieniu Tan"],"categories":[],"content":"","date":1719389525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719389525,"objectID":"2e31a86affd6d6551bc054be303d7504","permalink":"http://localhost:1313/publication/ren-ijcv-2024/","publishdate":"2024-06-26T08:58:18Z","relpermalink":"/publication/ren-ijcv-2024/","section":"publication","summary":"Deep learning-based face recognition models are vulnerable to adversarial attacks.In contrast to general noises, the presence of imperceptible adversarial noises can lead to catastrophic errors in deep face recognition models. The primary difference between adversarial noise and general noise lies in its specificity. Adversarial attack methods give rise to noises tailored to the characteristics of the individual image and recognition model at hand. Diverse samples and recognition models can engender specific adversarial noise patterns, which pose significant challenges for adversarial defense. Addressing this challenge in the realm of face recognition presents a more formidable endeavor due to the inherent nature of face recognition as an open set task. In order to tackle this challenge, it is imperative to employ customized processing for each individual input sample. Drawing inspiration from the biological immune system, which can identify and respond to various threats, this paper aims to create an artificial immune system (AIS) to provide adversarial defense for face recognition. The proposed defense model incorporates the principles of antibody cloning, mutation, selection, and memory mechanisms to generate a distinct “antibody” for each input sample, where in the term “antibody” refers to a specialized noise removal manner. Furthermore,we introduce a self-supervised adversarial training mechanism that serves as a simulated rehearsal of immune system invasions. Extensive experimental results demonstrate the efficacy of the proposed method, surpassing state-of-the-art adversarial defense methods. The source code is available here, or you can visit this website: https://github.com/RenMin1991/SIDE.","tags":null,"title":"Artificial Immune System of Secure Face Recognition Against Adversarial Attacks","type":"publication"},{"authors":null,"categories":null,"content":"1. Artificial Immune System of Secure Face Recognition Against Adversarial Attacks Min Ren, Yunlong Wang, Yuhao Zhu, Yongzhen Huang, Zhenan Sun, Qi Li, Tieniu Tan. Artificial Immune System of Secure Face Recognition Against Adversarial Attacks. International Journal of Computer Vision\nhttps://arxiv.org/pdf/2406.18144\nDeep learning-based face recognition models are vulnerable to adversarial attacks.In contrast to general noises, the presence of imperceptible adversarial noises can lead to catastrophic errors in deep face recognition models. The primary difference between adversarial noise and general noise lies in its specificity. Adversarial attack methods give rise to noises tailored to the characteristics of the individual image and recognition model at hand. Diverse samples and recognition models can engender specific adversarial noise patterns, which pose significant challenges for adversarial defense. Addressing this challenge in the realm of face recognition presents a more formidable endeavor due to the inherent nature of face recognition as an open set task. In order to tackle this challenge, it is imperative to employ customized processing for each individual input sample. Drawing inspiration from the biological immune system, which can identify and respond to various threats, this paper aims to create an artificial immune system (AIS) to provide adversarial defense for face recognition. The proposed defense model incorporates the principles of antibody cloning, mutation, selection, and memory mechanisms to generate a distinct “antibody” for each input sample, where in the term “antibody” refers to a specialized noise removal manner. Furthermore,we introduce a self-supervised adversarial training mechanism that serves as a simulated rehearsal of immune system invasions. Extensive experimental results demonstrate the efficacy of the proposed method, surpassing state-of-the-art adversarial defense methods. The source code is available here, or you can visit this website: https://github.com/RenMin1991/SIDE.\n","date":1719187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719187200,"objectID":"ad9cae2c1a725ba7b96fc6f22b440d02","permalink":"http://localhost:1313/post/recent-published-papers-2403/","publishdate":"2024-06-24T00:00:00Z","relpermalink":"/post/recent-published-papers-2403/","section":"post","summary":"Artificial Immune System of Secure Face Recognition Against Adversarial Attacks","tags":[],"title":"One paper was accepted to International Journal of Computer Vision (IJCV 2024)","type":"post"},{"authors":["Caiyong Wang","Haiqing Li","Yixin Zhang","Guangzhe Zhao","Yunlong Wang","Zhenan Sun"],"categories":null,"content":"","date":1718582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718582400,"objectID":"7fd6e3faef756dd1a729335a5de77e6d","permalink":"http://localhost:1313/publication/wang-tbiom-2024-1/","publishdate":"2024-06-17T11:44:49.293511Z","relpermalink":"/publication/wang-tbiom-2024-1/","section":"publication","summary":"This paper investigates a deep learning based unified framework for accurate sclera segmentation and recognition, named Sclera-TransFuse. Unlike previous CNN-based methods, our framework incorporates Vision Transformer and CNN to extract complementary feature representations, which are beneficial to both subtasks. Specifically, for sclera segmentation, a novel two-stream hybrid model, referred to as Sclera-TransFuse-Seg, is developed to integrate classical ResNet-34 and recently emerging Swin Transformer encoders in parallel. The dual-encoders firstly extract coarse-and fine-grained feature representations at hierarchical stages, separately. Then a Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual-encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. With the results of sclera segmentation, the sclera ROI image is generated for sclera feature extraction. Additionally, a new sclera recognition model, termed as Sclera-TransFuse-Rec, is proposed by combining lightweight EfficientNet B0 and multi-scale Vision Transformer in sequential to encode local and global sclera vasculature feature representations. Extensive experiments on several publicly available databases suggest that our framework consistently achieves state-of-the-art performance on various sclera segmentation and recognition benchmarks, including the 8th Sclera Segmentation and Recognition Benchmarking Competition (SSRBC 2023). A UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through https://github.com/lhqqq/Sclera-TransFuse.","tags":["Smart Iris Recognition","Sclera segmentation","sclera recognition"],"title":"Sclera-TransFuse: Fusing Vision Transformer and CNN for Accurate Sclera Segmentation and Recognition","type":"publication"},{"authors":["Jianze Wei","Yunlong Wang","Xingyu Gao","Ran He","Zhenan Sun"],"categories":[],"content":"","date":1718323925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323925,"objectID":"64f330b965a70c46e35c7e524966cd98","permalink":"http://localhost:1313/publication/wei-tifs-2024_1/","publishdate":"2024-06-14T08:58:18Z","relpermalink":"/publication/wei-tifs-2024_1/","section":"publication","summary":"Accurate iris segmentation, especially around the iris inner and outer boundaries, is still a formidable challenge. Pixels within these areas are difficult to semantically distinguish since they have similar visual characteristics and close spatial positions. To tackle this problem, the paper proposes an iris segmentation graph neural network (ISeGraph) for accurate segmentation. ISeGraph regards individual pixels as nodes within the graph and constructs self-adaptive edges according to multi-faceted knowledge, including visual similarity, positional correlation, and semantic consistency for feature aggregation. Specifically, visual similarity strengthens the connections between nodes sharing similar visual characteristics, while positional correlation assigns weights according to the spatial distance between nodes. In contrast to the above knowledge, semantic consistency maps nodes into a semantic space and learns pseudo-labels to define relationships based on label consistency. ISeGraph leverages multi-faceted knowledge to generate self-adaptive relationships for accurate iris segmentation. Furthermore, a pixel-wise adaptive normalization module is developed to increase the feature discriminability. It takes informative features in the shallow layer as a reference to improve the segmentation features from a statistical perspective. Experimental results on three iris datasets illustrate that the proposed method achieves superior performance in iris segmentation, increasing the segmentation accuracy in areas near the iris boundaries.","tags":[],"title":"Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"1. Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation Zihui Yan, Lingxiao He, Yunlong Wang, Kunbo Zhang, Zhenan Sun, Tieniu Tan. \u0026ldquo;Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation\u0026rdquo;. Machine Intelligence Research (Volume: 21).\nhttps://doi.org/10.1007/s11633-023-1415-y\nIn the daily application of an iris-recognition-at-a-distance (IAAD) system, many ocular images of low quality are acquired. As the iris part of these images is often not qualified for the recognition requirements, the more accessible periocular regions are a good complement for recognition. To further boost the performance of IAAD systems, a novel end-to-end framework for multi-modal ocular recognition is proposed. The proposed framework mainly consists of iris/periocular feature extraction and matching, unsupervised iris quality assessment, and a score-level adaptive weighted fusion strategy. First, ocular feature reconstruction (OFR) is proposed to sparsely reconstruct each probe image by high-quality gallery images based on proper feature maps. Next, a brand new unsupervised iris quality assessment method based on random multiscale embedding robustness is proposed. Different from the existing iris quality assessment methods, the quality of an iris image is measured by its robustness in the embedding space. At last, the fusion strategy exploits the iris quality score as the fusion weight to coalesce the complementary information from the iris and periocular regions. Extensive experimental results on ocular datasets prove that the proposed method is obviously better than unimodal biometrics, and the fusion strategy can significantly improve the recognition performance.\n2. CASIA-Iris-Africa: A Large-scale African Iris Image Database Jawad Muhammad, Yunlong Wang, Junxing Hu, Kunbo Zhang, Zhenan Sun. \u0026ldquo;CASIA-Iris-Africa: A Large-scale African Iris Image Database\u0026rdquo;. Machine Intelligence Research (Volume: 21).\nhttps://doi.org/10.1007/s11633-022-1402-8\nIris biometrics is a phenotypic biometric trait that has proven to be agnostic to human natural physiological changes. Research on iris biometrics has progressed tremendously, partly due to publicly available iris databases. Various databases have been available to researchers that address pressing iris biometric challenges such as constraint, mobile, multispectral, synthetics, long-distance, contact lenses, liveness detection, etc. However, these databases mostly contain subjects of Caucasian and Asian docents with very few Africans. Despite many investigative studies on racial bias in face biometrics, very few studies on iris biometrics have been published, mainly due to the lack of racially diverse large-scale databases containing sufficient iris samples of Africans in the public domain. Furthermore, most of these databases contain a relatively small number of subjects and labelled images. This paper proposes a large-scale African database named Chinese Academy of Sciences Institute of Automation (CASIA)-Iris-Africa that can be used as a complementary database for the iris recognition community to mediate the effect of racial biases on Africans. The database contains 28 717 images of 1 023 African subjects (2 046 iris classes) with age, gender, and ethnicity attributes that can be useful in demographically sensitive studies of Africans. Sets of specific application protocols are incorporated with the database to ensure the database’s variability and scalability. Performance results of some open-source state-of-the-art (SOTA) algorithms on the database are presented, which will serve as baseline performances. The relatively poor performances of the baseline algorithms on the proposed database despite better performance on other databases prove that racial biases exist in these iris recognition algorithms.\n3. Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation Jianze Wei, Yunlong Wang, Xingyu Gao, Ran He, Zhenan Sun. \u0026ldquo;Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation\u0026rdquo;. IEEE Transactions on Information Forensics and Security ( Volume: 19).\nhttps://doi.org/10.1109/TIFS.2024.3407508\nAccurate iris segmentation, especially around the iris inner and outer boundaries, is still a formidable challenge. Pixels within these areas are difficult to semantically distinguish since they have similar visual characteristics and close spatial positions. To tackle this problem, the paper proposes an iris segmentation graph neural network (ISeGraph) for accurate segmentation. ISeGraph regards individual pixels as nodes within the graph and constructs self-adaptive edges according to multi-faceted knowledge, including visual similarity, positional correlation, and semantic consistency for feature aggregation. Specifically, visual similarity strengthens the connections between nodes sharing similar visual characteristics, while positional correlation assigns weights according to the spatial distance between nodes. In contrast to the above knowledge, semantic consistency maps nodes into a semantic space and learns pseudo-labels to define relationships based on label consistency. ISeGraph leverages multi-faceted knowledge to generate self-adaptive relationships for accurate iris segmentation. Furthermore, a pixel-wise adaptive normalization module is developed to increase the feature discriminability. It takes informative features in the shallow layer as a reference to improve the segmentation features from a statistical perspective. Experimental results on three iris datasets illustrate that the proposed method achieves superior performance in iris segmentation, increasing the segmentation accuracy in areas near the iris boundaries.\n","date":1718323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323200,"objectID":"0d5e305222a17709c78acfda1445eed3","permalink":"http://localhost:1313/post/recent-published-papers-24/","publishdate":"2024-06-14T00:00:00Z","relpermalink":"/post/recent-published-papers-24/","section":"post","summary":"Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation, CASIA-Iris-Africa: A Large-scale African Iris Image Database, Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation","tags":["smart iris recognition"],"title":"Two paper were accepted to Mahcine Intelligence Research (MIR) and another one was accpeted to TIFS 2024","type":"post"},{"authors":["Kaiduo Zhang","Muyi Sun","Jianxin Sun","Kunbo Zhang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1715760725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715760725,"objectID":"2a689966a90af37cfcb9ef47e7482c50","permalink":"http://localhost:1313/publication/kunbo-ijcv-2024/","publishdate":"2023-05-15T08:58:18Z","relpermalink":"/publication/kunbo-ijcv-2024/","section":"publication","summary":"Generating human images from open-vocabulary text descriptions is an exciting but challenging task. Previous methods (i.e., Text2Human) face two challenging problems: (1) they cannot well handle the open-vocabulary setting by arbitrary text inputs (i.e., unseen clothing appearances) and heavily rely on limited preset words (i.e., pattern styles of clothing appearances); (2) the generated human image is inaccuracy in open-vocabulary settings. To alleviate these drawbacks, we propose a flexible diffusion-based framework, namely HumanDiffusion, for open-vocabulary text-driven human image generation (HIG). The proposed framework mainly consists of two novel modules: the Stylized Memory Retrieval (SMR) module and the Multi-scale Feature Mapping (MFM) module. Encoded by the vision-language pretrained CLIP model, we obtain coarse features of the local human appearance. Then, the SMR module utilizes an external database that contains clothing texture details to refine the initial coarse features. Through SMR refreshing, we can achieve the HIG task with arbitrary text inputs, and the range of expression styles is greatly expanded. Later, the MFM module embedding in the diffusion backbone can learn fine-grained appearance features, which effectively achieves precise semantic-coherence alignment of different body parts with appearance features and realizes the accurate expression of desired human appearance. The seamless combination of the proposed novel modules in HumanDiffusion realizes the freestyle and high accuracy of text-guided HIG and editing tasks. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SOTA) performance, especially in the open-vocabulary setting.","tags":null,"title":"Open-Vocabulary Text-Driven Human Image Generation","type":"publication"},{"authors":null,"categories":null,"content":"1. Open-Vocabulary Text-Driven Human Image Generation Kaiduo Zhang, Muyi Sun, Jianxin Sun, Kunbo Zhang, Zhenan Sun, Tieniu Tan. \u0026ldquo;Open-Vocabulary Text-Driven Human Image Generation\u0026rdquo;. 2024 International Journal of Computer Vision.\nhttps://link.springer.com/article/10.1007/s11263-024-02079-7\nGenerating human images from open-vocabulary text descriptions is an exciting but challenging task. Previous methods (i.e., Text2Human) face two challenging problems: (1) they cannot well handle the open-vocabulary setting by arbitrary text inputs (i.e., unseen clothing appearances) and heavily rely on limited preset words (i.e., pattern styles of clothing appearances); (2) the generated human image is inaccuracy in open-vocabulary settings. To alleviate these drawbacks, we propose a flexible diffusion-based framework, namely HumanDiffusion, for open-vocabulary text-driven human image generation (HIG). The proposed framework mainly consists of two novel modules: the Stylized Memory Retrieval (SMR) module and the Multi-scale Feature Mapping (MFM) module. Encoded by the vision-language pretrained CLIP model, we obtain coarse features of the local human appearance. Then, the SMR module utilizes an external database that contains clothing texture details to refine the initial coarse features. Through SMR refreshing, we can achieve the HIG task with arbitrary text inputs, and the range of expression styles is greatly expanded. Later, the MFM module embedding in the diffusion backbone can learn fine-grained appearance features, which effectively achieves precise semantic-coherence alignment of different body parts with appearance features and realizes the accurate expression of desired human appearance. The seamless combination of the proposed novel modules in HumanDiffusion realizes the freestyle and high accuracy of text-guided HIG and editing tasks. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SOTA) performance, especially in the open-vocabulary setting.\n","date":1715731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715731200,"objectID":"b0c70e4e98d2bcd7643903a7e0677d15","permalink":"http://localhost:1313/post/recent-published-papers-2401/","publishdate":"2024-05-15T00:00:00Z","relpermalink":"/post/recent-published-papers-2401/","section":"post","summary":"Open-Vocabulary Text-Driven Human Image Generation","tags":[],"title":"One paper was accepted to International Journal of Computer Vision (IJCV 2024)","type":"post"},{"authors":["Zihui Yan","Lingxiao He","Yunlong Wang","Kunbo Zhang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"fc70d4362ace9b847998f1ccaae0fe8c","permalink":"http://localhost:1313/publication/yan-mir-2024-1/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/publication/yan-mir-2024-1/","section":"publication","summary":"In the daily application of an iris-recognition-at-a-distance (IAAD) system, many ocular images of low quality are acquired. As the iris part of these images is often not qualified for the recognition requirements, the more accessible periocular regions are a good complement for recognition. To further boost the performance of IAAD systems, a novel end-to-end framework for multi-modal ocular recognition is proposed. The proposed framework mainly consists of iris/periocular feature extraction and matching, unsupervised iris quality assessment, and a score-level adaptive weighted fusion strategy. First, ocular feature reconstruction (OFR) is proposed to sparsely reconstruct each probe image by high-quality gallery images based on proper feature maps. Next, a brand new unsupervised iris quality assessment method based on random multiscale embedding robustness is proposed. Different from the existing iris quality assessment methods, the quality of an iris image is measured by its robustness in the embedding space. At last, the fusion strategy exploits the iris quality score as the fusion weight to coalesce the complementary information from the iris and periocular regions. Extensive experimental results on ocular datasets prove that the proposed method is obviously better than unimodal biometrics, and the fusion strategy can significantly improve the recognition performance.","tags":[],"title":"Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation","type":"publication"},{"authors":["Jawad Muhammad","Yunlong Wang","Junxing Hu","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1705047125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705047125,"objectID":"542300fe5d9b1d19e8f0ca0bdc05a127","permalink":"http://localhost:1313/publication/jawad-mir-2024-2/","publishdate":"2024-01-12T08:58:18Z","relpermalink":"/publication/jawad-mir-2024-2/","section":"publication","summary":"Iris biometrics is a phenotypic biometric trait that has proven to be agnostic to human natural physiological changes. Research on iris biometrics has progressed tremendously, partly due to publicly available iris databases. Various databases have been available to researchers that address pressing iris biometric challenges such as constraint, mobile, multispectral, synthetics, long-distance, contact lenses, liveness detection, etc. However, these databases mostly contain subjects of Caucasian and Asian docents with very few Africans. Despite many investigative studies on racial bias in face biometrics, very few studies on iris biometrics have been published, mainly due to the lack of racially diverse large-scale databases containing sufficient iris samples of Africans in the public domain. Furthermore, most of these databases contain a relatively small number of subjects and labelled images. This paper proposes a large-scale African database named Chinese Academy of Sciences Institute of Automation (CASIA)-Iris-Africa that can be used as a complementary database for the iris recognition community to mediate the effect of racial biases on Africans. The database contains 28 717 images of 1 023 African subjects (2 046 iris classes) with age, gender, and ethnicity attributes that can be useful in demographically sensitive studies of Africans. Sets of specific application protocols are incorporated with the database to ensure the database’s variability and scalability. Performance results of some open-source state-of-the-art (SOTA) algorithms on the database are presented, which will serve as baseline performances. The relatively poor performances of the baseline algorithms on the proposed database despite better performance on other databases prove that racial biases exist in these iris recognition algorithms.","tags":[""],"title":"CASIA-Iris-Africa: A Large-scale African Iris Image Database","type":"publication"},{"authors":["Junxing Hu","Hongwen Zhang","Zerui Chen","Mengcheng Li","Yunlong Wang","Yebin Liu","Zhenan Sun"],"categories":[],"content":"","date":1702455125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702455125,"objectID":"90421256f69f7e5cae53317914b10b13","permalink":"http://localhost:1313/publication/hu-aaai-2024/","publishdate":"2023-12-13T08:58:18Z","relpermalink":"/publication/hu-aaai-2024/","section":"publication","summary":"Reconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. Our method consists of two components: explicit contact prediction and implicit shape reconstruction. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, our method can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin. Our code is publicly available at https://junxinghu.github.io/projects/hoi.html.","tags":null,"title":"Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images","type":"publication"},{"authors":null,"categories":null,"content":"1. Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images Junxing Hu,Hongwen Zhang,Zerui Chen,Mengcheng Li,Yunlong Wang,Yebin Liu,Zhenan Sun. Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images. The Association for the Advancement of Artificial Intelligence.\nhttps://arxiv.org/abs/2305.20089\nReconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. Our method consists of two components: explicit contact prediction and implicit shape reconstruction. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, our method can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin. Our code is publicly available at https://junxinghu.github.io/projects/hoi.html.\n","date":1702425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702425600,"objectID":"399add8feec726edc0f8d236cd883ac0","permalink":"http://localhost:1313/post/recent-published-papers-2402/","publishdate":"2023-12-13T00:00:00Z","relpermalink":"/post/recent-published-papers-2402/","section":"post","summary":"Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images","tags":[],"title":"One paper was accepted to The Association for the Advancement of Artificial Intelligence (AAAI 2024)","type":"post"},{"authors":["Ru Yiwei","Li Peipei","Sun Muyi","Wang Yunlong","Zhang Kunbo","Li Qi","He Zhaofeng","Sun Zhenan","et.al"],"categories":[],"content":"","date":1698394325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698394325,"objectID":"019eb083b5ed44cd4cd8965bd7169551","permalink":"http://localhost:1313/publication/ru-mm-2023/","publishdate":"2023-10-27T08:58:18Z","relpermalink":"/publication/ru-mm-2023/","section":"publication","summary":"Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a \"small yet powerful\" physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at https://remap-dataset.github.io/ReMAP.","tags":["iris segmentation","iris localization"],"title":"Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence","type":"publication"},{"authors":["Li Haiqing","Wang Caiyong","Zhao Guangzhe","He Zhaofeng","Wang Yunlong","Zhenan Sun"],"categories":[],"content":"","date":1695888725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695888725,"objectID":"43991d04165b4fe95686a8b69535e6b9","permalink":"http://localhost:1313/publication/haiqing-ijcb-2023/","publishdate":"2024-03-01T08:58:18Z","relpermalink":"/publication/haiqing-ijcb-2023/","section":"publication","summary":"Sclera segmentation is a crucial step in sclera recognition, which has been greatly advanced by Convolutional Neural Networks (CNNs). However, when dealing with non-ideal eye images, many existing CNN-based approaches are still prone to failure. One major reason is that due to the limited range of receptive fields, CNNs are difficult to effectively model global semantic relevance and thus robustly resist noise interference. To solve this problem, this paper proposes a novel two-stream hybrid model, named Sclera-TransFuse, to integrate classical ResNet-34 and recently emerging Swin Transformer encoders. Specially, the self-attentive Swin Transformer has shown a strong ability in capturing long-range spatial dependencies and has a hierarchical structure similar to CNNs. The dual encoders firstly extract coarse- and fine-grained feature representations at hierarchical stages, separately. Then a novel Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. Experimental results show that Sclera-TransFuse achieves state-of-the-art performance on various sclera segmentation benchmarks. Additionally, a UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through https://github.com/Ihqqq/Sclera-TransFuse.","tags":null,"title":"Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"1. Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation Li Haiqing, Wang Caiyong, Zhao Guangzhe, He Zhaofeng, Wang Yunlong,Zhenan Sun. Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation. seventh International Joint Conference on Biometrics (IJCB), 2023. https://ijcb2023.ieee-biometrics.org/\nSclera segmentation is a crucial step in sclera recognition, which has been greatly advanced by Convolutional Neural Networks (CNNs). However, when dealing with non-ideal eye images, many existing CNN-based approaches are still prone to failure. One major reason is that due to the limited range of receptive fields, CNNs are difficult to effectively model global semantic relevance and thus robustly resist noise interference. To solve this problem, this paper proposes a novel two-stream hybrid model, named Sclera-TransFuse, to integrate classical ResNet-34 and recently emerging Swin Transformer encoders. Specially, the self-attentive Swin Transformer has shown a strong ability in capturing long-range spatial dependencies and has a hierarchical structure similar to CNNs. The dual encoders firstly extract coarse- and fine-grained feature representations at hierarchical stages, separately. Then a novel Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. Experimental results show that Sclera-TransFuse achieves state-of-the-art performance on various sclera segmentation benchmarks. Additionally, a UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through https://github.com/Ihqqq/Sclera-TransFuse.\n","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"8bf7c72633b09a9848d2d2cc08aa21c4","permalink":"http://localhost:1313/post/recent-published-papers-2301/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/post/recent-published-papers-2301/","section":"post","summary":"Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation","tags":["smart iris recognition"],"title":"One paper was accepted to International Joint Conference on Biometrics (IJCB 2023)","type":"post"},{"authors":["Junxing Hu","Hongwen Zhang","Yunlong Wang","Min Ren","Zhenan Sun"],"categories":[],"content":"","date":1693469525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693469525,"objectID":"111dfb472cf18cbc5e00e9bdd9ef4592","permalink":"http://localhost:1313/publication/hu-tcsvt-2023-2/","publishdate":"2023-08-31T08:58:18Z","relpermalink":"/publication/hu-tcsvt-2023-2/","section":"publication","summary":"3D human pose and shape estimation from a single RGB image is an appealing yet challenging task. Due to the graph-like nature of human parametric models, a growing number of graph neural network-based approaches have been proposed and achieved promising results. However, existing methods build graphs for different instances based on the same template SMPL mesh, neglecting the geometric perception of individual properties. In this work, we propose an end-to-end method named Personalized Graph Generation (PGG) to construct the geometry-aware graph from an intermediate predicted human mesh. Specifically, a convolutional module initially regresses a coarse SMPL mesh tailored for each sample. Guided by the 3D structure of this personalized mesh, PGG extracts the local features from the 2D feature map. Then, these geometry-aware features are integrated with the specific coarse SMPL parameters as vertex features. Furthermore, a body-oriented adjacency matrix is adaptively generated according to the coarse mesh. It considers individual full-body relations between vertices, enhancing the perception of body geometry. Finally, a graph attentional module is utilized to predict the residuals to get the final results. Quantitative experiments across four benchmarks and qualitative comparisons on more datasets show that the proposed method outperforms state-of-the-art approaches for 3D human pose and shape estimation.","tags":[],"title":"Personalized Graph Generation for Monocular 3D Human Pose and Shape Estimation","type":"publication"},{"authors":["Min Ren","Yunlong Wang","Yuhao Zhu","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1689840725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689840725,"objectID":"964aacbc1eb0477cef76be2091014f50","permalink":"http://localhost:1313/publication/ren-tpami-2023/","publishdate":"2023-07-20T08:58:18Z","relpermalink":"/publication/ren-tpami-2023/","section":"publication","summary":"Occlusion is a common problem with biometric recognition in the wild. The generalization ability of CNNs greatly decreases due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrating the merits of both CNNs and graph models to overcome occlusion problems in biometric recognition, called multiscale dynamic graph representation (MS-DGR). More specifically, a group of deep features reflected on certain subregions is recrafted into a feature graph (FG). Each node inside the FG is deemed to characterize a specific local region of the input sample, and the edges imply the co-occurrence of non-occluded regions. By analyzing the similarities of the node representations and measuring the topological structures stored in the adjacent matrix, the proposed framework leverages dynamic graph matching to judiciously discard the nodes corresponding to the occluded parts. The multiscale strategy is further incorporated to attain more diverse nodes representing regions of various sizes. Furthermore, the proposed framework exhibits a more illustrative and reasonable inference by showing the paired nodes. Extensive experiments demonstrate the superiority of the proposed framework, which boosts the accuracy in both natural and occlusion-simulated cases by a large margin compared with that of baseline methods.","tags":["biometrics","deep learning","face recognition","graph neural networks","iris recognition"],"title":"Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions","type":"publication"},{"authors":null,"categories":null,"content":"1. Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions Min Ren, Yunlong Wang, Yuhao Zhu, Kunbo Zhang, Zhenan Sun. \u0026ldquo;Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions\u0026rdquo;. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) (Volume: 45)\nhttp://doi.org/10.1109/TPAMI.2023.3298836\nOcclusion is a common problem with biometric recognition in the wild. The generalization ability of CNNs greatly decreases due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrating the merits of both CNNs and graph models to overcome occlusion problems in biometric recognition, called multiscale dynamic graph representation (MS-DGR). More specifically, a group of deep features reflected on certain subregions is recrafted into a feature graph (FG). Each node inside the FG is deemed to characterize a specific local region of the input sample, and the edges imply the co-occurrence of non-occluded regions. By analyzing the similarities of the node representations and measuring the topological structures stored in the adjacent matrix, the proposed framework leverages dynamic graph matching to judiciously discard the nodes corresponding to the occluded parts. The multiscale strategy is further incorporated to attain more diverse nodes representing regions of various sizes. Furthermore, the proposed framework exhibits a more illustrative and reasonable inference by showing the paired nodes. Extensive experiments demonstrate the superiority of the proposed framework, which boosts the accuracy in both natural and occlusion-simulated cases by a large margin compared with that of baseline methods.\n2. Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics Jawad Muhammad, Caiyong Wang, Yunlong Wang, Kunbo Zhang, Zhenan Sun. \u0026ldquo;Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics\u0026rdquo;. IEEE Transactions on Information Forensics and Security(Volume:18).\nhttp://doi.org/10.1109/TIFS.2023.3268504\nIn recent years, unconstrained iris biometrics has become more prevalent due to its wide range of user applications. However, it also presents numerous challenges to the Iris pre-processing task of Localization and Segmentation (ILS). Many ILS techniques have been proposed to address these challenges, among which the most effective is the CNN-based methods. Training the CNN is data-intensive, and most of the existing CNN-based ILS approaches do not incorporate iris-specific features that can reduce their data dependence, despite the limited labelled iris data in the available databases. These trained CNN models built upon these databases can be sub-optimal. Hence, this paper proposes a guided CNN-based ILS approach IrisGuideNet. IrisGuideNet involves incorporating novel iris-specific heuristics named Iris Regularization Term (IRT), deep supervision technique, and hybrid loss functions in the training pipeline, which guides the network and reduces the model data dependence. A novel Iris Infusion Module (IIM) that utilizes the geometrical relationships between the ILS outputs to refine the predicted outputs is introduced at network inference. The proposed model is trained and evaluated with various datasets. Experimental results show that IrisGuideNet has outperformed most models across all the database categories. The codes implementation of the proposed IrisGuideNet will be available at: https://github.com/mohdjawadi/IrisGuidenet.\n","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"e2da5d3a3a8c1e7086cf574f086a2719","permalink":"http://localhost:1313/post/recent-published-papers-23/","publishdate":"2023-07-20T00:00:00Z","relpermalink":"/post/recent-published-papers-23/","section":"post","summary":"Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions, Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics","tags":["smart iris recognition"],"title":"One paper was accepted to T-PAMI 2023 and another was accpeted to TIFS 2023","type":"post"},{"authors":["Jawad Muhammad","Caiyong Wang","Yunlong Wang","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1681863125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681863125,"objectID":"a950bb07c9edc7aca305d82ecaaa30f9","permalink":"http://localhost:1313/publication/jawad-tifs-2023-1/","publishdate":"2023-04-19T08:58:18Z","relpermalink":"/publication/jawad-tifs-2023-1/","section":"publication","summary":"In recent years, unconstrained iris biometrics has become more prevalent due to its wide range of user applications. However, it also presents numerous challenges to the Iris pre-processing task of Localization and Segmentation (ILS). Many ILS techniques have been proposed to address these challenges, among which the most effective is the CNN-based methods. Training the CNN is data-intensive, and most of the existing CNN-based ILS approaches do not incorporate iris-specific features that can reduce their data dependence, despite the limited labelled iris data in the available databases. These trained CNN models built upon these databases can be sub-optimal. Hence, this paper proposes a guided CNN-based ILS approach IrisGuideNet. IrisGuideNet involves incorporating novel iris-specific heuristics named Iris Regularization Term (IRT), deep supervision technique, and hybrid loss functions in the training pipeline, which guides the network and reduces the model data dependence. A novel Iris Infusion Module (IIM) that utilizes the geometrical relationships between the ILS outputs to refine the predicted outputs is introduced at network inference. The proposed model is trained and evaluated with various datasets. Experimental results show that IrisGuideNet has outperformed most models across all the database categories. The codes implementation of the proposed IrisGuideNet will be available at: https://github.com/mohdjawadi/IrisGuidenet.","tags":[],"title":"Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics","type":"publication"},{"authors":null,"categories":null,"content":"Introduction Various forms of iris databases had been published that address some specific research problems and are made available to the biometrics research community such as: constraint databases; mobile iris; multispectral; synthetics; iris at a distance; contact lenses; liveness detection; Aging, etc. However, even though these databases have addressed many of the problems mentioned above, most of them contain subjects of primarily Caucasian and Asian docents with very few Africans and many times zero Africans. It is particularly challenging as this has created a research blind spot for African cohorts in these databases. Despite many of the reported investigative studies on racial bias in face biometrics, very few iris racial bias-related studies have been published, which can be due to the insufficient number of other races databases such as the Africans. Recently, face recognition algorithms have been reported to be biased toward specific demographics. Most prominently, many investigative studies have reported higher false-positive rates in African subject cohorts than in other cohorts. Due to the unavailability of the required quantity of African cohorts in the publicly available iris databases, similar studies would be difficult to replicate on iris biometrics. We presents a mainly African dataset that can be useful in addressing some of these challenges.\nDescription The iris images were captured in Nigeria, Africa. The capturing exercise was conducted across various locations in multiple sessions over three months. Due to logistical constraints, the capturing exercise was limited to the northern part of Nigeria. Each subject volunteer was asked to sign a consent form authorizing the data to be used for only research purposes. The iris sensor device for capturing the images is the IKUSBE30 iris sensor from IrisKing. The setup for the capturing exercise is shown in Fig. 1. Each volunteer was asked to hold the device, standing or sitting, based on their preference. As such, the iris images were captured at various degrees of head postures.\nFig. 1: Iris Capturing set-ups (indoor and outdoor)\nThe procedure for image capturing is in two steps:\nStep 1: Straight Gaze Capture: In this step, the volunteer was asked to hold the sensor and gaze into its lenses while moving the eyes in small ranges to the left, right, up, or down for approximately 3 minutes. During this time, images were periodically captured automatically at constant intervals. The eye movement ensures that the iris position and orientation are highly diversified across the captured images. Samples of the captured images in this step are shown in Fig. 2 (left). Step 2: Open-Close Capture: In this step, the volunteer was asked to open and close the eyes while gazing into the device for approximately 3 minutes. The iris images were automatically captured within this period. The essence of the opening and closing of the eyes is to ensure that the images contain irises of various sizes and degrees of occlusion, from fully closed eyes to half open and fully open eyes. This procedure will ultimately improve image diversity. Exemplary samples are shown in Fig. 2 (right). Fig. 2: Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)\nAfter the capturing sessions, the stored images were processed. The processing steps comprise automatic image selection to discard images with duplicate content, followed by manual image selection that guarantees the selected images\u0026rsquo; reliability. In the automatic selection, all the images from a single eye of one subject were organized into a matrix, with each column representing an image. The images of the subject\u0026rsquo;s eyes were then processed, and duplicates were removed using this procedure. The remaining images were then manually processed. The manual selection involves one-by-one human inspection of each image to identify damaged images and those images with no irises captured in them. Some sample images of one subject from the generated dataset are shown in Fig. 3, the labelled sampled images in Fig. 4, the summary of the database is presented in Table 1 and the database subjects\u0026rsquo; age distribution is shown in Fig. 5.\nFig. 3: Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)\nFig. 4: Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)\nTab. 1: Summary of the generated database Fig. 5: Age distribution of the generated database\nDatabase Organization The database package comprises of the following components organised in multiple directories:\nimage folder: This contain the iris images. Each image is named with SubjectID_Eye_ImageNumber.jpg. That represents: SubjectID: a unique number for each subject Eye is a letter that can be either “L” for left eye or “R” for right eye ImageNumber is a sequential number for images for the same subject iris_edge folder: This contain the corresponding 1-pixel iris edge binary images iris_edge_mask folder: This contain the corresponding iris binary masks of the images pupil_edge folder: This contain the corresponding 1-pixel pupil edge binary images pupil_edge_mask folder: This contain the corresponding pupil binary masks of the images params folder: This contain the corresponding ini files that describes radius and orientation both the iris and pupil used in the generation of iris edge, iris mask, pupil edge, pupil and mask images Protocols folder: This contain the files named with each of the defined proposed protocols. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper. Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications. Copyright and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-Iris-Africa collected by the Chinese Academy of Sciences’ Institute of Automation (CASIA)” and a reference to “CASIA-Iris-Africa Image Database, http://biometrics.idealtest.org/” should be included.\n","date":1676246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676246400,"objectID":"66d0d6f3bf8316080b45062306a26832","permalink":"http://localhost:1313/dataset/casia-iris-africa/","publishdate":"2023-02-13T00:00:00Z","relpermalink":"/dataset/casia-iris-africa/","section":"dataset","summary":"A mainly African dataset for iris recognition.","tags":["Dataset"],"title":"CASIA-Iris-Africa","type":"dataset"},{"authors":["Caiyong Wang","Tianhao Lu","Gaosheng Wu","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1673943125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673943125,"objectID":"68d744bcde3e0d7018e5dd16894f0cc6","permalink":"http://localhost:1313/publication/caiyong-ijcb-2022/","publishdate":"2023-01-17T08:58:18Z","relpermalink":"/publication/caiyong-ijcb-2022/","section":"publication","summary":"Iris images captured in less-constrained environments, especially at long distances often suffer from the interference of low resolution, resulting in the loss of much valid iris texture information for iris recognition. In this paper, we propose a dual-encoder super-resolution generative adversarial network (D-ESRGAN) for compensating texture lost of the raw image meanwhile maintaining the newly generated textures more natural. Specifically, the proposed D-ESRGAN not only integrates the residual CNN encoder to extract local features, but also employs an emerging vision transformer encoder to capture global associative information. The local and global features from two encoders are further fused for the subsequent reconstruction of high-resolution features. During the training, we develop a three-stage strategy to alleviate the problem that generative adversarial networks are prone to collapse. Moreover, to boost the iris recognition performance, we introduce a triplet loss to push away the distance of super-resolved iris images with different IDs, and pull the distance of super-resolved iris images with the same ID much closer. Experimental results on the public CASIA-Iris-distance and CASIA-Iris-M1 datasets show that D-ESRGAN archives better performance than state-of-the-art baselines in terms of both super-resolution image quality metrics and iris recognition metric.","tags":["iris super-resolution","smart iris recognition"],"title":"D-ESRGAN: A Dual-Encoder GAN with Residual CNN and Vision Transformer for Iris Image Super-Resolution","type":"publication"},{"authors":["Shubo Zhou","Liang Hu","Yunlong Wang","Zhenan Sun","Kunbo Zhang","Xue-qin Jiang"],"categories":[],"content":"\n","date":1673856725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673856725,"objectID":"6f4e3e793148952aaf18940abc8fdbd2","permalink":"http://localhost:1313/publication/zhow-tcsvt-2023/","publishdate":"2023-01-16T08:58:18Z","relpermalink":"/publication/zhow-tcsvt-2023/","section":"publication","summary":"As an aperture-divided computational imaging system, microlens array (MLA) -based light field (LF) imaging is playing an increasingly important role in computer vision. As the trade-off between the spatial and angular resolutions, deep learning (DL) -based image super-resolution (SR) methods have been applied to enhance the spatial resolution. However, in existing DL-based methods, the depth-varying defocus is not considered both in dataset development and algorithm design, which restricts many applications such as depth estimation and object recognition. To overcome this shortcoming, a super-resolution task that reconstructs all-in-focus high-resolution (HR) LF images from low-resolution (LR) LF images is proposed by designing a large dataset and proposing a convolutional neural network (CNN) -based SR method. The dataset is constructed by using Blender software, consisting of 150 light field images used as training data, and 15 light field images used as validation and testing data. The proposed network is designed by proposing the dilated deformable convolutional network (DCN) -based feature extraction block and the LF subaperture image (SAI) Deblur-SR block. The experimental results demonstrate that the proposed method achieves more appealing results both quantitatively and qualitatively.","tags":["Light Field Super-Resolution"],"title":"AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus","type":"publication"},{"authors":null,"categories":null,"content":"1. AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus Shubo Zhou, Liang Hu, Yunlong Wang, Zhenan Sun, Kunbo Zhang, Xue-qin Jiang. \u0026ldquo;AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus\u0026rdquo;. Transactions on Circuits and Systems for Video Technology\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026arnumber=10018388\nAs an aperture-divided computational imaging system, microlens array (MLA) -based light field (LF) imaging is playing an increasingly important role in computer vision. As the trade-off between the spatial and angular resolutions, deep learning (DL) -based image super-resolution (SR) methods have been applied to enhance the spatial resolution. However, in existing DL-based methods, the depth-varying defocus is not considered both in dataset development and algorithm design, which restricts many applications such as depth estimation and object recognition. To overcome this shortcoming, a super-resolution task that reconstructs all-in-focus high-resolution (HR) LF images from low-resolution (LR) LF images is proposed by designing a large dataset and proposing a convolutional neural network (CNN) -based SR method. The dataset is constructed by using Blender software, consisting of 150 light field images used as training data, and 15 light field images used as validation and testing data. The proposed network is designed by proposing the dilated deformable convolutional network (DCN) -based feature extraction block and the LF subaperture image (SAI) Deblur-SR block. The experimental results demonstrate that the proposed method achieves more appealing results both quantitatively and qualitatively.\n","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673827200,"objectID":"c075ee35f1ca88380ecf1fde4abdfea3","permalink":"http://localhost:1313/post/recent-published-papers-2302/","publishdate":"2023-01-16T00:00:00Z","relpermalink":"/post/recent-published-papers-2302/","section":"post","summary":"AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus","tags":[],"title":"One paper was accepted to The Association for Transactions on Circuits and Systems for Video Technology(TCSVT 2023)","type":"post"},{"authors":["Mengmeng Cui","Wei Wang","Kunbo Zhang","Zhenan Sun","Liang Wang"],"categories":[],"content":"","date":1671005525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671005525,"objectID":"582b3aba7dd16818f3ba060ad8dc2464","permalink":"http://localhost:1313/publication/kunbo-itip-2022/","publishdate":"2022-12-14T08:58:18Z","relpermalink":"/publication/kunbo-itip-2022/","section":"publication","summary":"Recent studies of video action recognition can be classified into two categories: the appearance-based methods and the pose-based methods. The appearance-based methods generally cannot model temporal dynamics of large motion well by virtue of optical flow estimation, while the pose-based methods ignore the visual context information such as typical scenes and objects, which are also important cues for action understanding. In this paper, we tackle these problems by proposing a Pose-Appearance Relational Network (PARNet), which models the correlation between human pose and image appearance, and combines the benefits of these two modalities to improve the robustness towards unconstrained real-world videos. There are three network streams in our model, namely pose stream, appearance stream and relation stream. For the pose stream, a Temporal Multi-Pose RNN module is constructed to obtain the dynamic representations through temporal modeling of 2D poses. For the appearance stream, a Spatial Appearance CNN module is employed to extract the global appearance representation of the video sequence. For the relation stream, a Pose-Aware RNN module is built to connect pose and appearance streams by modeling action-sensitive visual context information. Through jointly optimizing the three modules, PARNet achieves superior performances compared with the state-of-the-arts on both the pose-complete datasets (KTH, Penn-Action, UCF11) and the challenging pose-incomplete datasets (UCF101, HMDB51, JHMDB), demonstrating its robustness towards complex environments and noisy skeletons. Its effectiveness on NTU-RGBD dataset is also validated even compared with 3D skeleton-based methods. Furthermore, an appearance-enhanced PARNet equipped with a RGB-based I3D stream is proposed, which outperforms the Kinetics pre-trained competitors on UCF101 and HMDB51. The better experimental results verify the potentials of our framework by integrating various modules.","tags":null,"title":"Pose-Appearance Relational Modeling for Video Action Recognition","type":"publication"},{"authors":["Jianze Wei","Yunlong Wang","Huaibo Huang","Ran He","Zhenan Sun","Xingyu Gao"],"categories":[],"content":"","date":1668384725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668384725,"objectID":"6e009fb95341bb3cdc3e3e0a209786a3","permalink":"http://localhost:1313/publication/wei-tifs-2022/","publishdate":"2022-11-14T08:58:18Z","relpermalink":"/publication/wei-tifs-2022/","section":"publication","summary":"The iris patterns of the human contain a large amount of randomly distributed and irregularly shaped microstructures. These microstructures make the human iris informative biometric traits. To learn identity representation from them, this paper regards each iris region as a potential microstructure and proposes contextual measures (CM) to model the correlations between them. CM adopts two parallel branches to learn global and local contexts in iris image. The first one is the globally contextual measure branch. It measures the global context involving the relationships between all regions for feature aggregation and is robust to local occlusions. Besides, we improve its spatial perception considering the positional randomness of the microstructures. The other one is the locally contextual measure branch. This branch considers the role of local details in the phenotypic distinctiveness of iris patterns and learns a series of relationship atoms to capture contextual information from a local perspective. In addition, we develop the perturbation bottleneck to make sure that the two branches learn divergent contexts. It introduces perturbation to limit the information flow from input images to identity features, forcing CM to learn discriminative contextual information for iris recognition. Experimental results suggest that global and local contexts are two different clues critical for accurate iris recognition. The superior performance on four benchmark iris datasets demonstrates the effectiveness of the proposed approach in within-database and cross-database scenarios.","tags":["smart iris recognition"],"title":"Contextual Measures for Iris Recognition","type":"publication"},{"authors":["Jawad Muhammad","Yunlong Wang","Leyuan Wang","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1667463125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667463125,"objectID":"6faf6aa8349445f0c5928b03975a2e67","permalink":"http://localhost:1313/publication/jawad-ccbr-2022/","publishdate":"2022-11-03T08:58:18Z","relpermalink":"/publication/jawad-ccbr-2022/","section":"publication","summary":"Recently, the problem of racial bias in facial biometric systems has generated considerable attention from the media and biometric community. Many investigative studies have been published on estimating the bias between Caucasians and Asians, Caucasians and Africans, and other racial comparisons. These studies have reported inferior performances of both Asians and Africans when compared to other races. However, very few studies have highlighted the comparative differences in performance as a function of race between Africans and Asians. More so, those previous studies were mainly concentrated on a single aspect of facial biometrics and were usually conducted with images potentially captured with multiple camera sensors, thereby compounding their findings. This paper presents a comparative racial bias study of Asians with Africans on various facial biometric tasks. The images used were captured with the same camera sensor and under controlled conditions. We examine the performances of many DCNN-based models on face detection, facial landmark detection, quality assessment, verification, and identification. The results suggested higher performance on the Asians compared to the Africans by most algorithms under the same imaging and testing conditions.","tags":[""],"title":"An Empirical Comparative Analysis of Africans with Asians Using DCNN Facial Biometric Models","type":"publication"},{"authors":["Matej Vitek","Abhijit Das","Diego Rafael Lucio","et.al"],"categories":[],"content":"","date":1666311125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666311125,"objectID":"216c3a079aa534cb5721a7929a07de5f","permalink":"http://localhost:1313/publication/matej-tifs-2022/","publishdate":"2022-10-21T08:58:18Z","relpermalink":"/publication/matej-tifs-2022/","section":"publication","summary":"Bias and fairness of biometric algorithms have been key topics of research in recent years, mainly due to the societal, legal and ethical implications of potentially unfair decisions made by automated decision-making models. A considerable amount of work has been done on this topic across different biometric modalities, aiming at better understanding the main sources of algorithmic bias or devising mitigation measures. In this work, we contribute to these efforts and present the first study investigating bias and fairness of sclera segmentation models. Although sclera segmentation techniques represent a key component of sclera-based biometric systems with a considerable impact on the overall recognition performance, the presence of different types of biases in sclera segmentation methods is still underexplored. To address this limitation, we describe the results of a group evaluation effort (involving seven research groups), organized to explore the performance of recent sclera segmentation models within a common experimental framework and study performance differences (and bias), originating from various demographic as well as environmental factors. Using five diverse datasets, we analyze seven independently developed sclera segmentation models in different experimental configurations. The results of our experiments suggest that there are significant differences in the overall segmentation performance across the seven models and that among the considered factors, ethnicity appears to be the biggest cause of bias. Additionally, we observe that training with representative and balanced data does not necessarily lead to less biased results. Finally, we find that in general there appears to be a negative correlation between the amount of bias observed (due to eye color, ethnicity and acquisition device) and the overall segmentation performance, suggesting that advances in the field of semantic segmentation may also help with mitigating bias.","tags":["Sclera Segmentation"],"title":"Exploring Bias in Sclera Segmentation Models: A Group Evaluation Approach","type":"publication"},{"authors":["Yunlong Wang","Mupei Li","Zhengquan Luo","Zhenan Sun"],"categories":[],"content":"","date":1665648725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665648725,"objectID":"ce82012fe5faffbfe5a0730f1b3118fe","permalink":"http://localhost:1313/publication/wang-ijcb-2022/","publishdate":"2022-10-13T08:58:18Z","relpermalink":"/publication/wang-ijcb-2022/","section":"publication","summary":"Light Field Focal Stack (LFFS) can be efficiently rendered from a light field (LF) image captured by plenoptic cameras. Differences in the 3D surface and texture of biometric samples are internally reflected in the defocus blur and local patterns between the rendered slices of LFFS. This unique property makes LFFS quite appropriate to differentiate presentation attack instruments (PAIs) from bona fide samples. A patch-based dual-view network (PDVN) is proposed in this paper to leverage the merits of LFFS for face presentation attack detection (PAD). First, original LFFS data are divided into various local patches along spatial dimensions, which distracts the model from learning the useless facial semantics and greatly relieve the problem of insufficient samples. The strategy of dual-view branches is innovatively proposed, wherein the original view and microscopic view can simultaneously contribute to liveness detection. Separable 3D convolution on the focal dimension is verified to be more effective than vanilla 3D convolution for extracting discriminative features from LFFS data. The voting mechanism on predictions of patch LFFS samples further strengthens the robustness of the proposed framework. PDVN is compared with other face PAD methods on IST LLFFSD dataset and achieves perfect performance, i.e., ACER drops to 0.","tags":["Liveness Detection","Light Field Imaging"],"title":"PDVN: A Patch-based Dual-view Network for Face Liveness Detection using Light Field Focal Stack","type":"publication"},{"authors":null,"categories":null,"content":"Introduction CASIA-Polar is a face anti-spoofing dataset based on polarization imaging, which takes advantage of polarization information in material classification to enable robust face anti-spoofing research.\nSetup of dataset collection The hardware system used to collect this dataset is shown in Fig.1, consists of a Lucid Phoenix PHX050S-P polarized camera equipped with Sony’s polarization sensor and a Mindvision MVGE501GC-T RGB camera.\nTypes of spoofing attacks include printed paper, printed glossy photographs, electronic displays, silicone masks, rubber masks, and customized silicone prosthetic heads. High-quality face samples are first captured by the MVGE501GC-T RGB camera and these high-quality samples are then printed on paper and photographs or displayed on a computer screen to produce artifacts. At the same time, both genuine and spoofing attack face samples were captured when the subjects and presentation attack were standing at or be placed at six distances, i.e. 1m, 1.5m, 2m, 3m, 4m, and 5m.\nFig. 1: Setup of dataset collection\nThe types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.\nStatistics of the Dataset The dataset includes 22,174 samples from 121 subjects. All subjects had visible light and polarized images taken. The genuine face has 14,698 samples, while the spoofing attack has about 7,476 samples.\nFigure 2 shows an example of the face images collected in the dataset, with the first row being the visible image and the second row is the corresponding DoLP image.\nFig.2: The attacks present in CASIA-Polar\nCopyright and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved.\nE-mail: sir@cripac.ia.ac.cn\n","date":1663804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663804800,"objectID":"9951897a1a6032ab81981f0afe0d6610","permalink":"http://localhost:1313/dataset/casia-polar/","publishdate":"2022-09-22T00:00:00Z","relpermalink":"/dataset/casia-polar/","section":"dataset","summary":"Face Anti-spoofing Dataset Based on Polarization Imaging","tags":["Dataset"],"title":"CASIA-Polar","type":"dataset"},{"authors":["Zhengquan Luo","Yunlong Wang","Nianfeng Liu","Zilei Wang"],"categories":[],"content":"","date":1661587925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661587925,"objectID":"a6dc5645e479037828f6dd99729f527c","permalink":"http://localhost:1313/publication/luo-iet-2022/","publishdate":"2022-08-27T08:58:18Z","relpermalink":"/publication/luo-iet-2022/","section":"publication","summary":"Iris presentation attack detection (PAD) is still an unsolved problem mainly due to the various spoof attack strategies and poor generalisation on unseen attackers. In this paper, the merits of both light field (LF) imaging and deep learning (DL) are leveraged to combine 2D texture and 3D geometry features for iris liveness detection. By exploring off-the-shelf deep features of planar-oriented and sequence-oriented deep neural networks (DNNs) on the rendered focal stack, the proposed framework excavates the differences in 3D geometric structure and 2D spatial texture between bona fide and spoofing irises captured by LF cameras. A group of pre-trained DL models are adopted as feature extractor and the parameters of SVM classifiers are optimised on a limited number of samples. Moreover, two-branch feature fusion further strengthens the framework's robustness and reliability against severe motion blur, noise, and other degradation factors. The results of comparative experiments indicate that variants of the proposed framework significantly surpass the PAD methods that take 2D planar images or LF focal stack as input, even recent state-of-the-art (SOTA) methods fined-tuned on the adopted database. Presentation attacks, including printed papers, printed photos, and electronic displays, can be accurately detected without fine-tuning a bulky CNN. In addition, ablation studies validate the effectiveness of fusing geometric structure and spatial texture features. The results of multi-class attack detection experiments also verify the good generalisation ability of the proposed framework on unseen presentation attacks.","tags":["Light Field Photography","Iris-liveness-detection"],"title":"Combining 2D texture and 3D geometry features for Reliable iris presentation attack detection using light field focal stack","type":"publication"},{"authors":["Min Ren","Yuhao Zhu","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1659312725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312725,"objectID":"2767832f714b23c236154173b19e536e","permalink":"http://localhost:1313/publication/ren-tifs-2022/","publishdate":"2022-08-01T08:58:18Z","relpermalink":"/publication/ren-tifs-2022/","section":"publication","summary":"Deep learning-based face recognition models are vulnerable to adversarial attacks. To curb these attacks, most defense methods aim to improve the robustness of recognition models against adversarial perturbations. However, the generalization capacities of these methods are quite limited. In practice, they are still vulnerable to unseen adversarial attacks. Deep learning models are fairly robust to general perturbations, such as Gaussian noises. A straightforward approach is to inactivate the adversarial perturbations so that they can be easily handled as general perturbations. In this paper, a plug-and-play adversarial defense method, named perturbation inactivation (PIN), is proposed to inactivate adversarial perturbations for adversarial defense. We discover that the perturbations in different subspaces have different influences on the recognition model. There should be a subspace, called the immune space, in which the perturbations have fewer adverse impacts on the recognition model than in other subspaces. Hence, our method estimates the immune space and inactivates the adversarial perturbations by restricting them to this subspace. The proposed method can be generalized to unseen adversarial perturbations since it does not rely on a specific kind of adversarial attack method. This approach not only outperforms several state-of-the-art adversarial defense methods but also demonstrates a superior generalization capacity through exhaustive experiments. Moreover, the proposed method can be successfully applied to four commercial APIs without additional training, indicating that it can be easily generalized to existing face recognition systems.","tags":["Face Recognition","Adversarial Defense"],"title":"Perturbation Inactivation Based Adversarial Defense for Face Recognition","type":"publication"},{"authors":null,"categories":null,"content":" Zhengquan Luo, Yunlong Wang*, Zilei Wang, Zhenan Sun, Tieniu Tan. Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring. Thirty-ninth International Conference on Machine Learning (ICML), 2022. https://icml.cc/Conferences/2022\nAttributes skew hinders the current federated learning (FL) frameworks from consistent optimization directions among the clients, which inevitably leads to performance reduction and unstable convergence. The core problems lie in that: 1) Domain-specific attributes, which are non-causal and only locally valid, are indeliberately mixed into global aggregation. 2) The one-stage optimizations of entangled attributes cannot simultaneously satisfy two conflicting objectives, i.e., generalization and personalization. To cope with these, we proposed disentangled federated learning (DFL) to disentangle the domain-specific and cross-invariant attributes into two complementary branches, which are trained by the proposed alternating local-global optimization independently. Importantly, convergence analysis proves that the FL system can be stably converged even if incomplete client models participate in the global aggregation, which greatly expands the application scope of FL. Extensive experiments verify that DFL facilitates FL with higher performance, better interpretability, and faster convergence rate, compared with SOTA FL methods on both manually synthesized and realistic attributes skew datasets.\n人工智能前沿讲习公众号：【源头活水】ICML 2022 | 共识表征提取和多样性传播的解构联邦学习框架\n","date":1657497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657497600,"objectID":"274b901a32a7097a24795bdbdcc800ca","permalink":"http://localhost:1313/post/recent-published-papers-2207/","publishdate":"2022-07-11T00:00:00Z","relpermalink":"/post/recent-published-papers-2207/","section":"post","summary":"Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring","tags":["smart iris recognition"],"title":"One paper was accepted to International Conference on Machine Learning (ICML 2022)","type":"post"},{"authors":["Zhengquan Luo","Yunlong Wang","Zilei Wang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1655712725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655712725,"objectID":"ef8a8e47ff61a01fc99d160f4f4711eb","permalink":"http://localhost:1313/publication/luo-cvprw-2022/","publishdate":"2022-06-20T08:58:18Z","relpermalink":"/publication/luo-cvprw-2022/","section":"publication","summary":"As biometric data undergo rapidly growing privacy concerns, building large-scale datasets has become more difficult. Unfortunately, current iris databases are mostly in small scale, e.g., thousands of iris images from hundreds of identities. What's worse, the heterogeneity among decentralized iris datasets hinders the current deep learning (DL) frameworks from obtaining recognition performance with robust generalization. It motivates us to leverage the merits of federated learning (FL) to solve these problems. However, traditional FL algorithms often employ model sharing for knowledge transfer, wherein the simple averaging aggregation lacks interpretability, and divergent optimization directions of clients lead to performance degradation. To overcome this interference, we propose FedIris with solid theoretical foundations, which attempts to employ the iris template as the communication carrier and formulate federated triplet (Fed-Triplet) for knowledge transfer. Furthermore, the massive heterogeneity among iris datasets may induce negative transfer and unstable optimization. The modified Wasserstein distance is embedded into the FedTriplet loss to reweight global aggregation, which drives the clients with similar data distributions to contribute more mutually. Extensive experimental results demonstrate that the proposed FedIris outperforms SOLO training, model-sharing-based FL training, and even centralized training.","tags":["smart iris recognition","federated learning"],"title":"FedIris: Towards More Accurate and Privacy-Preserving Iris Recognition via Federated Template Communication","type":"publication"},{"authors":["Yong He","Zhenan Sun","Caiyong Wang","Yunlong Wang","Yuhao Zhu"],"categories":[],"content":"","date":1655453525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655453525,"objectID":"90166b7a5c426df346b506985aa2a53d","permalink":"http://localhost:1313/publication/he-cas-2022/","publishdate":"2022-06-17T08:58:18Z","relpermalink":"/publication/he-cas-2022/","section":"publication","summary":"Eye location and state estimation are key steps in the preprocessing of biometrics recognition such as iris, sclera and periocular. Eye images captured in the non-cooperative environments often suffer from serious occlusions and complex backgrounds. To solve this problem, this paper proposes a robust and accurate single-stage framework based on eye landmarks to detect eye key points and estimate the left, right and open and closed states of eyes. In order to train and evaluate the proposed model, a new OCE-1000 dataset was created and manually labeled with eight key points, open and close state for left and right eyes of each image. Experimental results show that the proposed model achieves 98% accuracy of landmark location and 97% accuracy of eye state estimation on OCE-1000 dataset.","tags":null,"title":"EYE LOCATION AND STATE ESTIMATION BASED ON LANDMARKS","type":"publication"},{"authors":["Zhengquan Luo","Yunlong Wang","Zilei Wang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1655194325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655194325,"objectID":"8337776aba4545592f6743e155f600b5","permalink":"http://localhost:1313/publication/luo-icml-2022/","publishdate":"2022-06-14T08:58:18Z","relpermalink":"/publication/luo-icml-2022/","section":"publication","summary":"Attributes skew hinders the current federated learning (FL) frameworks from consistent optimization directions among the clients, which inevitably leads to performance reduction and unstable convergence. The core problems lie in that: 1) Domain-specific attributes, which are non-causal and only locally valid, are indeliberately mixed into global aggregation. 2) The one-stage optimizations of entangled attributes cannot simultaneously satisfy two conflicting objectives, i.e., generalization and personalization. To cope with these, we proposed disentangled federated learning (DFL) to disentangle the domain-specific and cross-invariant attributes into two complementary branches, which are trained by the proposed alternating local-global optimization independently. Importantly, convergence analysis proves that the FL system can be stably converged even if incomplete client models participate in the global aggregation, which greatly expands the application scope of FL. Extensive experiments verify that DFL facilitates FL with higher performance, better interpretability, and faster convergence rate, compared with SOTA FL methods on both manually synthesized and realistic attributes skew datasets.","tags":["federated learning"],"title":"Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring","type":"publication"},{"authors":null,"categories":null,"content":"Introduction The dataset for iris liveness detection based on light field (LF) imaging was collected by [1], wherein the first author is one of our collaborators. We have got the authority from the authors of [1] and released the LF focal stack data.\nSetup of dataset collection The dataset was captured using a lab-produced microlens based LF camera and a commercial device IKUSB-E30 under near-infrared (NIR) illumination. The setup of dataset collection was shown in Fig.1.\nFig. 1: Setup of dataset collection\nThe types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.\nStatistics of the Dataset The dataset contains 504 samples from 14 subjects, consisting of 230 LF images of bona fide iris and 274 LF images of spoofing iris. The respective sample number of the PAIs, i.e. printed papers, printed photos, and electronic display are 18, 122, 134.\nAn example of raw LF image containing both eyes printed on photos is shown in Fig.2. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.\nFig.2: An example of raw LF image containing both eyes printed on photos. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.\nThe LF toolbox released by [2] was utilized to decode raw LF images into 4D LF data. The eye regions were cropped from the same location of each sub-aperture image (SAI). The spatial resolution of each SAI after cropping is $128 \\times 96$, and the angular resolution is $7 \\times 7$. Examples from the same subject\u0026rsquo;s right eye in the dataset are shown in Fig.3.\nFig. 3: Examples from the same subject\u0026rsquo;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.\nThe rendered focal stack via digital refocusing has 145 slices around the best focus plane.\nThe details of the adopted database is listed in Table 1.\nCopyright and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved.\nTo receive a copy of the database, you can apply for it on our BIT website\nReference [1] 宋平, 黄玲, 王云龙, 刘菲, 孙哲南. 基于计算光场成像的虹膜活体检测方法. 自动化学报, 2019, 45(9): 1701-1712. (Ping Song, Huang Ling, Wang Yunlong, Liu Fei, and Sun Zhenan. Iris liveness detection based on light field imaging. ACTA AUTOMATICA SINICA, 45(9):1701–1712, 2019.)\n[2] Donald G Dansereau, Oscar Pizarro, and Stefan B Williams, “Decoding, calibration and rectification for lenselet-based plenoptic cameras,” in Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1027–1034.\n","date":1654387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654387200,"objectID":"fc666c3a8ea0419f47af77d8dfb7766a","permalink":"http://localhost:1313/dataset/casia-iris-lfld/","publishdate":"2022-06-05T00:00:00Z","relpermalink":"/dataset/casia-iris-lfld/","section":"dataset","summary":"Iris Liveness Detection Based on Light Field Imaging","tags":["Dataset"],"title":"CASIA-Iris-LFLD","type":"dataset"},{"authors":["Min Ren","Yunlong Wang","Zhaofeng He"],"categories":[],"content":"","date":1653725525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653725525,"objectID":"289ec7277266018f73c6161ba6e04a68","permalink":"http://localhost:1313/publication/ren-mir-2022/","publishdate":"2022-05-28T08:58:18Z","relpermalink":"/publication/ren-mir-2022/","section":"publication","summary":"Deep learning-based models are vulnerable to adversarial attacks. Defense against adversarial attacks is essential for sensitive and safety-critical scenarios. However, deep learning methods still lack effective and efficient defense mechanisms against adversarial attacks. Most of the existing methods are just stopgaps for specific adversarial samples. The main obstacle is that how adversarial samples fool the deep learning models is still unclear. The underlying working mechanism of adversarial samples has not been well explored, and it is the bottleneck of adversarial attack defense. In this paper, we build a causal model to interpret the generation and performance of adversarial samples. The self-attention/transformer is adopted as a powerful tool in this causal model. Compared to existing methods, causality enables us to analyze adversarial samples more naturally and intrinsically. Based on this causal model, the working mechanism of adversarial samples is revealed, and instructive analysis is provided. Then, we propose simple and effective adversarial sample detection and recognition methods according to the revealed working mechanism. The causal insights enable us to detect and recognize adversarial samples without any extra model or training. Extensive experiments are conducted to demonstrate the effectiveness of the proposed methods. Our methods outperform the state-of-the-art defense methods under various adversarial attacks.","tags":["Adversarial Defense"],"title":"Towards Interpretable Defense Against Adversarial Attacks via Causal Inference","type":"publication"},{"authors":null,"categories":null,"content":"1. Towards Interpretable Defense Against Adversarial Attacks via Causal Inference Min Ren, Yunlong Wang*, Zhaofeng He. Towards interpretable defense against adversarial attacks via causal inference. Machine Intelligence Research.\nhttp://doi.org/10.1007/s11633-022-1330-7\nDeep learning-based models are vulnerable to adversarial attacks. Defense against adversarial attacks is essential for sensitive and safety-critical scenarios. However, deep learning methods still lack effective and efficient defense mechanisms against adversari-al attacks. Most of the existing methods are just stopgaps for specific adversarial samples. The main obstacle is that how adversarial samples fool the deep learning models is still unclear. The underlying working mechanism of adversarial samples has not been well explored, and it is the bottleneck of adversarial attack defense. In this paper, we build a causal model to interpret the generation and performance of adversarial samples. The self-attention/transformer is adopted as a powerful tool in this causal model. Compared to existing methods, causality enables us to analyze adversarial samples more naturally and intrinsically. Based on this causal model, the working mechanism of adversarial samples is revealed, and instructive analysis is provided. Then, we propose simple and effective adversarial sample detection and recognition methods according to the revealed working mechanism. The causal insights enable us to detect and recognize adversarial samples without any extra model or training. Extensive experiments are conducted to demonstrate the effectiveness of the proposed methods. Our methods outperform the state-of-the-art defense methods under various adversarial attacks.\n2. FedIris: Towards More Accurate and Privacy-preserving Iris Recognition via Federated Template Communication Zhengquan Luo, Yunlong Wang*, Zilei Wang, Zhenan Sun,Tieniu Tan. IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022, International Workshop on Federated Learning for Computer Vision.\nhttps://sites.google.com/view/fedvision/home?authuser=0\nAs biometric data undergo rapidly growing privacy concerns, building large-scale datasets has become more difficult. Unfortunately, current iris databases are mostly in small scale, e.g., thousands of iris images from hundreds of identities. What\u0026rsquo;s worse, the heterogeneity among decentralized iris datasets hinders the current deep learning (DL) frameworks from obtaining recognition performance with robust generalization. It motivates us to leverage the merits of federated learning (FL) to solve these problems. However, traditional FL algorithms often employ model sharing for knowledge transfer, wherein the simple averaging aggregation lacks interpretability, and divergent optimization directions of clients lead to performance degradation. To overcome this interference, we propose FedIris with solid theoretical foundations, which attempts to employ the iris template as the communication carrier and formulate federated triplet (Fed-Triplet) for knowledge transfer. Furthermore, the massive heterogeneity among iris datasets may induce negative transfer and unstable optimization. The modified Wasserstein distance is embedded into the FedTriplet loss to reweight global aggregation, which drives the clients with similar data distributions to contribute more mutually. Extensive experimental results demonstrate that the proposed FedIris outperforms SOLO training, model-sharing-based FL training, and even centralized training.\n","date":1651881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651881600,"objectID":"a154e63db6c1d8e36925bf3d902bff69","permalink":"http://localhost:1313/post/recent-published-papers-2204/","publishdate":"2022-05-07T00:00:00Z","relpermalink":"/post/recent-published-papers-2204/","section":"post","summary":"Towards Interpretable Defense Against Adversarial Attacks via Causal Inference, FedIris: Towards More Accurate and Privacy-preserving Iris Recognition via Federated Template Communication","tags":["smart iris recognition"],"title":"One paper was accepted to Mahcine Intelligence Research (MIR) and another one was accpeted to CVPRW 2022","type":"post"},{"authors":["何勇","孙哲南","王财勇","王云龙","朱宇豪"],"categories":[],"content":"","date":1648800725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648800725,"objectID":"62d77d02b28f46c47c5073f537c6d48b","permalink":"http://localhost:1313/publication/heyong-caas-2022-new/","publishdate":"2023-01-17T08:58:18Z","relpermalink":"/publication/heyong-caas-2022-new/","section":"publication","summary":"眼睛定位和状态估计是虹膜、巩膜、眼周等生物特征识别中重要的预处理过程。非合作环境下捕获的 眼睛图像经常面临严重的遮挡和复杂的背景。为此提出一种鲁棒而准确的基于眼睛关键点的单阶段方法去定位眼睛的位置并估计眼睛的左右和开闭状态。为了训练和评估提出的模型，手工标注一个新的数据集OCE-1000，每幅图像标注左右两只眼睛共 ８个关键点，以及左右眼的开闭状态。实验结果表明，提出的模型在OCE-1000数据集上达到了98％的关键点定位准确率，眼睛状态估计的准确率为97％。","tags":null,"title":"基于关键点的眼睛定位和状态估计","type":"publication"},{"authors":null,"categories":null,"content":"Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors Jianze Wei, Huaibo Huang, Yunlong Wang, Ran He and Zhenan Sun, \u0026ldquo;Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors,\u0026rdquo; in IEEE Transactions on Information Forensics and Security, doi: 10.1109/TIFS.2022.3154240.\nThe uncontrollable acquisition process limits the performance of iris recognition. In the acquisition process, various inevitable factors, including eyes, devices, and environment, hinder the iris recognition system from learning a discriminative identity representation. This leads to severe performance degradation. In this paper, we explore uncertain acquisition factors and propose uncertainty embedding (UE) and uncertainty-guided curriculum learning (UGCL) to mitigate the influence of acquisition factors. UE represents an iris image using a probabilistic distribution rather than a deterministic point (binary template or feature vector) that is widely adopted in iris recognition methods. Specifically, UE learns identity and uncertainty features from the input image, and encodes them as two independent components of the distribution, mean and variance. Based on this representation, an input image can be regarded as an instantiated feature sampled from the UE, and we can also generate various virtual features through sampling. UGCL is constructed by imitating the progressive learning process of newborns. Particularly, it selects virtual features to train the model in an easy-to-hard order at different training stages according to their uncertainty. In addition, an instance-level enhancement method is developed by utilizing local and global statistics to mitigate the data uncertainty from image noise and acquisition conditions in the pixel-level space. The experimental results on six benchmark iris datasets verify the effectiveness and generalization ability of the proposed method on same-sensor and cross-sensor recognition.\nGithub repository：\rhttps://github.com/reborn20200813/uncertainty\nMultitask deep active contour-based iris segmentation for off-angle iris images Tianhao Lu, Caiyong Wang, Yunlong Wang, and Zhenan Sun \u0026ldquo;Multitask deep active contour-based iris segmentation for off-angle iris images,\u0026rdquo; Journal of Electronic Imaging 31(4), 041211 (26 February 2022).\nIris recognition has been considered as a secure and reliable biometric technology. However, iris images are prone to off-angle or are partially occluded when captured with fewer user cooperations. As a consequence, iris recognition especially iris segmentation suffers a serious performance drop. To solve this problem, we propose a multitask deep active contour model for off-angle iris image segmentation. Specifically, the proposed approach combines the coarse and fine localization results. The coarse localization detects the approximate position of the iris area and further initializes the iris contours through a series of robust preprocessing operations. Then, iris contours are represented by 40 ordered isometric sampling polar points and thus their corresponding offset vectors are regressed via a convolutional neural network for multiple times to obtain the precise inner and outer boundaries of the iris. Next, the predicted iris boundary results are regarded as a constraint to limit the segmentation range of noise-free iris mask. Besides, an efficient channel attention module is introduced in the mask prediction to make the network focus on the valid iris region. A differentiable, fast, and efficient SoftPool operation is also used in place of traditional pooling to keep more details for more accurate pixel classification. Finally, the proposed iris segmentation approach is combined with off-the-shelf iris feature extraction models including traditional OM and deep learning-based FeatNet for iris recognition. The experimental results on two NIR datasets CASIA-Iris-off-angle, CASIA-Iris-Africa, and a VIS dataset SBVPI show that the proposed approach achieves a significant performance improvement in the segmentation and recognition for both regular and off-angle iris images.\n","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"9c0daf2e2d284f56fb11448a778a3017","permalink":"http://localhost:1313/post/recent-published-papers-2203/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/post/recent-published-papers-2203/","section":"post","summary":"Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors, Multitask deep active contour-based iris segmentation for off-angle iris images","tags":["smart iris recognition"],"title":"One paper was accepted to IEEE TIFS and another one was accpeted to Journal of Electronic Imaging","type":"post"},{"authors":["Jianze Wei","Huaibo Huang","Yunlong Wang","Ran He","Zhenan Sun"],"categories":[],"content":"","date":1646007125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646007125,"objectID":"51e60ad9f2cdcae311c8e15c3e89ae1e","permalink":"http://localhost:1313/publication/wei-tifs-2022_2/","publishdate":"2022-02-28T08:58:18Z","relpermalink":"/publication/wei-tifs-2022_2/","section":"publication","summary":"The uncontrollable acquisition process limits the performance of iris recognition. In the acquisition process, various inevitable factors, including eyes, devices, and environment, hinder the iris recognition system from learning a discriminative identity representation. This leads to severe performance degradation. In this paper, we explore uncertain acquisition factors and propose uncertainty embedding (UE) and uncertainty-guided curriculum learning (UGCL) to mitigate the influence of acquisition factors. UE represents an iris image using a probabilistic distribution rather than a deterministic point (binary template or feature vector) that is widely adopted in iris recognition methods. Specifically, UE learns identity and uncertainty features from the input image, and encodes them as two independent components of the distribution, mean and variance. Based on this representation, an input image can be regarded as an instantiated feature sampled from the UE, and we can also generate various virtual features through sampling. UGCL is constructed by imitating the progressive learning process of newborns. Particularly, it selects virtual features to train the model in an easy-to-hard order at different training stages according to their uncertainty. In addition, an instance-level enhancement method is developed by utilizing local and global statistics to mitigate the data uncertainty from image noise and acquisition conditions in the pixel-level space. The experimental results on six benchmark iris datasets verify the effectiveness and generalization ability of the proposed method on same-sensor and cross-sensor recognition.","tags":["smart iris recognition"],"title":"Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors","type":"publication"},{"authors":["Tianhao Lu","Caiyong Wang","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1645085525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645085525,"objectID":"60c7238771ee6a050174eaf1179b49a2","permalink":"http://localhost:1313/publication/lu-jei-2022/","publishdate":"2022-02-17T08:58:18Z","relpermalink":"/publication/lu-jei-2022/","section":"publication","summary":" Iris recognition has been considered as a secure and reliable biometric technology. However, iris images are prone to off-angle or are partially occluded when captured with fewer user cooperations. As a consequence, iris recognition especially iris segmentation suffers a serious performance drop. To solve this problem, we propose a multitask deep active contour model for off-angle iris image segmentation. Specifically, the proposed approach combines the coarse and fine localization results. The coarse localization detects the approximate position of the iris area and further initializes the iris contours through a series of robust preprocessing operations. Then, iris contours are represented by 40 ordered isometric sampling polar points and thus their corresponding offset vectors are regressed via a convolutional neural network for multiple times to obtain the precise inner and outer boundaries of the iris. Next, the predicted iris boundary results are regarded as a constraint to limit the segmentation range of noise-free iris mask. Besides, an efficient channel attention module is introduced in the mask prediction to make the network focus on the valid iris region. A differentiable, fast, and efficient SoftPool operation is also used in place of traditional pooling to keep more details for more accurate pixel classification. Finally, the proposed iris segmentation approach is combined with off-the-shelf iris feature extraction models including traditional OM and deep learning-based FeatNet for iris recognition. The experimental results on two NIR datasets CASIA-Iris-off-angle, CASIA-Iris-Africa, and a VIS dataset SBVPI show that the proposed approach achieves a significant performance improvement in the segmentation and recognition for both regular and off-angle iris images.","tags":["iris segmentation","smart iris recognition"],"title":"Multitask deep active contour-based iris segmentation for off-angle iris images","type":"publication"},{"authors":null,"categories":null,"content":" Min Ren, Lingxiao He, Xingyu Liao, Wu Liu, Yunlong Wang, Tieniu Tan; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 14930-14939\nJ. Muhammad, Y. Wang, C. Wang, K. Zhang and Z. Sun, \u0026ldquo;CASIA-Face-Africa: A Large-Scale African Face Image Database,\u0026rdquo; in IEEE Transactions on Information Forensics and Security, vol. 16, pp. 3634-3646, 2021, doi: 10.1109/TIFS.2021.3080496.\nZ. Yan, L. He, Y. Wang, Z. Sun and T. Tan, \u0026ldquo;Flexible Iris Matching Based on Spatial Feature Reconstruction,\u0026rdquo; in IEEE Transactions on Biometrics, Behavior, and Identity Science, doi: 10.1109/TBIOM.2021.3108559.\nJ. Wei, Y. Wang, Y. Li, R. He and Z. Sun, \u0026ldquo;Cross-spectral Iris Recognition by Learning Device-specific Band,\u0026rdquo; in IEEE Transactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2021.3117291.\nJ. Hu, L. Wang, Z. Luo, Y. Wang and Z. Sun, \u0026ldquo;A Large-scale Database for Less Cooperative Iris Recognition,\u0026rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-6, doi: 10.1109/IJCB52358.2021.9484357.\nY. Ru, W. Zhou, Y. Liu, J. Sun and Q. Li, \u0026ldquo;Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection,\u0026rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484408.\nJ. Wei, R. He and Z. Sun, \u0026ldquo;Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples,\u0026rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484388.\nL. Wang, K. Zhang, Y. Wang and Z. Sun, \u0026ldquo;An End-to-End Autofocus Camera for Iris on the Move,\u0026rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484340.\nY. Tian, K. Zhang, L. Wang and C. Zhang, \u0026ldquo;Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method,\u0026rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484402.\nSun Z N, He R, Wang L, Kan M N, Feng J J, Zheng F, Zheng W S, Zuo W M, Kang W X, Deng W H, Zhang J, Han H, Shan SG, Wang Y L, Ru Y W, Zhu Y H, Liu Y F and He Y. 2021. Overview of biometrics research. Journal of Image and Graphics,26(06):1254-1329.\nLuo Z., Li H., Wang Y., Wang Z., Sun Z. (2021) Iris Normalization Beyond Appr-Circular Parameter Estimation. In: Feng J., Zhang J., Liu M., Fang Y. (eds) Biometric Recognition. CCBR 2021. Lecture Notes in Computer Science, vol 12878. Springer, Cham. https://doi.org/10.1007/978-3-030-86608-2_35\n","date":1636934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636934400,"objectID":"c6c5569eb03e9bec1da4b8a4037c2319","permalink":"http://localhost:1313/post/recent-published-papers-2111/","publishdate":"2021-11-15T00:00:00Z","relpermalink":"/post/recent-published-papers-2111/","section":"post","summary":"Congratulations to them!","tags":["smart iris recognition"],"title":"Overview of Recent Published Papers","type":"post"},{"authors":["Min Ren","Lingxiao He","Xingyu Liao","Wu Liu","Yunlong Wang","Tieniu Tan"],"categories":[],"content":"","date":1634256725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256725,"objectID":"a99f39dc6199feac3217217b2c1fa6a3","permalink":"http://localhost:1313/publication/ren-iccv-2021/","publishdate":"2021-10-15T08:58:18Z","relpermalink":"/publication/ren-iccv-2021/","section":"publication","summary":"Person re-identification (Re-ID) aims to match pedestrians under dis-joint cameras. Most Re-ID methods formulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space. Spatial-temporal information has been proven to be efficient to filter irrelevant negative samples and significantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufficiently. In this paper, we propose a novel instance-level and spatial-temporal disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, personalized information such as moving direction is explicitly considered to further narrow down the search space. Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribution, so that outliers can also be well modeled. Abundant experimental analyses on two datasets are presented, which demonstrates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively. Besides, in order to provide a better benchmark for person re-identification, we release a cleaned data list of DukeMTMC-reID with this paper: https://github.com/RenMin1991/cleaned-DukeMTMC-reID.","tags":["Person Re-identification"],"title":"Learning Instance-level Spatial-Temporal Patterns for Person Re-identification","type":"publication"},{"authors":["Jianze Wei","Yunlong Wang","Yi Li","Ran He","Zhenan Sun"],"categories":[],"content":"","date":1633335125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633335125,"objectID":"874ea27339cdb47c908c65e33d7233b4","permalink":"http://localhost:1313/publication/wei-tcsvt-2021/","publishdate":"2021-10-04T08:58:18Z","relpermalink":"/publication/wei-tcsvt-2021/","section":"publication","summary":"Cross-spectral recognition is still an open challenge in iris recognition. In cross-spectral iris recognition, there exist distinct device-specific bands between near-infrared (NIR) and visible (VIS) images, resulting in the distribution gap between samples from different spectra and thus severe degradation in recognition performance. To tackle this problem, we propose a new cross-spectral iris recognition method to learn spectral-invariant features by estimating device-specific bands. In the proposed method, Gabor Trident Network (GTN) first utilizes the Gabor function’s priors to perceive iris textures under different spectra, and then codes the device-specific band as the residual component to assist the generation of spectral-invariant features. By investigating the device-specific band, GTN effectively reduces the impact of device-specific bands on identity features. Besides, we make three efforts to further reduce the distribution gap. First, Spectral Adversarial Network (SAN) adopts a class-level adversarial strategy to align feature distributions. Second, Sample-Anchor (SA) loss upgrades triplet loss by pulling samples to their class center and pushing away from other class centers. Third, we develop a higher-order alignment loss to measures the distribution gap according to space bases and distribution shapes. Extensive experiments on five iris datasets demonstrate the efficacy of our proposed method for cross-spectral iris recognition.","tags":["smart iris recognition"],"title":"Cross-spectral Iris Recognition by Learning Device-specific Band","type":"publication"},{"authors":["Zhengquan Luo","Haiqing Li","Yunlong Wang","Zilei Wang","Zhenan Sun"],"categories":[],"content":"","date":1631059925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631059925,"objectID":"e532033672aba6d8246deba9a72319dd","permalink":"http://localhost:1313/publication/luo-ccbr-2021/","publishdate":"2021-09-08T08:58:18Z","relpermalink":"/publication/luo-ccbr-2021/","section":"publication","summary":"The requirement to recognize the iris image of low-quality is rapidly increasing with the practical application of iris recognition, especially the urgent need for high-throughput or applications in covert situations. The appr-circle fitting can not meet the needs due to the high time cost and non-accurate boundary estimation during the normalization process. Furthermore, the appr-circular hypothesis of iris and pupil is not entirely established due to the squint and occlusion in non-cooperative environments. To mitigate this problem, a multi-mask normalization without appr-circular parameter estimation is proposed to make full use of the segmented masks, which provide robust pixel-level iris boundaries. It bridges the segmentation and feature extraction to recognize the low-quality iris, which is thrown directly by the traditional methods. Thus, the complex samples with no appr-circular iris or massive occlusions can be recognized correctly. The extensive experiments are conducted on the representative and challenging databases to verify the generalization and the accuracy of the proposed iris normalization method. Besides, the throughput rate is significantly improved.","tags":["smart iris recognition"],"title":"Iris Normalization Beyond Appr-Circular Parameter Estimation","type":"publication"},{"authors":["Zihui Yan","Lingxiao He","Yunlong Wang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1630311125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630311125,"objectID":"da0b81a6674db28f33856601bd4bc7d9","permalink":"http://localhost:1313/publication/zihui-yan-tbiom-2021/","publishdate":"2021-08-30T08:58:18Z","relpermalink":"/publication/zihui-yan-tbiom-2021/","section":"publication","summary":"In an iris recognition-at-a-distance (IAAD) system used in surveillance scenarios, the camera usually captures a large number of low-quality images. These images exhibit partial occlusions due to eyelids and eyelashes, specular reflections, and severe deformations caused by pupil dilations and contractions. Recognizing these low-quality images is a challenging yet dominant problem in IAAD. To mitigate this issue, current iris recognition systems mostly filter out low-quality images by using strict criteria based on image quality evaluation. This strategy, however, wastes device capabilities and produces low throughput of subjects. Other systems require highly cooperative users. In this work, we propose a novel occlusion-robust, deformation-robust, and alignment-free framework for low-quality iris matching, which integrates the merits of deep features and sparse representation in an end-to-end learning process known as iris spatial feature reconstruction (ISFR). Here each probe image can be sparsely reconstructed on the basis of appropriate feature maps from gallery high-quality images. ISFR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two iris images, which naturally avoids the time-consuming alignment step. In summary, the distinctiveness of deep features, the robustness of sparse reconstruction, and the flexibility of multiscale matching strategy are unified in a general framework to attain more accurate and reasonable iris matching. Extensive experimental results on four public iris image databases demonstrate that the proposed method significantly outperforms both traditional and deep learning-based iris recognition methods.","tags":["smart iris recognition"],"title":"Flexible Iris Matching Based on Spatial Feature Reconstruction","type":"publication"},{"authors":["Junxing Hu","Leyuan Wang","Zhengquan Luo","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1628323925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628323925,"objectID":"6e7419464e96c98403a8272e692fdf9c","permalink":"http://localhost:1313/publication/hu-ijcb-2021/","publishdate":"2021-08-07T08:58:18Z","relpermalink":"/publication/hu-ijcb-2021/","section":"publication","summary":"Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This paper presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.","tags":["smart iris recognition"],"title":"A Large-scale Database for Less Cooperative Iris Recognition","type":"publication"},{"authors":["Caiyong Wang","Yunlong Wang","Kunbo Zhang","Jawad Muhammad","Tianhao Lu","Qi Zhang","Qichuan Tian","Zhaofeng He","Zhenan Sun","et.al"],"categories":[],"content":"","date":1626768725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626768725,"objectID":"fdea8c2c05fe0bc75fd57b5b8013cf3d","permalink":"http://localhost:1313/publication/caiyong-ijcb-2021/","publishdate":"2021-07-20T08:58:18Z","relpermalink":"/publication/caiyong-ijcb-2021/","section":"publication","summary":"For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and also been introduced in iris biometrics, especially iris segmentation. To investigate recent developments and attract more interest of researchers in the iris segmentation method, we organized the 2021 NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021) at the 2021 International Joint Conference on Biometrics (IJCB 2021). The challenge was used as a public platform to assess the performance of iris segmentation and localization methods on Asian and African NIR iris images captured in non-cooperative environments. The three best-performing entries achieved solid and satisfactory iris segmentation and localization results in most cases, and their code and models have been made publicly available for reproducibility research.","tags":["iris segmentation","iris localization"],"title":"NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization","type":"publication"},{"authors":["Caiyong Wang","Yunlong Wang","Kunbo Zhang","Jawad Muhammad","Tianhao Lu","Qi Zhang","Qichuan Tian","Zhaofeng He","Zhenan Sun"],"categories":[],"content":"","date":1626768725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626768725,"objectID":"256e18201224aff5b88662e3f447e365","permalink":"http://localhost:1313/publication/kunbo-itifs-2023-ne/","publishdate":"2021-07-20T08:58:18Z","relpermalink":"/publication/kunbo-itifs-2023-ne/","section":"publication","summary":"For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and also been introduced in iris biometrics, especially iris segmentation. To investigate recent developments and attract more interest of researchers in the iris segmentation method, we organized the 2021 NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021) at the 2021 International Joint Conference on Biometrics (IJCB 2021). The challenge was used as a public platform to assess the performance of iris segmentation and localization methods on Asian and African NIR iris images captured in non-cooperative environments. The three best-performing entries achieved solid and satisfactory iris segmentation and localization results in most cases, and their code and models have been made publicly available for reproducibility research.","tags":["iris segmentation","iris localization"],"title":"NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization","type":"publication"},{"authors":null,"categories":null,"content":"Introduction Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This work presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.\nDescription of CASIA-Iris-Degradation CASIA-Iris-Degradation contains 36,539 images from 255 Asian people. All images were collected under NIR illumination and two eyes were captured simultaneously. Details of the proposed database are shown in the table below. Setup of image collection As shown in the figure below, we built a collection room using black-out cloth. The curtain on one side of the room can be opened and closed artificially to control the natural light. Inside, there were camera (A), light sources (B, C), volunteer (D), and four directional markers (1-4). Each volunteer was asked to sit down, put their chin on the holder (0.75 m from the camera), keep their head as still as possible, and move their eyes according to instructions. To simulate the off-angle situation, the subject was required to look along a set of directions indicated by four markers in the visual field, while in other cases look straight ahead (i.e., the midpoint of marker 1 and 4). Statistics of the proposed database To simulate real image degradation, the proposed database is divided into four categories, and each of them is separated into three to five subsets as follows:\nIllumination The intensity of the VW light source was adjusted to four levels: Dark (0%), Weak (25%), Medium (50%), Strong (100%) to change the pupil size. In addition, images under natural light were also collected (with Dark level). Note that the intensity in other categories was set to the Medium level by default.\nOff-angle There are four directions: (1) Left, (2) Upper left, (3) Upper right, (4) Right. The left and right are in the horizontal direction, while the upper left and right angles are both 45 degrees.\nNonideal eye state Since it is difficult to keep eyes open all the time, we collected images of closed, squinted, and half-open eyes. Although most images of the former two classes have no effective iris region and are accompanied by blur, they can be used to train eye state detectors for fatigue driving detection or other relevant scenarios.\nOcclusion For occlusion, volunteers were required to wear glasses, masks and using a hand to cover the mouth and nose. During this section, the NIR light source was randomly moved slightly to generate light spots. Meanwhile, some glasses also had stains on the surface to occlude the iris. An unexpected observation is that for some elderly volunteers, their eyelids droop naturally, resulting in severe occlusion, which should arouse more attention.\nMore samples and annotations are presented below. Copyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA-Iris-Degradation database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as \u0026ldquo;Portions of the research in this paper use the CASIA-Iris-Degradation-V1.0 collected by the Chinese Academy of Sciences\u0026rsquo; Institute of Automation (CASIA)\u0026rdquo;.\nTo receive a copy of the database, you can apply for it on our BIT website\n","date":1624924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624924800,"objectID":"4f14c549e748e71567ae385b03685049","permalink":"http://localhost:1313/dataset/casia-iris-degradation/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/dataset/casia-iris-degradation/","section":"dataset","summary":"A Large-scale Database for Less Cooperative Iris Recognition","tags":["Dataset"],"title":"CASIA-Iris-Degradation","type":"dataset"},{"authors":["Jawad Muhammad","Yunlong Wang","Caiyong Wang","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1623802325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623802325,"objectID":"d83e85a74607af3ad54f0936477bee5a","permalink":"http://localhost:1313/publication/jawad-tifs-2021/","publishdate":"2021-06-16T08:58:18Z","relpermalink":"/publication/jawad-tifs-2021/","section":"publication","summary":"Face recognition is a popular and well-studied area with wide applications in our society. However, racial bias had been proven to be inherent in most State Of The Art (SOTA) face recognition systems. Many investigative studies on face recognition algorithms have reported higher false positive rates of African subjects cohorts than the other cohorts. Lack of large-scale African face image databases in public domain is one of the main restrictions in studying the racial bias problem of face recognition. To this end, we collect a face image database namely CASIA-Face-Africa which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The performances of SOTA face recognition algorithms without re-training are reported as baselines. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc. The database can be downloaded from our website.","tags":["smart iris recognition"],"title":"CASIA-Face-Africa: A Large-Scale African Face Image Database","type":"publication"},{"authors":["孙哲南","赫然","王亮","阚美娜","冯建江","郑方","郑伟诗","左旺孟","康文雄","邓伟洪","张杰","韩琥","山世光","王云龙","茹一伟","朱宇豪","刘云帆","何勇"],"categories":[],"content":"","date":1623744725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623744725,"objectID":"448b1021f30f4c6da68a761e458595b8","permalink":"http://localhost:1313/publication/zhenan-joiag-2021/","publishdate":"2021-06-15T08:58:18Z","relpermalink":"/publication/zhenan-joiag-2021/","section":"publication","summary":"从手机解锁、小区门禁到餐厅吃饭、超市收银，再到高铁进站、机场安检以及医院看病，人脸、虹膜和指纹等生物特征已成为人们进入万物互联世界的数字身份证。生物特征识别赋予机器自动探测、捕获、处理、分析和识别数字化生理或行为信号的高级智能，是一个典型而又复杂的模式识别问题，一直处于人工智能技术发展前沿，在新一代人工智能规划、“互联网+”行动计划等国家战略中具有重要地位。由于生物特征识别涉及公众利益攸关的隐私、道德和法律等问题，近期也引起了广泛的社会关注。本文系统综述了生物特征识别学科发展现状、新兴方向、存在问题和可行思路，深入梳理了人脸、虹膜、指纹、掌纹、静脉、声纹、步态、行人重识别以及多模态融合识别的研究进展，以人脸为例重点介绍了生物特征识别领域近些年受到关注的新方向——对抗攻击和防御、深度伪造和反伪造，最后剖析总结了生物特征识别领域存在的3大挑战问题——“感知盲区”、“决策误区”和“安全红区”。本文认为必须变革和创新生物特征的传感、认知和安全机制，才有可能取得复杂场景生物识别学术研究和技术应用的根本性突破，破除现有生物识别技术的弊端，朝着“可感”、“可知”和“可信”的新一代生物特征识别总体目标发展。","tags":["iris super-resolution","smart iris recognition"],"title":"生物特征识别学科发展报告","type":"publication"},{"authors":["Leyuan Wang","Kunbo Zhang","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1623715925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623715925,"objectID":"b95859f772a8ca6e0881f96403be7716","permalink":"http://localhost:1313/publication/wang-ijcb-2021/","publishdate":"2021-06-15T08:58:18Z","relpermalink":"/publication/wang-ijcb-2021/","section":"publication","summary":"For distant iris recognition, a long focal length lens is generally used to ensure the resolution of iris images, which reduces the depth of field and leads to potential defocus blur. To accommodate users at different distances, it is necessary to control focus quickly and accurately. While for users in motion, it is expected to maintain the correct focus on the iris area continuously. In this paper, we introduced a novel rapid autofocus camera for active refocusing of the iris area of the moving objects using a focus-tunable lens. Our end-to-end computational algorithm can predict the best focus position from one single blurred image and generate the proper lens diopter control signal automatically. This scene-based active manipulation method enables real-time focus tracking of the iris area of a moving object. We built a testing bench to collect real-world focal stacks for evaluation of the autofocus methods. Our camera has reached an autofocus speed of over 50 fps. The results demonstrate the advantages of our proposed camera for biometric perception in static and dynamic scenes. The code is available at https://github.com/Debatrix/AquulaCam","tags":["smart iris recognition"],"title":"An End-to-End Autofocus Camera for Iris on the Move","type":"publication"},{"authors":null,"categories":null,"content":"1. Introduction A key problem of iris recognition at a distance is that a large portion of captured iris images is nonideal because of narrow depth of field (DoF), noncooperative user movement, incongruous exposure time and so on. Current iris recognition systems usually filter out these low-quality images using strict criteria of image quality evaluation (IQA). However, this strategy inevitably leads to a waste of device capacity and low throughput. Therefore, a better and practical solution is to make the utmost of degraded iris images for personal identification. We announce the availability of a long-range captured dataset containing 3,756 iris images of various degradation factors from 98 subjects. An evaluation benchmark is built upon the dataset for a comparative study on preprocessing and recognition of NIR iris images in high-throughput scenarios. The datasets, manual annotations and evaluation toolkit are publicly available.\n2. Descriptions and Statistics of the Database Image collection The schematic and setup of blur-varying iris image collection of this database at a distance are shown as following.\nThe next generation of CASIA-LR-Cam bundled with NIR illumination at a wavelength of 830 nm was employed as the capturing device. Its standoff distance is approximately 1.2 meters with a DoF of over 20 centimeters. The field of view (FoV) is approximately 20 degrees. The device was placed in an indoor environment under no extra lighting sources. During the process of image collection, the subjects were obliged to move freely in the restricted square area 1.0~1.4 meters away from the device. Specifically, they could casually step forward and backward, left and right.\nWhile moving inside the restricted area, the subjects were guided by the indication signal on the GUI screen to look at different directions for approximately 30 seconds in a single session. Two separate sessions were launched in the daytime under the same conditions, and the interval was one week. If the subject was wearing glasses, he or she needed to take them off in either of the two sessions (play the video and see).\nSession 1 With Glasses\nSession 2 No glasses\nThe acquired iris image sequences were captured at 5~10 frames per second. The resolution of each frame was 3840x2748. The frames in which irides were completely invisible caused by blinking or squinting were thrown away. Then evenly spaced images are extracted from the processed sequence every 5 frames. On average, approximately 20 images of each subject were retained.\nStatistics of the dataset Attributes The database Camera Type CASIA-LR-Cam II Illumination NIR and natural lighting sources Total pixel 3840x2748 Cropped eye region 640x480 Sessions Two separate sessions Institution of subjects Graduate students and staff of CASIA and TAfIRT Standoff distance 1.0~1.4m Working mode Step freely within a moderate square area Depth of field ca. 20cm No. of subjects 98 No. of Classes 195 No. of Images 3,765 Images per class ca. 19 Pairs of Images 39,418 intraclass and 7,406,312 interclass Mannual Annotations Each image in the dataset is manually annotated with binary maps of iris masks, inner and outer iris boundaries, upper and lower eyelids, and sclera masks shown as below.\nEvaluation Toolkit IrisStat_V3.0.rar\nThe package of evaluation toolkit is organized as below.\nIrisIQA\r│\r└───config/\r│\r└───out/ │\r└───utils/\r| computeMotionblur.m\r| computeSharpness.m\r| ini2struct.m\r| Integral.m\r| progressbar.m\r│ struct2ini.m\r|\r└───MotionBlur_Main.m\r│\r└───Sharpness_Main.m\rSegmentation\r│\r└───config/\r│\r└───out/ │\r└───utils/\r| evalSeg.m\r| Hausdorff.m\r| ini2struct.m\r| progressbar.m\r| struct2ini.m │\r└───IrisSeg_Main.m\rRecognition\r│\r└───config/\r│\r└───out/ │\r└───utils/\r| ACC.m\r| Bitshift.m\r| colors.mat\r| compute_iriscode_sim.m\r| compute_om_sim.m\r| compute_vector_sim.m\r| draw_CMC_curve.m\r| draw_DET_curve.m\r| EER.m\r| IdentiACC.m\r| linspecer.m\r| Merge_Multi_CMC_Curve.m\r| Merge_Multi_Det_Curve.m\r| plot_styles.mat\r| progressbar.m\r| VerfiACC.m\r│\r└───IrisRec_main.m\rThe main function of iris IQA evaluating sharpness is Sharpness_Main.m. The main function of iris segmentation evaluation is IrisSeg_Main.m. The main function of iris recognition evaluation is IrisRec_Main.m. Database Organization The database package comprises the following components organized in multiple directories.\n|\rcircle_params\r| │\r| └───xxxL(R)_xx.ini\r|\rellipse_params\r| │\r| └───xxxL(R)_xx.ini |\rimage\r| │\r| └───xxxL(R)_xx.jpg\r|\riris_edge\r| │\r| └───xxxL(R)_xx.png\r|\riris_edge_mask\r| │\r| └───xxxL(R)_xx.png\r|\riris_edge_rough\r| │\r| └───xxxL(R)_xx.png\r|\riris_mask\r| │\r| └───xxxL(R)_xx.png\r|\rlower_eyelids_edge\r| │\r| └───xxxL(R)_xx.png\r|\rlower_eyelids_edge_rough\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_edge\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_edge_mask\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_edge_rough\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_mask\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_edge_rough\r| │\r| └───xxxL(R)_xx.png\r|\rpupil_mask\r| │\r| └───xxxL(R)_xx.png\r|\rsclera_mask\r| │\r| └───xxxL(R)_xx.png\r|\rup_eyelids_edge\r| │\r| └───xxxL(R)_xx.png\r|\rup_eyelids_edge_rough\r| │\r| └───xxxL(R)_xx.png\r|\rvis_result\r| │\r| └───xxxL(R)_xx.png\r|\rvis_result_new\r| │\r| └───xxxL(R)_xx.png\r|\rimgList.txt\rThe file naming rule is \u0026ldquo;xxxL(R)_xx\u0026rdquo;, where \u0026ldquo;xxx\u0026rdquo; denotes the unique identifier of the subject, \u0026ldquo;L\u0026rdquo; denotes left eye and \u0026ldquo;R\u0026rdquo; denotes right eye and \u0026ldquo;xx\u0026rdquo; denotes the index of the image in the class, e.g., 001L_01. All the filenames of the iris images and belonging classes are stored in imgList.txt.\nCopyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the dataset collected by Smart Iris Recognition (SIR) group from the Chinese Academy of Sciences, Institute of Automation (CASIA)”.\nTo receive a copy of the database, a non-student researcher must manually sign the License Agreement and agree to observe the restrictions. The signed document should be digitized and sent through email to: sir@cripac.ia.ac.cn\n","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623196800,"objectID":"3f1791b18f452b99518efbb984532daa","permalink":"http://localhost:1313/dataset/blurred_iris_benchmark/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/dataset/blurred_iris_benchmark/","section":"dataset","summary":"We announce the availability of a long-range captured dataset containing 3,756 iris images of varying blur levels from 98 subjects. An evaluation benchmark is built upon the dataset for a comparative study on preprocessing and recognition of spatially variant blurred NIR iris images.","tags":["Dataset"],"title":"An Evaluation Benchmark for High-throughput Iris Recognition at a Distance","type":"dataset"},{"authors":null,"categories":null,"content":"Over the past few months, the Program Committee and Area Chairs worked very hard to review each of the 164 papers submitted to IJCB 2021. The reviewers were carefully selected and assigned papers for review in their areas of expertise. A double-blind review policy was adopted. The review process resulted in the selection of the following 66 (40.2%) highly qualified papers to be included in the Technical Program.\n42: A Large-scale Database for Less Cooperative Iris Recognition\n44: Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection\n87: Contrastive Uncertainty Learning for Iris Recognition With Insufficient Labeled Samples\n76: An End-to-End Autofocus Camera for Iris on the Move\n117: Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method\n","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"4249b763a1399d43020dd7dc3754e437","permalink":"http://localhost:1313/post/ijcb2021accept/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/post/ijcb2021accept/","section":"post","summary":"Congratulations to them!.","tags":["smart iris recognition"],"title":"5 papers were accepted by IJCB2021","type":"post"},{"authors":null,"categories":null,"content":"For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and have also affected the iris biometrics, especially iris segmentation. To investigate recent developments and attract more interests of researchers in the iris segmentation method, we are planning to host the challenge competition. In this challenge, we aim to benchmark the performance of iris segmentation on NIR iris images from Asian and African people captured in non-cooperative environments. Moreover, we specially split the general iris segmentation task in the conventional iris recognition pipeline into the segmentation of noise-free iris mask and the localization of inner and outer boundaries of the iris, which are narrowly referred to as iris segmentation and iris localization. Therefore, the challenge encourages the submission of a complete solution taking the iris segmentation and iris localization into consideration.\nOrganisers: Dr. Caiyong Wang, Dr. Yunlong Wang, Dr. Kunbo Zhang, Jawad Muhammad, Tianhao Lu, Prof. Qichuan Tian, Prof. Zhaofeng He, Prof. Zhenan Sun\nPreferred contact person: Dr. Caiyong Wang, wangcaiyong at bucea.edu.cn\nWebsite: https://sites.google.com/view/nir-isl2021/home\nSchedule:\nRegistration closes: April 20, 2021\rPrediction results and technical reports submission deadline: April 20, 2021\rResults announcement: April 30, 2021\rMore Competition:\rhttp://ijcb2021.iapr-tc4.org/competitions/\nResult ll together 30 research groups registered for NIR-ISL 2021, out of which 14 took part in the final round and submitted a total of 27 valid models for scoring.\nAccording to our ranking rules, each submitted entry was assigned one ranking score per evaluation metric and set of testing data. The final ranking was then obtained by adding all 4(evaluation metrics)x5(datasets)(=20) ranking scores (rank sum). The entry with the smallest sum was placed top in the final ranking.\nThe top-3 winning solutions of NIR-ISL 2021 are:\n1st place: Lao Yang Sprint Team (Yiwen Zhang, Tianbao Liu, and Wei Yang, from School of Biomedical Engineering, Southern Medical University)\n2nd place: SUEP-Pixsur (Dongliang Wu, Yingfeng Liu, Ruiye Zhou, and Huihai Wu, from Shanghai University of Electric Power)\n3rd place: EyeCool (Hao Zhang, Junbao Wang, Jiayi Wang, and Wantong Xiong, from College of Science, Northeastern University)\nCongratulations to them!\nFollowing are the team details and results. More details can be found in the future IJCB 2021 summary paper.\n","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"9bb68068f71945fe43263ad80bb5879b","permalink":"http://localhost:1313/post/nir-isl2021/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/post/nir-isl2021/","section":"post","summary":"In this challenge, we aim to benchmark the performance of iris segmentation on NIR iris images from Asian and African people captured in non-cooperative environments.","tags":["smart iris recognition"],"title":"NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021)","type":"post"},{"authors":["Yu Tian","Kunbo Zhang","Leyuan Wang","Chong Zhang"],"categories":[],"content":"","date":1617783125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617783125,"objectID":"352876a55a79c03fd8588f2fc1391da1","permalink":"http://localhost:1313/publication/yu-tian-ijcb-2021/","publishdate":"2021-04-07T08:58:18Z","relpermalink":"/publication/yu-tian-ijcb-2021/","section":"publication","summary":"Spectacles reflection removal is a challenging problem in iris recognition research. The reflection of the spectacles usually contaminates the iris image acquired under infrared illumination. The intense light reflection caused by the active light source makes reflection removal more challenging than normal scenes since important iris texture features are entirely obscured. Eliminating unnecessary reflections can effectively improve iris recognition system performance. This paper proposes a spectacle reflection removal algorithm based on ray coding and ray tracking to remove spectacle reflection in iris images. By decoding the light source’s encoded light beam, the iris imaging device eliminates most of the stray light. Our binocular imaging device tracks the light path to obtain parallax information and realizes reflected light spot removal through image fusion. We designed a prototype system to verify our proposed method in this paper. This method can effectively eliminate reflections without changing iris texture and improve iris recognition in complex scenarios.","tags":["smart iris recognition"],"title":"Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method","type":"publication"},{"authors":["Yiwei Ru","Wanting Zhou","Yunfan Liu","Jianxin Sun","Qi Li"],"categories":[],"content":"","date":1617783125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617783125,"objectID":"c114a41d934a853d7fe7a2e06f4b36d0","permalink":"http://localhost:1313/publication/yiwei-ru-ijcb-2021/","publishdate":"2021-04-07T08:58:18Z","relpermalink":"/publication/yiwei-ru-ijcb-2021/","section":"publication","summary":"Deep forgery detection on video data has attracted remarkable research attention in recent years due to its potential in defending forgery attacks. However, existing methods either only focus on the visual evidence within individual images, or are too sensitive to fluctuations across frames. To address these issues, this paper propose a novel model, named Bita-Net, to detect forgery faces in video data. The network design of Bita-Net is inspired by the mechanism of how human beings detect forgery data, i.e. browsing and scrutinizing, which is reflected by the two-pathway architecture of Bita-Net. Concretely, the browsing pathway scans the entire video at a high frame rate to check the temporal consistency, while the scrutinizing pathway focuses on analyzing key frames of the video at a lower frame rate. Furthermore, an attention branch is introduced to improve the forgery detection ability of the scrutinizing pathway. Extensive experiment results demonstrate the effectiveness and generalization ability of Bita-Net on various popular face forensics detection datasets, including FaceForensics++, CelebDF, DeepfakeTIMIT and UADFV.","tags":["Facial Video Forgery Detection"],"title":"Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection","type":"publication"},{"authors":["Jianze Wei","Ran He","Zhenan Sun"],"categories":[],"content":"","date":1617783125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617783125,"objectID":"47a259738a0e6303ad5c545b115b0857","permalink":"http://localhost:1313/publication/wei-ijcb-2021/","publishdate":"2021-04-07T08:58:18Z","relpermalink":"/publication/wei-ijcb-2021/","section":"publication","summary":"Cross-database recognition is still an unavoidable challenge when deploying an iris recognition system to a new environment. In the paper, we present a compromise problem that resembles the real-world scenario, named iris recognition with insufficient labeled samples. This new problem aims to improve the recognition performance by utilizing partially-or un-labeled data. To address the problem, we propose Contrastive Uncertainty Learning (CUL) by integrating the merits of uncertainty learning and contrastive self-supervised learning. CUL makes two efforts to learn a discriminative and robust feature representation. On the one hand, CUL explores the uncertain acquisition factors and adopts a probabilistic embedding to represent the iris image. In the probabilistic representation, the identity information and acquisition factors are disentangled into the mean and variance, avoiding the impact of uncertain acquisition factors on the identity information. On the other hand, CUL utilizes probabilistic embeddings to generate virtual positive and negative pairs. Then CUL builds its contrastive loss to group the similar samples closely and push the dissimilar samples apart. The experimental results demonstrate the effectiveness of the proposed CUL for iris recognition with insufficient labeled samples.","tags":["smart iris recognition"],"title":"Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples","type":"publication"},{"authors":["Zhenan Sun","He Ran","Wang Liang","Kan Meina","Feng Jianjiang","Zheng Fang","Zheng Weishi","Zuo Wangmeng","Kang Wenxiong","Deng Weihong","Zhang Jie","Han Hu","Shan Shiguang","Wang Yunlong","Ru Yiwei","Zhu Yuhao","Liu Yunfan","He Yong"],"categories":[],"content":"","date":1615450325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615450325,"objectID":"0d416338513022475da63f53d30fc86b","permalink":"http://localhost:1313/publication/sun-jig-2021/","publishdate":"2021-03-11T08:58:18Z","relpermalink":"/publication/sun-jig-2021/","section":"publication","summary":"Biometrics, such as face, iris, and fingerprint recognition, have become digital identity proof for people to enter the “Internet of Everything” . For example, one may be asked to present the biometric identifier for unlocking mobile, passing access control at airports, rail stations, and paying at supermarkets or restaurants. Biometric recognition empowers a machine to automatically detect, capture, process, analyze, and recognize digital physiological or behavioral signals with advanced intelligence. Thus, biometrics requires interdisciplinary research of science and technology involving optical engineering, mechanical engineering, electronic engineering, machine learning, pattern recognition, computer vision, digital image processing, signal analysis, cognitive science, neuroscience, human-computer interaction, and information security. Biometrics is a typical and complex pattern recognition problem, which is a frontier research direction of artificial intelligence. In addition, biometric identification is a key development area of Chinese strategies, such as the Development Plan on the New Generation of Artificial Intelligence and the “Internet Plus” Action Plan. The development of biometric identification involves public interest, privacy, ethics, and law issues; thus, it has also attracted widespread attention from the society. This article systematically reviews the development status, emerging directions, existing problems, and feasible ideas of biometrics and comprehensively summarizes the research progress of face, iris, fingerprint, palm print, finger / palm vein, voiceprint, gait recognition, person reidentification, and multimodal biometric fusion. The overview of face recognition includes face detection, facial landmark localization, 2D face feature extraction and recognition, 3D face feature extraction and recognition, facial liveness detection, and face video based biological signal measurement. The overview of iris recognition includes iris image acquisition, iris segmentation and localization, iris liveness detection, iris image quality assessment, iris feature extraction, heterogeneous iris recognition, fusion of iris and other modalities, security problems of iris biometrics, and future trends of iris recognition. The overview of fingerprint recognition includes latent fingerprint recognition, fingerprint liveness detection, distorted fingerprint recognition, 3D fingerprint capturing, and challenges and trends of fingerprint biometrics. The overview of palm print recognition mainly introduces databases, feature models, matching strategies, and open problems of palm print biometrics. The overview of vein biometrics introduces main datasets and algorithms for finger vein, dorsal hand vein, and palm vein, and then points out the remaining unsolved problems and development trend of vein recognition. The overview of gait recognition introduces model-based and model-free methods for gait feature extraction and matching. The overview of person reidentification introduces research progress of new methods under supervised, unsupervised and weakly supervised conditions, gait database virtualization, generative gait models, and new problems, such as clothes changing, black clothes, and partial occlusions. The overview of voiceprint recognition introduces the history of speaker recognition, robustness of voiceprint, spoofing attacks, and antispoofing methods. The overview of multibiometrics introduces image-level, feature-level, score-level, and decision-level information fusion methods and deep learning based fusion approaches. Taking face as the exemplar biometric modality, new research directions that have received great attentions in the field of biometric recognition in recent years, i. e. , adversarial attack and defense as well as Deepfake and anti-Deepfake, are also introduced. Finally, we analyze and summarize the three major challenges in the field of biometric recognition——— “ the blind spot of biometric sensors”, “ the decision errors of biometric algorithms” and “the red zone of biometric security” . Therefore, the sensing, cognition, and security mechanisms of biometrics are necessary to achieve a fundamental breakthrough in the academic research and technologies applications of biometrics in complex scenarios to address the shortcomings of the existing biometric technologies and to move towards the overall goal of developing a new generation of “ perceptible, “ robust”, and “ trustworthy” biometric identification technology.","tags":["smart iris recognition"],"title":"Overview of biometrics research","type":"publication"},{"authors":null,"categories":null,"content":"Introduction Over the years, numerous face databases have been published that brought about exciting breakthrough in the facial biometric research field, most especially from the recent trend of deep learning contributions. However, as investigated by researchers, most of these databases are demographically imbalanced and often contain few number of African cohorts. Of those with relatively large number of the Africans, the databases are usually wild (downloaded from the internet or digitalised from printed photographs). Methods that adopt these skewed databases often exhibits some form of performance bias that can result to unintended consequences for real time applications. As such, there is need for more demographically inclusive datasets. CASIA-Face-Africa is developed to provide solution to this problem. It is an all-African database that is made to be used as a complementary database with the existing databases to balance the number of the African cohorts in the published datasets and improve their demographic inclusiveness.\nDESCRIPTION The database images were captured at various locations in Nigeria, Africa. About 1150 volunteers participated in the capturing exercise. The images of each subject were captured concurrently using 3 cameras. Two visible wavelength (VW) cameras and one near-infrared (NIR) camera. The capturing was done in various sessions over a period of 3 months. Some of the subjects have images captured in multiple sessions while majority of the subjects have their images captured in a single session. Figure 1: The cameras arrangement set-up was made to be same in all the capturing sessions\nFor each capturing instance, 3 to 10 still images were captured by each camera at a fixed time interval of 1 to 3 seconds. For some subjects, an external illumination light source was used for capturing additional images of that subjects. Also, some subjects were asked to use face accessory such as eye glasses for multiple capturing. The captured images were then organized and their land mark labelled. The organized database comprises a total of 38,546 images from 1,183 subjects. Specifically, 12,063 images captured by VW camera 1 at the resolution of 1332×1080, 13,232 images are captured by VW camera 2 at the resolution of 787 × 962, and 13,251 images are captured by NIR camera at the resolution of 983 × 877. Some samples of the captured images are shown in Figure 2, Figure 3 and Figure 4. Figure 2: Sample of a single subject images\nFigure 3: Sample of subject Expressions\nFigure 4: Sample of labelled face images\nDatabase Organization The database package comprises of the following components organised in multiple directories:\nImages folder: This contain the actual face images. Each file is named as SubjectID_ImageNumber.jpg. The SubjectID is a unique number for each subject and the ImageNumber is a sequential number for images of the same subject. Subjects folder: This contain the corresponding ini files that describe the unique subjects in the images. Each file is named with the ID of the subject as SubjectID.ini. Attributes folder: This contain the corresponding ini files that describes each individual face image. Each file is named with its corresponding face image name as SubjectID_ImageNumber.ini Protocols folder: This contain the files named with each of the defined proposed protocols. Example of the names: ID-V-All-Ep1.ini, ID-I-Split-Ep3.ini, etc. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper. Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications. Copyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as \u0026ldquo;Portions of the research in this paper use the CASIA-Face-Africa collected by the Chinese Academy of Sciences\u0026rsquo; Institute of Automation (CASIA)\u0026rdquo;.\nTo receive a copy of the database, you can apply for it on our BIT website\nReference [1] Muhammad Jawad, Yunlong Wang, Caiyong Wang, Kunbo Zhang, Zhenan Sun. “CASIA-Face-Africa: A Large-scale African Face Image Database,” IEEE Transactions on Information Forensics and Security (TIFS), vol.16, pp. 3634-3646, 2021.\n","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"8a3a1319f8584aee9085605c16f3c031","permalink":"http://localhost:1313/dataset/casia-face-africa/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/dataset/casia-face-africa/","section":"dataset","summary":"CASIA-Face-Africa is an all-African database that is made to be used as a complementary database with the existing databases to balance the number of the African cohorts in the published datasets and improve their demographic inclusiveness","tags":["Dataset"],"title":"CASIA-Face-Africa","type":"dataset"},{"authors":null,"categories":null,"content":"Zhang published paper, All-in-Focus Iris Camera With a Great Capture Volume, wins the PC chairs choice best paper award runner-Up at the 2020 International Joint Conference on Biometrics (IJCB 2020).\nThe 2020 International Joint Conference on Biometrics (IJCB 2020) combines two major biometrics research conferences, the Biometrics Theory, Applications and Systems (BTAS) conference and the International Conference on Biometrics (ICB). The blending of these two conferences in 2020 is through a special agreement between the IEEE Biometrics Council and the IAPR TC-4, and should present an exciting event for the entire worldwide biometrics research community.\nIn this work, a novel all-in-focus iris imaging system is developed. It using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth offield extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed.\n","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"c63290fa64425cc14ee4abed7f9a838e","permalink":"http://localhost:1313/post/ijcb2020award/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/post/ijcb2020award/","section":"post","summary":"Zhang published paper, All-in-Focus Iris Camera With a Great Capture Volume, wins the PC chairs choice best paper award runner-Up at the 2020 International Joint Conference on Biometrics (IJCB 2020).","tags":["smart iris recognition"],"title":"Kunbo Zhang Wins IJCB 2020 Best Paper Award Runner-Up","type":"post"},{"authors":["M. Vitek","A. Das","Y. Pourcenoux","et.al"],"categories":[],"content":"","date":1601280725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601280725,"objectID":"184e50ec90d79b5fddef397448fa0d77","permalink":"http://localhost:1313/publication/vitek-ijcb-2020/","publishdate":"2020-09-28T08:58:18Z","relpermalink":"/publication/vitek-ijcb-2020/","section":"publication","summary":"The paper presents a summary of the 2020 Sclera Segmentation Benchmarking Competition (SSBC), the 7th in the series of group benchmarking efforts centred around the problem of sclera segmentation. Different from previous editions, the goal of SSBC 2020 was to evaluate the performance of sclera-segmentation models on images captured with mobile devices. The competition was used as a platform to assess the sensitivity of existing models to i) differences in mobile devices used for image capture and ii) changes in the ambient acquisition conditions. 26 research groups registered for SSBC 2020, out of which 13 took part in the final round and submitted a total of 16 segmentation models for scoring. These included a wide variety of deep-learning solutions as well as one approach based on standard image processing techniques. Experiments were conducted with three recent datasets. Most of the segmentation models achieved relatively consistent performance across images captured with different mobile devices (with slight differences across devices), but struggled most with low-quality images captured in challenging ambient conditions, i.e., in an indoor environment and with poor lighting.","tags":["Sclera Segmentation"],"title":"SSBC 2020: Sclera Segmentation Benchmarking Competition in the Mobile Environment","type":"publication"},{"authors":null,"categories":null,"content":"\r\u0026emsp;\u0026emsp;虹膜识别是进一步提升安全性、可靠性的必由之路 \u0026emsp;\u0026emsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\r\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;———谭铁牛院士 \u0026emsp;\u0026emsp;虹膜识别是最具潜力的生物识别方法之一，是识别率高、非接触性、防欺骗性好的识别方法。虹膜属于人眼的一部分，如下图所示。\r\u0026emsp;\u0026emsp;人眼的外观主要由巩膜、虹膜、瞳孔三部分组成，其中巩膜即眼球外围的白色部分，约占人眼总面积的30%，眼睛的中心为瞳孔部分，约占5%，虹膜位于巩膜和瞳孔之间，约占整个眼睛的65%，包含了最丰富的纹理信息，外观上看，虹膜由许多腺窝、褶皱、色素斑等构成，是人体中最独特的结构之一。因为瞳孔、虹膜和巩膜一般颜色不同，灰度值呈梯度变化，所以根据它们灰度不同，可以将它们明显分开。从几何形状可以看出，虹膜的内、外边界可以近似为圆形，这使它具有易检测性。临床观察发现：虹膜在人的一生当中几乎不发生变化，只有很少的虹膜纹理可能会由于年龄或者外伤导致纹理破坏。\r\u0026emsp;\u0026emsp;作为表示个人身份的标识物，必须具备作为身份标识的重要特征。人脸、指纹等许多生物特征具有作为身份标识的特性，但是，虹膜在这些特性方面表现的更为突出，具有许多先天优势，是其他生物特征无法与之媲美的。\r普遍性————虹膜是每个人天生都具有的。唯一性————虹膜的纤维组织细节复杂而丰富，每个人错综复杂的虹膜独一无二，只与虹膜的形成过程有关。稳定性————虹膜从婴儿胚胎发育的第三个月起开始发育，到第八个月虹膜的主要纹理结构已经成形。非入侵检测————从一定距离即可获得虹膜数字图像，无需用户接触设备。可接受程度好————虹膜识别以其认证准确度高、速度快、安全性高，被用户所接受。可检测性————利用图像处理技术检测出虹膜边界，易于拟合分割和和归一化防伪性高————虹膜的半径小，在可见光下中国人的虹膜图像呈现深褐色，看不到纹理信息，需要虹膜图像专业采集设备和用户的配合，所以一般情况下很难被盗取防欺骗性好————虹膜的唯一性决定了不同人眼的虹膜很难被冒充模仿。\r\u0026emsp;\u0026emsp;生物特征识别通过捕获生物样本，然后采用数学方法把样本转化成相同大小的模板，提取有效的可区别性特征，就可以客观地和其他一个完整的虹膜身份识别系统主要由四个部分组成：虹膜图像获取、虹膜图像预处理、虹膜特征提取、模式比对。\r虹膜图像获取 \u0026emsp;虹膜图像采集的目的是为了获取有效的虹膜图像，在传统的虹膜识别场景中，通常采用专业的成像装置在近红外光（波长700nm到900nm）照射和用户的配合下才能捕获清晰的高分辨率虹膜图像。近些年来，随着光学镜头、传感器和计算成像技术等的发展，虹膜识别的可用距离不断变大，相关装置也变得越来越轻巧实用，“远距离”、“行进中”、“移动端”和“可见光下”等少约束场景的虹膜识别对于用户使用时的约束越来越少，极大地提升了虹膜识别应用范围和用户友好性。\r虹膜图像预处理\r虹膜图像除了必须的虹膜区域以外，也包含了诸如巩膜、睫毛、瞳孔等非虹膜区域，因此不能直接用于虹膜识别。其次，一些噪声茹照明变化、睫毛遮挡、镜面反射、瞳孔放缩等会明显增加虹膜的类内差异，降低虹膜的识别率。常规的虹膜预处理步骤包括：虹膜活体检测、虹膜图像质量评估、虹膜分割、虹膜归一化和虹膜图像增强。\r虹膜图像特征分析\r虹膜图像特征分析主要包含两部分：特征提取和对比分类。虹膜特征提取是指从归一化的虹膜图像中提取紧凑有区分的虹膜特征，然后使用计算机可以存储和读取的格式进行编码。虹膜比对和分类(或者称为匹配)是指将提取的虹膜特征编码和事先在数据库注册过的虹膜特征编码通过某种相似性度量比如汉明距离、余弦距离进行对比，计算相似性分数，依次确定用户身份。\r","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"ceaaad5587e16c03d7ce4d8b845013a6","permalink":"http://localhost:1313/project/iris-recognition/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/project/iris-recognition/","section":"project","summary":"Iris recognition is considered to be the safest and most accurate recognition method","tags":["Iris Recognition"],"title":"Iris Recognition","type":"project"},{"authors":["Min Ren"],"categories":null,"content":"\r","date":1597909378,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597909378,"objectID":"7dc9fb0cd97bbb03983cbff577741d36","permalink":"http://localhost:1313/publication/afdr-iris-recognition/afdr-iris-recognition/","publishdate":"2020-08-20T07:42:58Z","relpermalink":"/publication/afdr-iris-recognition/afdr-iris-recognition/","section":"publication","summary":"Alignment Free, Iris Recognition, Preprocessing.","tags":["Source Themes"],"title":"Alignment Free and Distortion Robust Iris Recognition","type":"publication"},{"authors":null,"categories":null,"content":"\r\u0026emsp;\u0026emsp;包括计算摄影（例如，人像模式和闪光反射）和增强现实效果（例如，虚拟化身）在内的大量实际应用程序都依赖于通过跟踪虹膜来估计眼睛位置。一旦获得了准确的虹膜跟踪，我们就可以确定从相机到用户的距离，而无需使用专用的深度传感器。反过来，这可以改善各种用例，从计算摄影到适当大小的眼镜和帽子的虚拟试戴，到根据视听者的距离采用字体大小的可用性增强。\r由于有限的计算资源，可变的光照条件和遮挡物（例如头发或人斜视）的存在，虹膜跟踪是在移动设备上解决的一项艰巨任务。通常，会使用复杂的专用硬件，从而限制了可在其中应用该解决方案的设备范围。\r由MediaPipe Iris实现的眼睛重新着色示例 \u0026emsp;\u0026emsp;谷歌日前发布了用于精确虹膜估计的全新机器学习模型：MediaPipe Iris。所述模型以MediaPipe Face Mesh的研究作为基础，而它无需专用硬件就能够通过单个RGB摄像头实时追踪涉及虹膜，瞳孔和眼睛轮廓的界标。利用虹膜界标，模型同时能够在不使用深度传感器的情况下以相对误差小于10％的精度确定对象和摄像头之间的度量距离。请注意，虹膜追踪不会推断人们正在注视的位置，同时不能提供任何形式的身份识别。MediaPipe是一个旨在帮助研究人员和开发者构建世界级机器学习解决方案与应用程序的开源跨平台框架，所以在MediaPipe中实现的这一系统能够支持大多数现代智能手机，PC，笔记本电脑，甚至是Web。\r用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定 1. 用于虹膜追踪的机器学习管道\n谷歌介绍道，开发系统的第一步利用了之前针对3D Face Meshes的研究，亦即通过高保真面部界标来生成近似面部几何形状的网格。根据所述网格，研究人员分离出原始图像中的眼睛区域以用于虹膜追踪模型。然后，谷歌将问题分为两个部分：眼睛轮廓估计和虹膜位置。他们设计了一个由一元化编码器组成的多任务模型，每个组件对应一个任务，这样就能够使用特定于任务的训练数据。\n用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定 \u0026emsp;\u0026emsp;为了将裁剪后的眼睛区域用于模型训练，团队手动注释了大约50万张图像。其中，图像涵盖了不同地理位置的各种照明条件和头部姿势，如下所示\r用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定 裁剪的眼睛区域形成模型的输入，而它将通过单独的组件预测界标 2. 虹膜深度：用单个图像进行深度估计 无需任何专门的硬件，这个虹膜追踪模型能够以不到10％的误差确定对象到摄像头的度量距离。相关的原理事实是，人眼的水平直径虹膜基本恒定为11.7±0.5毫米。作为说明，请想象将针孔摄像头模型投影到正方形像素的传感器。你可以使用摄像头的焦距估计从面部界标到对象的距离，而这可以通过Camera Capture API或直接从捕获图像的EXIF元数据，以及其他摄像头固有参数进行获取。给定焦距，对象到摄像头的距离与对象眼睛的物理尺寸成正比，如下图所示\n利用类似的三角形，我们可以根据焦距（f）和虹膜大小来计算对象的距离（d） 左边：在Pixel 2运行的MediaPipe Iris正在以cm为单位估计度量距离，没有采用任何深度摄像头；右边：groud-truth深度 \u0026emsp;\u0026emsp;为了量化所述方法的精确性，研究人员收集了200多位被试的正向同步视频和深度图像，并将其与iPhone 11的深度传感器进行比较。团队使用激光测距设备，通过实验确定iPhone 11的深度传感器在2米以内的误差小于2％。对于使用虹膜大小进行深度估算的方法，平均相对误差为4.3％，标准偏差是2.4％。谷歌对有眼镜被试和正常视力被试（不计入隐形眼镜情况）测试了所述方法，并发现眼镜会将平均相对误差略微提高到4.8％（标准偏差是3.1％）。另外，实验没有测试存在任何眼睛疾病的被试。考虑到MediaPipe Iris不需要专门的硬件，所述结果表明系统能够支持一系列成本范围的设备根据单张图像获取度量深度\r估计误差的直方图（左边），以及实际和估计距离的比较（右边） 3. 发布MediaPipe Iris\n\u0026emsp;\u0026emsp;这个虹膜和深度估计模型将作为支持PC，移动设备和Web的跨平台MediaPipe管道发布。正如谷歌在最近一篇关于MediaPipe的博文所述，团队利用WebAssembly和XNNPACK在浏览器中本地运行Iris ML管道，无需将任何数据发送到云端。\r使用MediaPipe的WASM堆栈。你可以在浏览器种运行模型 仅使用包含EXIF数据的单张图片计算虹膜深度 4. 未来方向\n\u0026emsp;\u0026emsp;谷歌计划进一步扩展MediaPipe Iris模型，实现更稳定的追踪性能以降低误差，并将其部署用于无障碍用例。谷歌在相关文档和随附的Model Card中详细说明了预期的用途，限制和模型的公平性，从而确保模型的使用符合谷歌的AI原则。请注意，任何形式的监视监控都明显超出应用范围，故不予支持。团队表示：“我们希望的是，通过向广泛的研究与开发社区提供这种虹膜感知功能，从而促使创造性用例的出现，激发负责任的新应用和新研究途径。”","date":1596844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596844800,"objectID":"81cf5b1dee52c3a2704cf032b84764b9","permalink":"http://localhost:1313/post/mediapipe-iris/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/post/mediapipe-iris/","section":"post","summary":"Real time iris tracking and depth estimation","tags":["Iris Recognition"],"title":"MediaPipe Iris","type":"post"},{"authors":null,"categories":null,"content":" 8月5日晚芒果台，中科院自动化研究所智能感知与计算研究中心助理研究员王云龙老师带你探索虹膜识别的奥秘\n","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596585600,"objectID":"18b6e328f4025e374d7cf62a484c9871","permalink":"http://localhost:1313/post/how-does-iris-recognize-identity-successfully/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/post/how-does-iris-recognize-identity-successfully/","section":"post","summary":"If the eye is the window of the soul, the iris is equivalent to the ID card of the window. So, what is the secret in our eyes?","tags":["Internal"],"title":"How does iris recognize identity successfully?","type":"post"},{"authors":null,"categories":null,"content":" 7月13日播出的CCTV-1《生活圈》节目中，谭铁牛院士现身为观众介绍了虹膜识别技术。谭铁牛院士介绍到：“虹膜识别是一种相对比较新颖的生物特征识别技术，下一步虹膜识别技术会进一步朝着移动化、便捷化以及和其他的相关的生物特征识别技术，比如人脸识别技术，相融合的方向发展，具有非常广阔的发展空间。”\n\u0026emsp;\u0026emsp;自动化所孙哲南研究员也携团队相关成果做客节目，具体讲解了虹膜识别技术的优势与应用。\r虹膜识别技术的优势\r\u0026emsp;\u0026emsp;虹膜识别是利用人眼表面黑色瞳孔和白色巩膜之间圆环状的区域进行身份识别的技术。虹膜识别的优势在于： 第一，虹膜先天具有非常高的唯一性。虹膜中可以发现证明至少244个独立变量来决定其唯一性，而指纹和人脸大概只有十几个或者几十个这样的变量。\n第二，虹膜终身不变。年龄的增长、化妆或者整容可以改变人的容貌，却无法改变虹膜\n虹膜识别的应用 1. 虹膜识别应用于手机\n孙哲南研究员在节目中展示了团队研发的虹膜识别解锁手机，在手机终端装载虹膜识别模块，直接刷眼就可以解锁手机。防护镜、墨镜甚至黑暗的环境都不会成为虹膜识别的阻碍。\n2. 虹膜识别应用于电脑\n使用虹膜解锁电脑，刷眼后一瞬间即可安全登陆，省去了总是忘记密码与密码被盗的烦恼。\n3. 虹膜识别防盗门锁\n只需对准虹膜采集框，即可解锁开门。团队展示的虹膜锁采用近红外主动光源成像，即使在光线很暗的楼道内，虹膜锁也可以正常工作。\n4. 虹膜识别收费闸机\n想象一下，当我们驾车通过收费闸机时，只需要刷一下眼睛，就可以自动收费抬杆，这是一种什么样的感觉呢？将来，这一系统也可以应用于高速公路ETC中，驾驶员就可以直接通过眼神识别进行缴费。\n其实，虹膜识别的应用远不止这些，并且在不远的将来，它还可以在更多地方得以运用，为人们的生活提供超乎想象的便利！\n","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594598400,"objectID":"f23406c5ec08c2fbf3e3e66608b5ebc2","permalink":"http://localhost:1313/post/academician-introduces-you-to-iris-recognition/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/post/academician-introduces-you-to-iris-recognition/","section":"post","summary":"If the eye is the window of the soul, the iris is equivalent to the ID card of the window. So, what is the secret in our eyes?","tags":["Internal"],"title":"Academician introduces you to iris recognition","type":"post"},{"authors":null,"categories":null,"content":"\rDownload the whole database (1.86GB) OR\nDownload the separated subsets below\nDownload CASIA-Iris-Interval (30.9MB) Download CASIA-Iris-Lamp (390MB) Download CASIA-Iris-Twins (60MB) Download CASIA-Iris-Distance(767MB) Download CASIA-Iris-Thousand (490MB) Download CASIA-Iris-Syn (171MB) 1. Introduction With the pronounced need for reliable personal identification, iris recognition has become an important enabling technology in our society. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. Automatic iris recognition has to face unpredictable variations of iris images in real-world applications. For example, recognition of iris images of poor quality, nonlinearly deformed iris images, iris images at a distance, iris images on the move, and faked iris images all are open problems in iris recognition. A basic work to solve the problems is to design and develop a high quality iris image database including all these variations. Moreover, a novel iris image database may help identify some frontier problems in iris recognition and leads to a new generation of iris recognition technology.\nCASIA Iris Image Database (CASIA-Iris) developed by our research group has been released to the international biometrics community and updated from CASIA-IrisV1 to CASIA-IrisV3 since 2002. More than 3,000 users from 70 countries or regions have downloaded CASIA-Iris and much excellent work on iris recognition has been done based on these iris image databases. Although great progress of iris recognition has been achieved since 1990s, the rapid growth of iris recognition applications has clearly highlighted two challenges, i.e. usability and scalability.\nUsability is the largest bottleneck of current iris recognition. It is a trend to develop long-range iris image acquisition systems for friendly user authentication. However, iris images captured at a distance are more challenging than traditional close-up iris images. Lack of long-range iris image data in the public domain has hindered the research and development of next-generation iris recognition systems.\nMost current iris recognition methods have been typically evaluated on medium sized iris image databases with a few hundreds of subjects. However, more and more large-scale iris recognition systems are deployed in real-world applications. Many new problems are met in classification and indexing of large-scale iris image databases. So scalability is another challenging issue in iris recognition.\nIn order to promote research on long-range and large-scale iris recognition systems, we are pleased to release to the public domain CASIA Iris Image Database V4.0 (or CASIA-IrisV4 for short).\n2. Brief Descriptions and Statistics of the Database CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn.\nCASIA-IrisV4 contains a total of 54,601 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.\n2.1 CASIA-Iris-Interval Iris images of CASIA-Iris-Interval were captured with our self-developed close-up iris camera (Fig.1). The most compelling feature of our iris camera is that we have designed a circular NIR LED array, with suitable luminous flux for iris imaging. Because of this novel design, our iris camera can capture very clear iris images (see Fig.2). CASIA-Iris-Interval is well-suited for studying the detailed texture features of iris images.\nFig.1 The self-developed iris camera used for collection of CASIA-Iris-Interval Fig.2 Example iris images in CASIA-Iris-Interval\n2.2 CASIA-Iris-Lamp CASIA-Iris-Lamp was collected using a hand-held iris sensor produced by OKI (Fig.3). A lamp was turned on/off close to the subject to introduce more intra-class variations when we collected CASIA-Iris-Lamp. Elastic deformation of iris texture (Fig.4) due to pupil expansion and contraction under different illumination conditions is one of the most common and challenging issues in iris recognition. So CASIA-Iris-Lamp is good for studying problems of non-linear iris normalization and robust iris feature representation.\nFig.3 The hand-held iris camera used for collection of CASIA-Iris-Lamp Fig.4 Example iris images in CASIA-Iris-Lamp\n2.3 CASIA-Iris-Twins CASIA-Iris-Twins contains iris images of 100 pairs of twins, which were collected during Annual Twins Festival in Beijing using OKI\u0026rsquo;s IRISPASS-h camera (Fig.5). Although iris is usually regarded as a kind of phenotypic biometric characteristics and even twins have their unique iris patterns, it is interesting to study the dissimilarity and similarity between iris images of twins.\nFig.5 Example iris images in CASIA-Iris-Twins\n2.4 CASIA-Iris-Distance CASIA-Iris-Distance contains iris images captured using our self-developed long-range multi-modal biometric image acquisition and recognition system (LMBS, Fig.6). The advanced biometric sensor can recognize users from 3 meters away by actively searching iris, face or palmprint patterns in the visual field via an intelligent multi-camera imaging system. The LMBS is human-oriented by fusing computer vision, human computer interaction and multi-camera coordination technologies and improves greatly the usability of current biometric systems. The iris images of CASIA-Iris-Distance were captured by a high resolution camera so both dual-eye iris and face patterns are included in the image region of interest (Fig. 7). And detailed facial features such as skin pattern are also visible for multi-modal biometric information fusion.\nFig.6 The biometric sensor used for collection of CASIA-Iris-Distance Fig.7 An example image in CASIA-Iris-Distance\n2.5 CASIA-Iris-Thousand CASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Fig. 8) produced by IrisKing. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.\nFig.8 The iris camera used for collection of CASIA-Iris-Thousand Fig.9 An example image in CASIA-Iris-Thousand\n2.6 CASIA-Iris-Syn CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in [1] (Fig. 10). Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in [1] that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.\nFig. 10 Flowchart of the iris texture synthesis method for generation of CASIA-Iris-Syn Fig. 11 Example iris images in CASIA-Iris-Syn\n3. Database Organization The file name of each image in CASIA-IrisV4 is unique to each other and denotes some useful properties associated with the image such as subset category, left/right/double, subject ID, class ID, image ID etc. The file naming rules of all six subsets are listed as follows:\nThe images of CASIA-Iris-Interval are stored as:\nroot_path/CASIA-Iris-Interval/YYY/S1YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\nThe images of CASIA-Iris-Lamp are stored as:\nroot_path/CASIA-Iris-Lamp/YYY/E/S2YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\nThe images of CASIA-Iris-Twins are stored as:\nroot_path/CASIA-Iris-Twins\\XX\\YE\\S3XXYENN.jpg\nXX: the index of family\nY: the identifier to one of the twins\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\nThe images of CASIA-Iris-Distance are stored as:\nroot_path/CASIA-Iris-Distance/YYY/S4YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘D’ denotes dual-eye iris image\nNN: the index of the image in the class\nThe images of CASIA-Iris-Thousand are stored as:\n$ root path$ /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\nThe images of CASIA-Iris-Syn are stored as:\nroot_path/CASIA-Iris-Syn/YYY/S6YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘S’ denotes it is a synthesized iris image\nNN: the index of the image in the class\n4. Copyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-IrisV4 collected by the Chinese Academy of Sciences\u0026rsquo; Institute of Automation (CASIA)” and a reference to “CASIA Iris Image Database, http://biometrics.idealtest.org/” should be included. A copy of all reports and papers that are for public or general release that use the CASIA-IrisV4 should be forwarded upon release or publication to:\nProfessor Tieniu Tan\nCenter for Biometrics and Security Research\nNational Laboratory of Pattern Recognition\nInstitute of Automation, Chinese Academy of Sciences\nP.O.Box 2728\nBeijing 100190\nChina\nor send electronic copies to znsun@nlpr.ia.ac.cn.\nQuestions regarding this database can be addressed to Dr. Zhenan Sun at\nDr. Zhenan Sun\nCenter for Biometrics and Security Research\nNational Laboratory of Pattern Recognition\nInstitute of Automation, Chinese Academy of Sciences\nP.O.Box 2728\nBeijing 100190\nChina\nTel: +86 10 8261 0278\nFax: +86 10 6255 1993\nEmail: znsun@nlpr.ia.ac.cn\nPublications Tieniu Tan, Zhaofeng He, Zhenan Sun, \u0026ldquo;Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition\u0026rdquo;, Image and Vision Computing, Vol.28, No. 2, 2010, pp.223-230. T. Tan and L. Ma, “Iris Recognition: Recent Progress and Remaining Challenges”, Proc. of SPIE, Vol. 5404, pp. 183-194, 12-13 Apr 2004, Orlando, USA. Zhenan Sun, Tieniu Tan, \u0026ldquo;Ordinal Measures for Iris Recognition,\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 12, 2009, pp. 2211 - 2226. Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, \u0026ldquo;Towards Accurate and Fast Iris Segmentation for Iris Biometrics\u0026rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 9, 2009, pp.1670 - 1684. L. Ma, T. Tan, Y. Wang and D. Zhang, “Personal Identification Based on Iris Texture Analysis”, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Vol. 25, No. 12, pp.1519-1533, 2003. Li Ma, Tieniu Tan, Yunhong Wang and Dexin Zhang, “Efficient Iris Recognition by Characterizing Key Local Variations”, IEEE Trans. on Image Processing, Vol. 13, No.6, pp. 739- 750, 2004. L. Ma, T. Tan, D. Zhang and Y. Wang, “Local Intensity Variation Analysis for Iris Recognition, Pattern Recognition”, Vol.37, No.6, pp. 1287-1298, 2004. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, IEEE Transactions on Systems, Man, and Cybernetics-Part Cï¼ŒVolume 35, Issue 3, 2005, pp.435 - 441. Zhenan Sun, Tieniu Tan, Yunhong Wang, “Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition”, Proceedings of International Workshop on Biometric Authentication (BioAW), Lecture Notes in Computer Science, Vol.3087, 2004, pp. 270-282. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 418-425. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Robust Direction Estimation of Gradient Vector Field for Iris Recognition”, Proceedings of the 17th International Conference on Pattern Recognition, Vol.2, 2004, pp.783-786. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Cascading Statistical And Structural Classifiers For Iris Recognition”, Proceedings of IEEE International Conference on Image Processing, 2004, pp.1261-1264. Zhenan Sun, Tieniu Tan, Yunhong Wang, “Iris Recognition Based on Non-local Comparisons”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 67-77. Zhenan Sun, Tieniu Tan, and Xianchao Qiu, \u0026ldquo;Graph Matching Iris Image Blocks with Local Binary Pattern\u0026rdquo;, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 366-372. Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418. Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, “Robust and Fast Assessment of Iris Image Quality”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 464 - 471. Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “An Appearance-Based Method for Iris Detection”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp.1091-1096, 2004, Korea. Jiali Cui, Yunhong Wang, Junzhou Huang, Tieniu Tan, Zhenan Sun and Li Ma, “An Iris Image Synthesis Method Based on PCA and Super-Resolution”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 4, pp. 471-474, 23-26 August 2004, Cambridge, UK. Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “A Fast and Robust Iris Localization Method Based on Texture Segmentation”, Proc. of SPIE, Vol. 5404, pp. 401-408, 2004, USA. Jiali Cui, Yunhong Wang, Li Ma, Tieniu Tan and Zhenan Sun, “An Iris Recognition Algorithm Using Local Extreme Points”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 442-449. Jiali Cui, Yunhong Wang, Tieniu Tan and Zhenan Sun, “Fast Recursive Mathematical Morphological Transforms”, Proc. of the 3rd International Conference on Image and Graphics (ICIG), pp. 422-425, 2004, Hong Kong. Junzhou Huang, Tieniu Tan, Li Ma, and Yunhong Wang, Phase Correlation Based Iris Image Registration Model, Journal of Computer Science and Technology, Vol.20, No.3, pp.419-425, May 2005. L. Ma, Y. Wang and T. Tan, “Iris Recognition Based on Multichannel Gabor Filtering”, Proc. of the 5th Asian Conference on Computer Vision (ACCV), Vol. I, pp.279-283, Jan 22-25, 2002, Melbourne, Australia. L. Ma, Y. Wang and T. Tan, “Iris Recognition Using Circular Symmetric Filters”, Proc. of IAPR International Conference on Pattern Recognitionï¼ˆICPRï¼‰, Vol. II, pp. 414-417, August 11-15, 2002, Quebec, Canada. J. Z. Huang, L. Ma, T. N. Tan and Y. H. Wang, “Learning-Based Enhancement Model of Iris”, Proc. of British Machine Vision Conference (BMVC), pp. 153-162, 2003. J. Z. Huang, L. Ma, and Y. H. Wang and T. N. Tan, “Iris Model Based on Local Orientation Description”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp. 954-959, 2004, Korea. J. Z. Huang, Y. H. Wang, T. N. Tan and J. L. Cui, “A New Iris Segmentation Model”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 3, pp. 554-557, 23-26 August 2004, Cambridge, UK. J. Z. Huang, Y. H. Wang, J. L. Cui and T. N. Tan, “Noise Removal and Impainting Model for Iris Image”, Proc. of IEEE International Conference on Image Processing (ICIP), pp. 869-872, 2004, Singapore. Yuqing He, Yangsheng Wang and Tieniu Tan, “Iris Image Capture System Design For Personal Identification”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 546-552. Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, \u0026ldquo;Robust and Fast Assessment of Iris Image quality\u0026rdquo;, Proc. of International Conference of Biometrics, pp. 464-471, 2006. Zhuoshi Wei, Tieniu Tan and Zhenan Sun, \u0026ldquo;Nonlinear Iris Deformation Correction Based on Gaussian Model\u0026rdquo;, International Conference of Biometrics, pp 780-789, 2007. Zhuoshi Wei, Yufei Han, Zhenan Sun and Tieniu Tan, Palmprint Image Synthesis: A Preliminary Study, Proc. of IEEE International Conference on Image Processing, 2008. Zhuoshi Wei, Tieniu Tan and Zhenan Sun, Synthesis of Large Realistic Iris Databases Using Patch-based Sampling, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008. Zhuoshi Wei, Xianchao Qiu, Zhenan Sun and Tieniu Tan, Counterfeit Iris Detection Based on Texture Analysis, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008. Zhaofeng He, Tieniu Tan and Zhenan Sun, “Iris Localization via Pulling and Pushing”, Proc. of the 18th IEEE International Conference on Pattern Recognition (ICPR'06), Vol.4, pp. 366-369, 2006, Hongkong. Zhaofeng He, Tieniu Tan, Zhenan Sun, Xianchao Qiu, Cheng Zhong and Wenbo Dong, Boosting Ordinal Features for Iris Recognition, Proc. of the 26th IEEE International Conference on Computer Vision and Pattern Recognition (CVPR’08) , pp. 1-8, June 23-28, Alaska, USA Zhaofeng He, Zhenan Sun, Tieniu Tan and Xianchao Qiu, Enhanced Usability of Iris Recognition via Efficient User Interface and Iris Image Restoration, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California Accepted. Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, Robust Eyelid, Eyelash and Shadow Localization for Iris Recognition”, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California, Accepted. Zhaofeng He, Tieniu Tan, Zhenan Sun and Zhuoshi Wei, “Efficient Iris Spoof Detection via Boosted Local Binary Patterns”, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1080-1090, 2009. Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418. Xianchao Qiu, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Coarse Iris Classification by Learned Visual Dictionary\u0026rdquo;, In Proc. of The 2nd International Conference on Biometrics, pp. 770–779, Seoul, Korea, Aug. 2007. Xianchao Qiu, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Global Texture Analysis of Iris Images for Ethnic Classification\u0026rdquo;, In Proc. of The 1st International Conference on Biometrics, pp. 411–418, Hong Kong, China. Jan. 2006. Wenbo Dong, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Self-adaptive iris image acquisition system, Proc. SPIE vol. 6944, 1-9, 2008. Wenbo Dong, Zhenan Sun, Tieniu Tan, How to make iris recognition easier?, Proc. of the 19th International Conference on Pattern Recognition, pp.1-4, 2008. Wenbo Dong, Zhenan Sun, Tieniu Tan, Zhuoshi Wei, \u0026ldquo;Quality-based dynamic threshold for iris matching\u0026rdquo;, In Proceedings of IEEE International Conference on Image Processing, 2009. Long Zhang, Zhenan Sun, Tieniu Tan and Shungeng Hu, \u0026ldquo;Robust Biometric Key Extraction Based on Iris Cryptosystem\u0026rdquo;, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1060-1069, 2009. Hui Zhang, Zhenan Sun, and Tieniu Tan, Contact lens detection based on weighted LBP, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010. Hui Zhang, Zhenan Sun, and Tieniu Tan, Statistics of Local Surface Curvatures for Mis-Localized Iris Detection, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010. Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Texture Removal for Adaptive Level Set based Iris Segmentation\u0026rdquo;, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010. Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Hierarchical Fusion of Face and Iris for Personal Identification\u0026rdquo;, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010. ","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594598400,"objectID":"ae1e247fa12af13a5e4adbf9c5bd5314","permalink":"http://localhost:1313/dataset/casia-irisv4/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/dataset/casia-irisv4/","section":"dataset","summary":"CASIA Iris Image Database Version 4.0","tags":["Internal"],"title":"CASIA-IrisV4","type":"dataset"},{"authors":["Yunlong Wang","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1593698267,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593698267,"objectID":"1b1eef3547be9058f1fe6e4e353e3184","permalink":"http://localhost:1313/publication/wang-icpr-2020/","publishdate":"2020-07-02T21:57:47+08:00","relpermalink":"/publication/wang-icpr-2020/","section":"publication","summary":"The primitive basis of image based material recognition builds upon the fact that discrepancies in the reflectances of distinct materials lead to imaging differences under multiple viewpoints. LF cameras possess coherent abilities to capture multiple sub-aperture views (SAIs) within one exposure, which can provide appropriate multi-view sources for material recognition. In this paper, a unified Factorize-Connect-Merge (FCM) deep-learning pipeline is proposed to solve problems of light field image based material recognition. 4D light-field data as input is initially decomposed into consecutive 3D light-field slices. Shallow CNN is leveraged to extract low-level visual features of each view inside these slices. As to establish correspondences between these SAIs, Bidirectional Long-Short Term Memory (Bi-LSTM) network is built upon these low-level features to model the imaging differences. After feature selection including concatenation and dimension reduction, effective and robust feature representations for material recognition can be extracted from 4D light-field data. Experimental results indicate that the proposed pipeline can obtain remarkable performances on both tasks of single-pixel material classification and whole-image material segmentation. In addition, the proposed pipeline can potentially benefit and inspire other researchers who may also take LF images as input and need to extract 4D light-field representations for computer vision tasks such as object classification, semantic segmentation and edge detection. ","tags":["Light Field Imaging","Material Recognition"],"title":"A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Kunbo Zhang","Zilei Wang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1593697016,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593697016,"objectID":"2109ff2c79baba13b70eeb80dca5a2c1","permalink":"http://localhost:1313/publication/wang-tci-2020/","publishdate":"2020-07-02T21:36:56+08:00","relpermalink":"/publication/wang-tci-2020/","section":"publication","summary":"Multi-view properties of light field (LF) imaging enable exciting applications such as auto-refocusing, depth estimation and 3D reconstruction. However, limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards more practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. We have proposed an end-to-end deep learning framework named Pseudo 4DCNN to solve these problems in a conference paper. Rethinking on the overall paradigm, we further extend pseudo 4DCNN and propose a novel loss function which is applicable for all tasks of light field reconstruction i.e. EPI Structure Preserving (ESP) loss function. This loss function is proposed to attenuate the blurry edges and artifacts caused by averaging effect of L2 norm based loss function. Furthermore, the extended Pseudo 4DCNN is compared with recent state-of-the-art (SOTA) approaches on more publicly available light field databases, as well as self-captured light field biometrics and microscopy datasets. Experimental results demonstrate that the proposed framework can achieve better performances than vanilla Pseudo 4DCNN and other SOTA methods, especially in the terms of visual quality under occlusions. The source codes and self-collected datasets for reproducibility are available online.","tags":["Light Field Imaging","Light Field Reconstruction","View Synthesis"],"title":"High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN","type":"publication"},{"authors":["Caiyong Wang","Yunlong Wang"," Boqiang Xu","Yong He"," Zhiwei Dong","Zhenan Sun"],"categories":[],"content":"","date":1586419925,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586419925,"objectID":"21e4c84fe092e44a692914e74d0e34a0","permalink":"http://localhost:1313/publication/wang-icassp-2020/","publishdate":"2020-04-09T08:58:18Z","relpermalink":"/publication/wang-icassp-2020/","section":"publication","summary":"This paper proposes a novel, lightweight deep convolutional neural network specifically designed for iris segmentation of noisy images acquired by mobile devices. Unlike previous studies, which only focused on improving the accuracy of segmentation mask using the popular CNN technology, our method is a complete end-to-end iris segmentation solution, i.e., segmentation mask and parameterized pupillary and limbic boundaries of the iris are obtained simultaneously, which further enables CNN-based iris segmentation to be applied in any regular iris recognition systems. By introducing an intermediate pictorial boundary representation, predictions of iris boundaries and segmentation mask have collectively formed a multi-label semantic segmentation problem, which could be well solved by a carefully adapted stacked hourglass network. Experimental results show that our method achieves competitive or state-of-the-art performance in both iris segmentation and localization on two challenging mobile iris databases.","tags":["smart iris recognition"],"title":"A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics","type":"publication"},{"authors":["Yu Tian","Kunbo Zhang","Leyuan Wang","Zhenan Sun"],"categories":[],"content":"","date":1584519125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584519125,"objectID":"6e551f4f8861318951bf314414454e98","permalink":"http://localhost:1313/publication/kunbo-cvpr-2020/","publishdate":"2020-03-18T08:58:18Z","relpermalink":"/publication/kunbo-cvpr-2020/","section":"publication","summary":"Face anti-spoofing is the key to preventing security breaches in biometric recognition applications. Existing software-based and hardwarebased face liveness detection methods are effective in constrained environments or designated datasets only. Deep learning method using RGB and infrared images demands a large amount of training data for new attacks. In this paper, we present a face anti-spoofing method in a realworld scenario by automatic learning the physical characteristics in polarization images of a real face compared to a deceptive attack.","tags":null,"title":"Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario","type":"publication"},{"authors":["Caiyong Wang","Jawad Muhammad","Yunlong Wang","Zhaofeng He","Zhenan Sun"],"categories":[],"content":"","date":1584346325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584346325,"objectID":"3ee5c89bd1268dae7be9e41cf47407ee","permalink":"http://localhost:1313/publication/wang-tifs-2020/","publishdate":"2020-03-16T08:58:18Z","relpermalink":"/publication/wang-tifs-2020/","section":"publication","summary":"Iris images captured in non-cooperative environments often suffer from adverse noise, which challenges many existing iris segmentation methods. To address this problem, this paper proposes a high-efficiency deep learning based iris segmentation approach, named IrisParseNet. Different from many previous CNN-based iris segmentation methods, which only focus on predicting accurate iris masks by following popular semantic segmentation frameworks, the proposed approach is a complete iris segmentation solution, i.e., iris mask and parameterized inner and outer iris boundaries are jointly achieved by actively modeling them into a unified multi-task network. Moreover, an elaborately designed attention module is incorporated into it to improve the segmentation performance. To train and evaluate the proposed approach, we manually label three representative and challenging iris databases, i.e., CASIA.v4-distance, UBIRIS.v2, and MICHE-I, which involve multiple illumination (NIR, VIS) and imaging sensors (long-range and mobile iris cameras), along with various types of noises. Additionally, several unified evaluation protocols are built for fair comparisons. Extensive experiments are conducted on these newly annotated databases, and results show that the proposed approach achieves state-of-the-art performance on various benchmarks. Further, as a general drop-in replacement, the proposed iris segmentation method can be used for any iris recognition methodology, and would significantly improve the performance of non-cooperative iris recognition.","tags":["Iris Segmentation"],"title":"Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition","type":"publication"},{"authors":["Caiyong Wang","Jawad Muhammad","Yunlong Wang","Zhaofeng He","Zhenan Sun"],"categories":[],"content":"","date":1584346325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584346325,"objectID":"c5d4bbf4be99f5e8c3056bbef6628e77","permalink":"http://localhost:1313/publication/wang-tifs-2021/","publishdate":"2020-03-16T08:58:18Z","relpermalink":"/publication/wang-tifs-2021/","section":"publication","summary":"Iris images captured in non-cooperative environments often suffer from adverse noise, which challenges many existing iris segmentation methods. To address this problem, this paper proposes a high-efficiency deep learning based iris segmentation approach, named IrisParseNet. Different from many previous CNN-based iris segmentation methods, which only focus on predicting accurate iris masks by following popular semantic segmentation frameworks, the proposed approach is a complete iris segmentation solution, i.e., iris mask and parameterized inner and outer iris boundaries are jointly achieved by actively modeling them into a unified multi-task network. Moreover, an elaborately designed attention module is incorporated into it to improve the segmentation performance. To train and evaluate the proposed approach, we manually label three representative and challenging iris databases, i.e., CASIA.v4-distance, UBIRIS.v2, and MICHE-I, which involve multiple illumination (NIR, VIS) and imaging sensors (long-range and mobile iris cameras), along with various types of noises. Additionally, several unified evaluation protocols are built for fair comparisons. Extensive experiments are conducted on these newly annotated databases, and results show that the proposed approach achieves state-of-the-art performance on various benchmarks. Further, as a general drop-in replacement, the proposed iris segmentation method can be used for any iris recognition methodology, and would significantly improve the performance of non-cooperative iris recognition.","tags":["Iris Segmentation"],"title":"Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition","type":"publication"},{"authors":["Ping Song","Ling Huang","Yunlong Wang","Fei Liu","Zhenan Sun"],"categories":[],"content":"","date":1581931385,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581931385,"objectID":"14bfbbd5a40c018816c9e48cda46b1b6","permalink":"http://localhost:1313/publication/song-automatica2020/","publishdate":"2020-02-17T17:23:05+08:00","relpermalink":"/publication/song-automatica2020/","section":"publication","summary":"Light-field (LF) imaging is a new method to capture both intensity and direction information of visual objects, providing promising solutions to biometrics. Iris recognition is a reliable personal identification method, however it is also vulnerable to spoofing attacks, such as iris patterns printed on contact lens or paper. Therefore iris liveness detection is an important module in iris recognition systems. In this paper, an iris liveness detection approach is proposed to take full advantages of intrinsic characteristics in light-field iris imaging. LF iris images are captured by using lab-made LF cameras, based on which the geometric features as well as the texture features are extracted using the LF digital refocusing technology. These features are combined for genuine and fake iris image classification. Experiments were carried out based on the self-collected near-infrared LF iris database, and the average classification error rate (ACER) of the proposed method is 3.69%, which is 5.94% lower than the best state-of-the-art method. Experimental results indicate the proposed method is able to work effectively and accurately to prevent spoofing attacks such as printed and screen-displayed iris input attacks.","tags":["Smart Iris Recognition","Light Field Imaging","Liveliness Detection"],"title":"Iris Liveness Detection Based on Light Field Imaging","type":"publication"},{"authors":["Kunbo Zhang","Zhenteng Shen","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1581927125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581927125,"objectID":"31a28568fc82801d805f164fe4fa795c","permalink":"http://localhost:1313/publication/zhang-ijcb2020/","publishdate":"2020-09-01T08:58:18Z","relpermalink":"/publication/zhang-ijcb2020/","section":"publication","summary":"Imaging volume of an iris recognition system has been restricting the throughput and cooperation convenience in biometric applications. Numerous improvement trials are still impractical to supersede the dominant fixed-focus lens in stand-off iris recognition due to incremental performance increase and complicated optical design. In this study, we develop a novel all-in-focus iris imaging system using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth of field extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed. In addition, the motorized reflection mirror adaptively steers the light beam to extend the horizontal and vertical field of views in an active manner. The proposed all-in-focus iris camera increases the depth of field up to 3.9 m which is a factor of 37.5 compared with conventional long focal lens. We also experimentally demonstrate the capability of this 3D light beam steering imaging system in real-time multi-person iris refocusing using dynamic focal stacks and the potential of continuous iris recognition for moving participants.","tags":["smart iris recognition"],"title":"All-in-Focus Iris Camera With a Great Capture Volume","type":"publication"},{"authors":["Leyuan Wang","Kunbo Zhang","Min Ren","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1581927125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581927125,"objectID":"c48de3584d9d93dae4ddf85389c46a17","permalink":"http://localhost:1313/publication/wang-ijcb-2020/","publishdate":"2020-09-01T08:58:18Z","relpermalink":"/publication/wang-ijcb-2020/","section":"publication","summary":"A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition. ","tags":["smart iris recognition","Image Quality Assessment (IQA)"],"title":"Recognition Oriented Iris Image Quality Assessment in the Feature Space","type":"publication"},{"authors":["Fei Liu","Shubo Zhou","Yunlong Wang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3c85d24c9d3f90d582905c2dad27d460","permalink":"http://localhost:1313/publication/liu-tip-2020/","publishdate":"2020-02-16T11:44:49.291517Z","relpermalink":"/publication/liu-tip-2020/","section":"publication","summary":"Binocular stereo vision (SV) has been widely used to reconstruct the depth information, but it is quite vulnerable to scenes with strong occlusions. As an emerging computational photography technology, light-field (LF) imaging brings about a novel solution to passive depth perception by recording multiple angular views in a single exposure. In this paper, we explore binocular SV and LF imaging to form the binocular-LF imaging system. An imaging theory is derived by modeling the imaging process and analyzing disparity properties based on the geometrical optics theory. Then an accurate occlusion-robust depth estimation algorithm is proposed by exploiting multibaseline stereo matching cues and defocus cues. The occlusions caused by binocular SV and LF imaging are detected and handled to eliminate the matching ambiguities and outliers. Finally, we develop a binocular-LF database and capture realworld scenes by our binocular-LF system to test the accuracy and robustness. The experimental results demonstrate that the proposed algorithm definitely recovers high quality depth maps with smooth surfaces and precise geometric shapes, which tackles the drawbacks of binocular SV and LF imaging simultaneously.","tags":["Light Field Imaging","Binocular Light-Field","Imaging Theory","Occlusion-Robust Depth Perception Application","Depth Estimation"],"title":"Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application","type":"publication"},{"authors":["Min Ren","Yunlong Wang","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"42391abf460f06da4099f460e39f9575","permalink":"http://localhost:1313/publication/ren-2020-dynamic/","publishdate":"2020-02-16T11:44:49.304474Z","relpermalink":"/publication/ren-2020-dynamic/","section":"publication","summary":"The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.","tags":null,"title":"Dynamic Graph Representation for Occlusion Handling in Biometrics","type":"publication"},{"authors":["Caiyong Wang","Yunlong Wang","Yunfan Liu","Zhaofeng He","Ran He","Zhenan Sun"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f4f5b8312490d53a2b9be623c9456692","permalink":"http://localhost:1313/publication/wang-tbiom-2020/","publishdate":"2020-02-16T11:44:49.293511Z","relpermalink":"/publication/wang-tbiom-2020/","section":"publication","summary":"Accurate sclera segmentation is critical for successful sclera recognition. However, studies on sclera segmentation algorithms are still limited in the literature. In this paper, we propose a novel sclera segmentation method based on the improved U-Net model, named as ScleraSegNet. We perform in-depth analysis regarding the structure of U-Net model, and propose to embed an attention module into the central bottleneck part between the contracting path and the expansive path of U-Net to strengthen the ability of learning discriminative representations. We compare different attention modules and find that channel-wise attention is the most effective in improving the performance of the segmentation network. Besides, we evaluate the effectiveness of data augmentation process in improving the generalization ability of the segmentation network. Experiment results show that the best performing configuration of the proposed method achieves state-of-the-art performance with F-measure values of 91.43%, 89.54% on UBIRIS.v2 and MICHE, respectively.","tags":["Smart Iris Recognition","Sclera segmentation","sclera recognition","U-net","attention mechanism","SSBC"],"title":"ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation","type":"publication"},{"authors":["宋平","黄玲","王云龙","刘菲","孙哲南"],"categories":[],"content":"","date":1567325525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567325525,"objectID":"0b940d91f89e309d2eeda12b98ae9b66","permalink":"http://localhost:1313/publication/songping-aas-2019/","publishdate":"2019-09-01T08:58:18Z","relpermalink":"/publication/songping-aas-2019/","section":"publication","summary":"光场成像相对传统光学成像是一次重大技术革新,高维光场信息为生物特征识别的发展与创新带来了新机遇.虹膜身份识别技术以其唯一性、稳定性、高精度等优势广泛应用于国防、教育、金融等各个领域,但是现有的虹膜识别系统容易被人造假体虹膜样本欺骗导致误识别.因此,虹膜活体检测是当前虹膜识别研究亟待解决的关键问题.本文提出一种基于计算光场成像的虹膜活体检测方法,通过软硬件结合的方式,充分挖掘四维光场数据的信息.本方法使用实验室自主研发的光场相机采集光场虹膜图像,利用光场数字重对焦技术提取眼周区域的立体结构特征和虹膜图像的纹理特征,进行特征融合与虹膜分类.在自主采集的近红外光场虹膜活体检测数据库上进行实验,本方法的平均分类错误率(Average classification error rate,ACER)为3.69%,在现有最佳方法的基础上降低5.94%.实验结果表明本方法可以准确有效地检测并阻止打印虹膜和屏显虹膜对系统的攻击.","tags":["iris super-resolution","smart iris recognition"],"title":"基于计算光场成像的虹膜活体检测方法","type":"publication"},{"authors":["Junxing Hu","Hui Zhang","Lihu Xiao","Jing Liu","Xingguang Li","Zhaofeng He"],"categories":["Biometrics","Computer Vision"],"content":"","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"3427756f1cbf3d871c8ee7196e18c40a","permalink":"http://localhost:1313/publication/hu-2019-icb/","publishdate":"2020-02-16T11:44:49.3025Z","relpermalink":"/publication/hu-2019-icb/","section":"publication","summary":"In this paper, we present an end-to-end model, namely Seg-Edge bilateral constraint network. The iris edge map generated from rich convolutional layers optimize the iris segmentation by aligning it with the iris boundary. The iris region produced by the coarse segmentation limits the scope. It makes the edge filtering pay more attention to the interesting target. We compress the model while keeping the performance levels almost intact and even better by using l1-norm. The proposed model advances the state-of-the-art iris segmentation accuracies.","tags":["Iris Segmentation","Bilateral Constrained Domain Transform","Model Pruning"],"title":"Seg-Edge Bilateral Constraint Network for Iris Segmentation","type":"publication"},{"authors":["Min Ren","Caiyong Wang","Yunlong Wang","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7ed96a89a31d1e4cf5ae2812d6a8560b","permalink":"http://localhost:1313/publication/ren-2019-alignment/","publishdate":"2020-02-16T11:44:49.3025Z","relpermalink":"/publication/ren-2019-alignment/","section":"publication","summary":"Iris recognition is a reliable personal identification method but there is still much room to improve its accuracy especially in less-constrained situations. For example, free movement of head pose may cause large rotation difference between iris images. And illumination variations may cause irregular distortion of iris texture. To match intra-class iris images with head rotation robustly, the existing soadminlutions usually need a precise alignment operation by exhaustive search within a determined range in iris image preprosessing or brute-force searching the minimum Hamming distance in iris feature matching. In the wild enviroments, iris rotation is of much greater uncertainty than that in constrained situations and exhaustive search within a determined range is impracticable. This paper presents a unified feature-level solution to both alignment free and distortion robust iris recognition in the wild. A new deep learning based method named Alignment Free Iris Network (AFINet) is  proposed, which utilizes a trainable VLAD (Vector of Locally Aggregated Descriptors) encoder called NetVLAD [18] to decouple the correlations between local representations and their spatial positions. And deformable convolution [5] is leveraged to overcome iris texture distortion by dense adaptive sampling. The results of extensive experiments on three public iris image databases and the simulated degradation databases show that AFINet significantly outperforms state-of-art iris recognition methods.","tags":null,"title":"Alignment Free and Distortion Robust Iris Recognition","type":"publication"},{"authors":["Jianze Wei","Yunlong Wang","Xiang Wu","Zhaofeng  He","Ran He","Zhenan Sun"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4ef9c0699a3577c43bd7614ef612239c","permalink":"http://localhost:1313/publication/csin-btas-2019/","publishdate":"2020-02-16T11:44:49.305472Z","relpermalink":"/publication/csin-btas-2019/","section":"publication","summary":"Due to the growing demand of iris biometrics, lots of new sensors are being developed for high-quality image acquisition. However, upgrading the sensor and re-enrolling for users is expensive and time-consuming. This leads to a dilemma where enrolling on one type of sensor but recognizing on the others. For this cross-sensor matching, the large gap between distributions of enrolling and recognizing images usually results in degradation in recognition performance. To alleviate this degradation, we propose Cross-sensor iris network (CSIN) by applying the adversarial strategy and weakening interference of sensor-specific information. Specifically, there are three valuable efforts towards learning discriminative iris features. Firstly, the proposed CSIN adds extra feature extractors to generate residual components containing sensor-specific information and then utilizes these components to narrow the distribution gap. Secondly, an adversarial strategy is borrowed from Generative Adversarial Networks to align feature distributions and further reduce the discrepancy of images caused by sensors. Finally, we extend triplet loss and propose instance-anchor loss to pull the instances of the same class together and push away from others. It is worth mentioning that the proposed method doesn’t need pair-same data or triplet, which reduced the cost of data preparation. Experiments on two real-world datasets validate the effectiveness of the proposed method in cross-sensor iris recognition.","tags":null,"title":"Cross-sensor iris recognition using adversarial strategy and sensor-specific information","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Kunbo Zhang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"eb4d0743b49b915838e1c049f782c32d","permalink":"http://localhost:1313/publication/wang-tip-2018/","publishdate":"2020-02-16T11:44:49.308464Z","relpermalink":"/publication/wang-tip-2018/","section":"publication","summary":"The low spatial resolution of light-field image poses significant difficulties in exploiting its advantage. To mitigate the dependency of accurate depth or disparity information as priors for light-field image super-resolution, we propose an implicitly multi-scale fusion scheme to accumulate contextual information from multiple scales for super-resolution reconstruction. The implicitly multi-scale fusion scheme is then incorporated into bidirectional recurrent convolutional neural network, which aims to iteratively model spatial relations between horizontally or vertically adjacent sub-aperture images of light-field data. Within the network, the recurrent convolutions are modified to be more effective and flexible in modeling the spatial correlations between neighboring views. A horizontal sub-network and a vertical sub-network of the same network structure are ensembled for final outputs via stacked generalization. Experimental results on synthetic and real-world data sets demonstrate that the proposed method outperforms other state-of-the-art methods by a large margin in peak signal-to-noise ratio and gray-scale structural similarity indexes, which also achieves superior quality for human visual systems. Furthermore, the proposed method can enhance the performance of light field applications such as depth estimation.","tags":["Light Field Imaging","Deep Learning","Light-Field Image Super Resolution","LFNet","Bidirectional Recurrent Convolutional Neural Network"],"title":"LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution","type":"publication"},{"authors":["Zihui Yan","Lingxiao He","Man Zhang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1516406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516406400,"objectID":"e0620a1e63047a625c9029c0a432199b","permalink":"http://localhost:1313/publication/yan-2018-liveness/","publishdate":"2018-05-20T00:00:00Z","relpermalink":"/publication/yan-2018-liveness/","section":"publication","summary":"In modern society, iris recognition has become increasingly popular. The security risk of iris recognition is increasing rapidly because of the attack by various patterns of fake iris. A German hacker organization called Chaos Computer Club cracked the iris recognition system of Samsung Galaxy S8 recently. In view of these risks, iris liveness detection has shown its significant importance to iris recognition systems. The state-of-the-art algorithms mainly rely on hand-crafted texture features which can only identify fake iris images with single pattern. In this paper, we proposed a Hierarchical Multiclass Iris Classification (HMC) for liveness detection based on CNN. HMC mainly focuses on iris liveness detection of multipattern fake iris. The proposed method learns the features of different fake iris patterns by CNN and classifies the genuine or fake iris images by hierarchical multi-class classification. This classification takes various characteristics of different fake iris patterns into account. All kinds of fake iris patterns are divided into two categories by their fake areas. The process is designed as two steps to identify two categories of fake iris images respectively. Experimental results demonstrate an extremely higher accuracy of iris liveness detection than other state-of-the-art algorithms. The proposed HMC remarkably achieves the best results with nearly 100% accuracy on NDContact, CASIA-Iris-Interval, CASIA-Iris-Syn and LivDetIris-2017-Warsaw datasets. The method also achieves the best results with 100% accuracy on a hybrid dataset which consists of ND-Contact and LivDet-Iris-2017-Warsaw dataset","tags":["Iris-liveness-detection"],"title":"Hierarchical Multi-class Iris Classification for Liveness Detection","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Zilei Wang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7ea1d1915c9430ad95008a9a2d7ba623","permalink":"http://localhost:1313/publication/wang-eccv-2018/","publishdate":"2020-02-16T11:44:49.306438Z","relpermalink":"/publication/wang-eccv-2018/","section":"publication","summary":"Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2 dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.","tags":["Computational Photography","Deep Learning","Light Field Reconstruction","End-to-end View Synthesis","Pseudo 4DCNN"],"title":"End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN","type":"publication"},{"authors":["Min Ren","Lingxiao He","Haiqing Li","Yunfan Liu","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1500508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500508800,"objectID":"d536e7e8dd5e6fb4e646edaaaa38a4dd","permalink":"http://localhost:1313/publication/ren-ccbr-2017/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/publication/ren-ccbr-2017/","section":"publication","summary":"In this paper, we study the problem of partial person reidentification (re-id). This problem is more difficult than general person re-identification because the body in probe image is not full. We propose a novel method, similarity-guided sparse representation (SG-SR), as a robust solution to improve the discrimination of the sparse coding. There are three main components in our method. In order to include multi-scale information, a dictionary consisting of features extracted from multiscale patches is established in the first stage. A low rank constraint is then enforced on the dictionary based on the observation that its subspaces of each class should have low dimensions. After that, a classification model is built based on a novel similarity-guided sparse representation which can choose vectors that are more similar to the probe feature vector. The results show that our method outperforms existing partial person re-identification methods significantly and achieves state-of-theart accuracy.","tags":[""],"title":"Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation","type":"publication"},{"authors":null,"categories":null,"content":" \u0026emsp;\u0026emsp;2017年4月27日，微软获得了一项虹膜识别技术的专利，未来该技术将纳入 Windows Hello ，用于微软旗下的智能手机、笔记本等设备中。\r\u0026emsp;\u0026emsp;虹膜识别是生物识别技术中的一种。其他的生物识别方法包括人脸、指纹、声音、视网膜、静脉识别等，而由于人类虹膜上拥有266个特征点，远高于其他生物识别技术的不到60个特征点，故被认为具有更高的精准性和安全性。\r虹膜识别是通过数学算法对人眼虹膜特征进行编码和对比的身份识别方法。根据专利文件描述，微软的智能设备可以从两个或者三个方向照明中拍摄用户眼睛的多张照片。每个角度的眼睛照片都能检测虹膜特征并创建不同的数据点。\n\u0026emsp;\u0026emsp;微软在其专利申请中指出，人眼是部分透明的三维结构。当光通过瞳孔传递到眼睛的视网膜上。从不同的方向用光照射眼睛，就可以获得许多图像帧的图像数据，并对至少两个图像帧的数据进行对比，找到相似的地方，获得相关的数据，这些数据与关注的眼睛区域是一致的。然后系统根据数据自动确定假眼睛的验证特点，从而用来验证真正的眼睛。\r","date":1492473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492473600,"objectID":"980c4d848fd82695ddc84c8f35ba3d09","permalink":"http://localhost:1313/post/iris-recognition-is-the-general-trend/","publishdate":"2017-04-18T00:00:00Z","relpermalink":"/post/iris-recognition-is-the-general-trend/","section":"post","summary":"Microsoft has applied for relevant patents.","tags":["Iris Recognition"],"title":"Iris recognition is the general trend?","type":"post"},{"authors":["Yunlong Wang","Guangqi Hou","Zhenan Sun","Zilei Wang","Tieniu Tan"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"2928bf922f6dfea0b2816c5dda87461e","permalink":"http://localhost:1313/publication/wang-icip-2016/","publishdate":"2020-02-16T11:44:49.30946Z","relpermalink":"/publication/wang-icip-2016/","section":"publication","summary":"","tags":["Light Field Image Processing","Super Resolution"],"title":"A simple and robust super resolution method for light field images","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2ad8a763bd2d0252e637ae3d73d00357","permalink":"http://localhost:1313/benchmark/iris-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/iris-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Iris Recognition"],"title":"Iris Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"800539ac7a5d691dc12638e217d01344","permalink":"http://localhost:1313/benchmark/light-field-photography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/light-field-photography/","section":"benchmark","summary":"An example of using the in-built project page.","tags":["Internal"],"title":"Light Field Photography","type":"benchmark"},{"authors":null,"categories":null,"content":"Light field The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled \u0026ldquo;Thoughts on Ray Vibrations\u0026rdquo;) that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase light field was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).\nThe 4D light field In a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.\nHowever, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the photic field (1981), while researchers in computer graphics call it the 4D light field (Levoy 1996) or Lumigraph (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.\nThe set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the st plane (and any objects that may lie astride or beyond it), each taken from an observer position on the uv plane. A light field parameterized this way is sometimes called a light slab.\nWays to create light fields Light fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.\nIn computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).\nDevices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in The Matrix), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).\nHow many images should be in a light field? The largest known light field (of Michelangelo\u0026rsquo;s statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the st plane, then you need images taken at finely spaced positions on the uv plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.\nThe number and arrangement of images in a light field, and the resolution of each image, are together called the \u0026ldquo;sampling\u0026rdquo; of the 4D light field. Analyses of light field sampling have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"f3939b65ca5c42e2fb3f6da617eea401","permalink":"http://localhost:1313/project/light-field-photography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/light-field-photography/","section":"project","summary":"Light field is a parameterized representation of four-dimensional radiation field which contains both position and direction information in space","tags":["Light Field Photography"],"title":"Light Field Photography","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ca512dce072f572757e9d9864616a179","permalink":"http://localhost:1313/benchmark/periocular-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/periocular-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Ocular Recognition"],"title":"Periocular Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"To be updated.....","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ac06c4cd588437d58836c7231f598f37","permalink":"http://localhost:1313/project/periocular-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/periocular-recognition/","section":"project","summary":"Face recognition derivative, periocular recognition","tags":["Ocular Recognition"],"title":"Periocular Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"51f301e55709d9e7c1f85ba49df9fc22","permalink":"http://localhost:1313/benchmark/sclera-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/sclera-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Ocular Recognition"],"title":"Sclera Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"To be updated.....","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b6c4b4b975d4de2d1e4d92dadf9663ae","permalink":"http://localhost:1313/project/sclera-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/sclera-recognition/","section":"project","summary":"Another ocular biometric after iris biometrics","tags":["Ocular Recognition"],"title":"Sclera Recognition","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"3abe601028448bda026fbb372863f404","permalink":"http://localhost:1313/benchmark/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/internal-project/","section":"benchmark","summary":"An example of using the in-built project page.","tags":["Demo"],"title":"The Other Internal Project","type":"benchmark"},{"authors":["Guangqi Hou","Chi Zhang","Yunlong Wang","Zhenan Sun"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"001f7d5877030a5fcb34529429dfd9cb","permalink":"http://localhost:1313/publication/hou-spie-2016/","publishdate":"2020-02-16T11:44:49.311463Z","relpermalink":"/publication/hou-spie-2016/","section":"publication","summary":"Counting the number of people is still an important task in social security applications, and a few methods based on video surveillance have been proposed in recent years. In this paper, we design a novel optical sensing system to directly acquire the depth map of the scene from one light-field camera. The light-field sensing system can count the number of people crossing the passageway, and record the direction and intensity of rays at a snapshot without any assistant light devices. Depth maps are extracted from the raw light-ray sensing data. Our smart sensing system is equipped with a passive imaging sensor, which is able to naturally discern the depth difference between the head and shoulders for each person. Then a human model is built. Through detecting the human model from light-field images, the number of people passing the scene can be counted rapidly. We verify the feasibility of the sensing system as well as the accuracy by capturing real-world scenes passing single and multiple people under natural illumination.","tags":["Light field photography","RGB-D sensor","People Counting"],"title":"4D light-field sensing system for people counting","type":"publication"},{"authors":["秦琳琳","陆林箭","石春","吴刚","王云龙"],"categories":[],"content":"","date":1425197525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425197525,"objectID":"64d1131786308cd55367adf1d46c4ae2","permalink":"http://localhost:1313/publication/qinlinlin-ny-2015/","publishdate":"2015-03-01T16:12:05+08:00","relpermalink":"/publication/qinlinlin-ny-2015/","section":"publication","summary":"根据现代温室监控与管理需求，基于物联网技术框架，设计并实现了一种基于物联网的温室智能监控系统。系统由现场监控子系统、远程监控子系统和数据库3部分组成。采用基于分布式CAN总线的硬件系统实现环境数据的实时采集与设备控制，将分布图法应用于采集系统离异数据的在线检测。为了提高远程监控子系统的响应速度与交互性，采用了基于异步JavaScript和XML技术（Ajax）的Web数据交互方式。结合温室环境调控的特点，将基于混杂自动机模型的温室温度系统智能控制算法应用于实际系统，实现了温室环境的自动调控。为保证设备控制的安全性，采用轮询法实现了现场监控子系统和远程监控子系统中设备状态的同步，并将基于Zernike矩的图像识别技术应用于双向型设备的状态检测，实现设备的自动校准。试验表明系统数据传输稳定，环境调控可靠，满足现代温室智能监控的需求。","tags":null,"title":"基于物联网的温室智能监控系统设计","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://localhost:1313/slides/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"","tags":null,"title":"","type":"slides"}]