<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Smart Identity Recognition</title>
    <link>https://nlpr-sir.github.io/</link>
      <atom:link href="https://nlpr-sir.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Smart Identity Recognition</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 08 Jul 2024 16:12:05 +0800</lastBuildDate>
    <image>
      <url>https://nlpr-sir.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_2.png</url>
      <title>Smart Identity Recognition</title>
      <link>https://nlpr-sir.github.io/</link>
    </image>
    
    <item>
      <title>Understanding Deep Face Representation via Attribute Recovery</title>
      <link>https://nlpr-sir.github.io/publication/ren-tifs-2024/</link>
      <pubDate>Mon, 08 Jul 2024 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-tifs-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to IEEE Transactions on Information Forensics and Security ( Early Access )</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2407/</link>
      <pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2407/</guid>
      <description>&lt;h2 id=&#34;1-understanding-deep-face-representation-via-attribute-recovery&#34;&gt;1. Understanding Deep Face Representation via Attribute Recovery&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Min Ren, Yuhao Zhu, Yunlong Wang, Yongzhen Huang, Zhenan Sun. Understanding Deep Face Representation via Attribute Recovery. IEEE Transactions on Information Forensics and Security ( Early Access )&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10587012&#34;&gt;https://ieeexplore.ieee.org/document/10587012&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;tifs-2024-pic1.png&#34; alt=&#34;tifs-2024&#34;&gt;&lt;/p&gt;
&lt;p&gt;Deep neural networks have proven to be highly effective in the face recognition task, as they can map raw samples into a discriminative high-dimensional representation space. However, understanding this complex space proves to be challenging for human observers. In this paper, we propose a novel approach that interprets deep face recognition models via facial attributes. To achieve this, we introduce a two-stage framework that recovers attributes from the deep face representations. This framework allows us to quantitatively measure the significance of facial attributes in relation to the recognition model. Moreover, this framework enables us to generate sample-specific explanations through counterfactual methodology. These explanations are not only understandable but also quantitative. Through the proposed approach, we are able to acquire a deeper understanding of how the recognition model conceptualizes the notion of “identity” and understand the reasons behind the error decisions made by the deep models. By utilizing attributes as an interpretable interface, the proposed method marks a paradigm shift in our comprehension of deep face recognition models. It allows a complex model, obtained through gradient backpropagation, to effectively “communicate” with humans. The source code is available here, or you can visit this website: &lt;a href=&#34;https://github.com/RenMin1991/Facial-Attribute-Recovery&#34;&gt;https://github.com/RenMin1991/Facial-Attribute-Recovery&lt;/a&gt; .&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artificial Immune System of Secure Face Recognition Against Adversarial Attacks</title>
      <link>https://nlpr-sir.github.io/publication/ren-ijcv-2024/</link>
      <pubDate>Wed, 26 Jun 2024 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-ijcv-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to International Journal of Computer Vision (IJCV 2024)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2403/</link>
      <pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2403/</guid>
      <description>&lt;h2 id=&#34;1-artificial-immune-system-of-secure-face-recognition-against-adversarial-attacks&#34;&gt;1. Artificial Immune System of Secure Face Recognition Against Adversarial Attacks&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Min Ren, Yunlong Wang, Yuhao Zhu, Yongzhen Huang, Zhenan Sun, Qi Li, Tieniu Tan. Artificial Immune System of Secure Face Recognition Against Adversarial Attacks. International Journal of Computer Vision&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.18144&#34;&gt;https://arxiv.org/pdf/2406.18144&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;ijcv-2024-pic1.png&#34; alt=&#34;ijcv-2024&#34;&gt;&lt;/p&gt;
&lt;p&gt;Deep learning-based face recognition models are vulnerable to adversarial attacks.In contrast to general noises, the presence of imperceptible adversarial noises can lead to catastrophic errors in deep face recognition models. The primary difference between adversarial noise and general noise lies in its specificity. Adversarial attack methods give rise to noises tailored to the characteristics of the individual image and recognition model at hand. Diverse samples and recognition models can engender specific adversarial noise patterns, which pose significant challenges for adversarial defense. Addressing this challenge in the realm of face recognition presents a more formidable endeavor due to the inherent nature of face recognition as an open set task. In order to tackle this challenge, it is imperative to employ customized processing for each individual input sample. Drawing inspiration from the biological immune system, which can identify and respond to various threats, this paper aims to create an artificial immune system (AIS) to provide adversarial defense for face recognition. The proposed defense model incorporates the principles of antibody cloning, mutation, selection, and memory mechanisms to generate a distinct “antibody” for each input sample, where in the term “antibody” refers to a specialized noise removal manner. Furthermore,we introduce a self-supervised adversarial training mechanism that serves as a simulated rehearsal of immune system invasions. Extensive experimental results demonstrate the efficacy of the proposed method, surpassing state-of-the-art adversarial defense methods. The source code is available here, or you can visit this website: &lt;a href=&#34;https://github.com/RenMin1991/SIDE&#34;&gt;https://github.com/RenMin1991/SIDE&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sclera-TransFuse: Fusing Vision Transformer and CNN for Accurate Sclera Segmentation and Recognition</title>
      <link>https://nlpr-sir.github.io/publication/wang-tbiom-2024-1/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tbiom-2024-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation</title>
      <link>https://nlpr-sir.github.io/publication/wei-tifs-2024_1/</link>
      <pubDate>Fri, 14 Jun 2024 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wei-tifs-2024_1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two paper were accepted to Mahcine Intelligence Research (MIR) and another one was accpeted to TIFS 2024</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-24/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-24/</guid>
      <description>&lt;h2 id=&#34;1-boosting-multi-modal-ocular-recognition-via-spatial-feature-reconstruction-and-unsupervised-image-quality-estimation&#34;&gt;1. Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Zihui Yan, Lingxiao He, Yunlong Wang, Kunbo Zhang, Zhenan Sun, Tieniu Tan. &amp;ldquo;Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation&amp;rdquo;. Machine Intelligence Research (Volume: 21).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/s11633-023-1415-y&#34;&gt;https://doi.org/10.1007/s11633-023-1415-y&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;MIR-2024-01-pic1.png&#34; alt=&#34;MIR-2024-01&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the daily application of an iris-recognition-at-a-distance (IAAD) system, many ocular images of low quality are acquired. As the iris part of these images is often not qualified for the recognition requirements, the more accessible periocular regions are a good complement for recognition. To further boost the performance of IAAD systems, a novel end-to-end framework for multi-modal ocular recognition is proposed. The proposed framework mainly consists of iris/periocular feature extraction and matching, unsupervised iris quality assessment, and a score-level adaptive weighted fusion strategy. First, ocular feature reconstruction (OFR) is proposed to sparsely reconstruct each probe image by high-quality gallery images based on proper feature maps. Next, a brand new unsupervised iris quality assessment method based on random multiscale embedding robustness is proposed. Different from the existing iris quality assessment methods, the quality of an iris image is measured by its robustness in the embedding space. At last, the fusion strategy exploits the iris quality score as the fusion weight to coalesce the complementary information from the iris and periocular regions. Extensive experimental results on ocular datasets prove that the proposed method is obviously better than unimodal biometrics, and the fusion strategy can significantly improve the recognition performance.&lt;/p&gt;
&lt;h2 id=&#34;2-casia-iris-africa-a-large-scale-african-iris-image-database&#34;&gt;2. CASIA-Iris-Africa: A Large-scale African Iris Image Database&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Jawad Muhammad, Yunlong Wang, Junxing Hu, Kunbo Zhang, Zhenan Sun. &amp;ldquo;CASIA-Iris-Africa: A Large-scale African Iris Image Database&amp;rdquo;. Machine Intelligence Research (Volume: 21).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/s11633-022-1402-8&#34;&gt;https://doi.org/10.1007/s11633-022-1402-8&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;MIR-2024-01-pic2.png&#34; alt=&#34;MIR-2024-01&#34;&gt;&lt;/p&gt;
&lt;p&gt;Iris biometrics is a phenotypic biometric trait that has proven to be agnostic to human natural physiological changes. Research on iris biometrics has progressed tremendously, partly due to publicly available iris databases. Various databases have been available to researchers that address pressing iris biometric challenges such as constraint, mobile, multispectral, synthetics, long-distance, contact lenses, liveness detection, etc. However, these databases mostly contain subjects of Caucasian and Asian docents with very few Africans. Despite many investigative studies on racial bias in face biometrics, very few studies on iris biometrics have been published, mainly due to the lack of racially diverse large-scale databases containing sufficient iris samples of Africans in the public domain. Furthermore, most of these databases contain a relatively small number of subjects and labelled images. This paper proposes a large-scale African database named Chinese Academy of Sciences Institute of Automation (CASIA)-Iris-Africa that can be used as a complementary database for the iris recognition community to mediate the effect of racial biases on Africans. The database contains 28 717 images of 1 023 African subjects (2 046 iris classes) with age, gender, and ethnicity attributes that can be useful in demographically sensitive studies of Africans. Sets of specific application protocols are incorporated with the database to ensure the database’s variability and scalability. Performance results of some open-source state-of-the-art (SOTA) algorithms on the database are presented, which will serve as baseline performances. The relatively poor performances of the baseline algorithms on the proposed database despite better performance on other databases prove that racial biases exist in these iris recognition algorithms.&lt;/p&gt;
&lt;h2 id=&#34;3-multi-faceted-knowledge-driven-graph-neural-network-for-iris-segmentation&#34;&gt;3. Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Jianze Wei, Yunlong Wang, Xingyu Gao, Ran He, Zhenan Sun. &amp;ldquo;Multi-Faceted Knowledge-Driven Graph Neural Network for Iris Segmentation&amp;rdquo;. IEEE Transactions on Information Forensics and Security ( Volume: 19).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/TIFS.2024.3407508&#34;&gt;https://doi.org/10.1109/TIFS.2024.3407508&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;TIFS-2024-06-pic1.png&#34; alt=&#34;TIFS-2024-06&#34;&gt;&lt;/p&gt;
&lt;p&gt;Accurate iris segmentation, especially around the iris inner and outer boundaries, is still a formidable challenge. Pixels within these areas are difficult to semantically distinguish since they have similar visual characteristics and close spatial positions. To tackle this problem, the paper proposes an iris segmentation graph neural network (ISeGraph) for accurate segmentation. ISeGraph regards individual pixels as nodes within the graph and constructs self-adaptive edges according to multi-faceted knowledge, including visual similarity, positional correlation, and semantic consistency for feature aggregation. Specifically, visual similarity strengthens the connections between nodes sharing similar visual characteristics, while positional correlation assigns weights according to the spatial distance between nodes. In contrast to the above knowledge, semantic consistency maps nodes into a semantic space and learns pseudo-labels to define relationships based on label consistency. ISeGraph leverages multi-faceted knowledge to generate self-adaptive relationships for accurate iris segmentation. Furthermore, a pixel-wise adaptive normalization module is developed to increase the feature discriminability. It takes informative features in the shallow layer as a reference to improve the segmentation features from a statistical perspective. Experimental results on three iris datasets illustrate that the proposed method achieves superior performance in iris segmentation, increasing the segmentation accuracy in areas near the iris boundaries.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open-Vocabulary Text-Driven Human Image Generation</title>
      <link>https://nlpr-sir.github.io/publication/kunbo-ijcv-2024/</link>
      <pubDate>Wed, 15 May 2024 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/kunbo-ijcv-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to International Journal of Computer Vision (IJCV 2024)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2401/</link>
      <pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2401/</guid>
      <description>&lt;h2 id=&#34;1-open-vocabulary-text-driven-human-image-generation&#34;&gt;1. Open-Vocabulary Text-Driven Human Image Generation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kaiduo Zhang, Muyi Sun, Jianxin Sun, Kunbo Zhang, Zhenan Sun, Tieniu Tan. &amp;ldquo;Open-Vocabulary Text-Driven Human Image Generation&amp;rdquo;. 2024 International Journal of Computer Vision.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-024-02079-7&#34;&gt;https://link.springer.com/article/10.1007/s11263-024-02079-7&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;IJCV-2024-05-pic1.png&#34; alt=&#34;IJCV-2024-05&#34;&gt;&lt;/p&gt;
&lt;p&gt;Generating human images from open-vocabulary text descriptions is an exciting but challenging task. Previous methods (i.e., Text2Human) face two challenging problems: (1) they cannot well handle the open-vocabulary setting by arbitrary text inputs (i.e., unseen clothing appearances) and heavily rely on limited preset words (i.e., pattern styles of clothing appearances); (2) the generated human image is inaccuracy in open-vocabulary settings. To alleviate these drawbacks, we propose a flexible diffusion-based framework, namely HumanDiffusion, for open-vocabulary text-driven human image generation (HIG). The proposed framework mainly consists of two novel modules: the Stylized Memory Retrieval (SMR) module and the Multi-scale Feature Mapping (MFM) module. Encoded by the vision-language pretrained CLIP model, we obtain coarse features of the local human appearance. Then, the SMR module utilizes an external database that contains clothing texture details to refine the initial coarse features. Through SMR refreshing, we can achieve the HIG task with arbitrary text inputs, and the range of expression styles is greatly expanded. Later, the MFM module embedding in the diffusion backbone can learn fine-grained appearance features, which effectively achieves precise semantic-coherence alignment of different body parts with appearance features and realizes the accurate expression of desired human appearance. The seamless combination of the proposed novel modules in HumanDiffusion realizes the freestyle and high accuracy of text-guided HIG and editing tasks. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SOTA) performance, especially in the open-vocabulary setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boosting multi-modal ocular recognition via spatial feature reconstruction and unsupervised image quality estimation</title>
      <link>https://nlpr-sir.github.io/publication/yan-mir-2024-1/</link>
      <pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/yan-mir-2024-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Iris-Africa: A Large-scale African Iris Image Database</title>
      <link>https://nlpr-sir.github.io/publication/jawad-mir-2024-2/</link>
      <pubDate>Fri, 12 Jan 2024 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/jawad-mir-2024-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images</title>
      <link>https://nlpr-sir.github.io/publication/hu-aaai-2024/</link>
      <pubDate>Wed, 13 Dec 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/hu-aaai-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to The Association for the Advancement of Artificial Intelligence (AAAI 2024)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2402/</link>
      <pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2402/</guid>
      <description>&lt;h2 id=&#34;1-learning-explicit-contact-for-implicit-reconstruction-of-hand-held-objects-from-monocular-images&#34;&gt;1. Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Junxing Hu,Hongwen Zhang,Zerui Chen,Mengcheng Li,Yunlong Wang,Yebin Liu,Zhenan Sun. Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images. The Association for the Advancement of Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.20089&#34;&gt;https://arxiv.org/abs/2305.20089&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;AAAI-2024-pic1.png&#34; alt=&#34;AAAI-2024&#34;&gt;&lt;/p&gt;
&lt;p&gt;Reconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. Our method consists of two components: explicit contact prediction and implicit shape reconstruction. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, our method can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin. Our code is publicly available at &lt;a href=&#34;https://junxinghu.github.io/projects/hoi.html&#34;&gt;https://junxinghu.github.io/projects/hoi.html&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence</title>
      <link>https://nlpr-sir.github.io/publication/ru-mm-2023/</link>
      <pubDate>Fri, 27 Oct 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ru-mm-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation</title>
      <link>https://nlpr-sir.github.io/publication/haiqing-ijcb-2023/</link>
      <pubDate>Thu, 28 Sep 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/haiqing-ijcb-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to International Joint Conference on Biometrics (IJCB 2023)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2301/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2301/</guid>
      <description>&lt;h2 id=&#34;1-sclera-transfuse-fusing-swin-transformer-and-cnn-for-accurate-sclera-segmentation&#34;&gt;1. Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Li Haiqing, Wang Caiyong, Zhao Guangzhe, He Zhaofeng, Wang Yunlong,Zhenan Sun. Sclera-TransFuse: Fusing Swin Transformer and CNN for Accurate Sclera Segmentation. seventh International Joint Conference on Biometrics (IJCB), 2023. &lt;a href=&#34;https://ijcb2023.ieee-biometrics.org/&#34;&gt;https://ijcb2023.ieee-biometrics.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;IJCB2023-pic1.png&#34; alt=&#34;IJCB 2023&#34;&gt;&lt;/p&gt;
&lt;p&gt;Sclera segmentation is a crucial step in sclera recognition, which has been greatly advanced by Convolutional Neural Networks (CNNs). However, when dealing with non-ideal eye images, many existing CNN-based approaches are still prone to failure. One major reason is that due to the limited range of receptive fields, CNNs are difficult to effectively model global semantic relevance and thus robustly resist noise interference. To solve this problem, this paper proposes a novel two-stream hybrid model, named Sclera-TransFuse, to integrate classical ResNet-34 and recently emerging Swin Transformer encoders. Specially, the self-attentive Swin Transformer has shown a strong ability in capturing long-range spatial dependencies and has a hierarchical structure similar to CNNs. The dual encoders firstly extract coarse- and fine-grained feature representations at hierarchical stages, separately. Then a novel Cross-Domain Fusion (CDF) module based on information interaction and self-attention mechanism is introduced to efficiently fuse the multi-scale features extracted from dual encoders. Finally, the fused features are progressively upsampled and aggregated to predict the sclera masks in the decoder meanwhile deep supervision strategies are employed to learn intermediate feature representations better and faster. Experimental results show that Sclera-TransFuse achieves state-of-the-art performance on various sclera segmentation benchmarks. Additionally, a UBIRIS.v2 subset of 683 eye images with manually labeled sclera masks, and our codes are publicly available to the community through &lt;a href=&#34;https://github.com/Ihqqq/Sclera-TransFuse&#34;&gt;https://github.com/Ihqqq/Sclera-TransFuse&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Personalized Graph Generation for Monocular 3D Human Pose and Shape Estimation</title>
      <link>https://nlpr-sir.github.io/publication/hu-tcsvt-2023-2/</link>
      <pubDate>Thu, 31 Aug 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/hu-tcsvt-2023-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions</title>
      <link>https://nlpr-sir.github.io/publication/ren-tpami-2023/</link>
      <pubDate>Thu, 20 Jul 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-tpami-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to T-PAMI 2023 and another was accpeted to TIFS 2023</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-23/</link>
      <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-23/</guid>
      <description>&lt;h2 id=&#34;1-multiscale-dynamic-graph-representation-for-biometric-recognition-with-occlusions&#34;&gt;1. Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Min Ren, Yunlong Wang, Yuhao Zhu, Kunbo Zhang, Zhenan Sun. &amp;ldquo;Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions&amp;rdquo;. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) (Volume: 45)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1109/TPAMI.2023.3298836&#34;&gt;http://doi.org/10.1109/TPAMI.2023.3298836&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;TPAMI-2023-07-pic1.png&#34; alt=&#34;TPAMI-2023-07&#34;&gt;&lt;/p&gt;
&lt;p&gt;Occlusion is a common problem with biometric recognition in the wild. The generalization ability of CNNs greatly decreases due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrating the merits of both CNNs and graph models to overcome occlusion problems in biometric recognition, called multiscale dynamic graph representation (MS-DGR). More specifically, a group of deep features reflected on certain subregions is recrafted into a feature graph (FG). Each node inside the FG is deemed to characterize a specific local region of the input sample, and the edges imply the co-occurrence of non-occluded regions. By analyzing the similarities of the node representations and measuring the topological structures stored in the adjacent matrix, the proposed framework leverages dynamic graph matching to judiciously discard the nodes corresponding to the occluded parts. The multiscale strategy is further incorporated to attain more diverse nodes representing regions of various sizes. Furthermore, the proposed framework exhibits a more illustrative and reasonable inference by showing the paired nodes. Extensive experiments demonstrate the superiority of the proposed framework, which boosts the accuracy in both natural and occlusion-simulated cases by a large margin compared with that of baseline methods.&lt;/p&gt;
&lt;h2 id=&#34;2-iris-guidenet-guided-localisation-and-segmentation-network-for-unconstrained-iris-biometrics&#34;&gt;2. Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Jawad Muhammad, Caiyong Wang, Yunlong Wang, Kunbo Zhang, Zhenan Sun. &amp;ldquo;Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics&amp;rdquo;. IEEE Transactions on Information Forensics and Security(Volume:18).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1109/TIFS.2023.3268504&#34;&gt;http://doi.org/10.1109/TIFS.2023.3268504&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;TIFS-2023-04-pic1.png&#34; alt=&#34;TIFS-2023-04&#34;&gt;&lt;/p&gt;
&lt;p&gt;In recent years, unconstrained iris biometrics has become more prevalent due to its wide range of user applications. However, it also presents numerous challenges to the Iris pre-processing task of Localization and Segmentation (ILS). Many ILS techniques have been proposed to address these challenges, among which the most effective is the CNN-based methods. Training the CNN is data-intensive, and most of the existing CNN-based ILS approaches do not incorporate iris-specific features that can reduce their data dependence, despite the limited labelled iris data in the available databases. These trained CNN models built upon these databases can be sub-optimal. Hence, this paper proposes a guided CNN-based ILS approach IrisGuideNet. IrisGuideNet involves incorporating novel iris-specific heuristics named Iris Regularization Term (IRT), deep supervision technique, and hybrid loss functions in the training pipeline, which guides the network and reduces the model data dependence. A novel Iris Infusion Module (IIM) that utilizes the geometrical relationships between the ILS outputs to refine the predicted outputs is introduced at network inference. The proposed model is trained and evaluated with various datasets. Experimental results show that IrisGuideNet has outperformed most models across all the database categories. The codes implementation of the proposed IrisGuideNet will be available at: &lt;a href=&#34;https://github.com/mohdjawadi/IrisGuidenet&#34;&gt;https://github.com/mohdjawadi/IrisGuidenet&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Iris-GuideNet: Guided Localisation and Segmentation Network for Unconstrained Iris Biometrics</title>
      <link>https://nlpr-sir.github.io/publication/jawad-tifs-2023-1/</link>
      <pubDate>Wed, 19 Apr 2023 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/jawad-tifs-2023-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Iris-Africa</title>
      <link>https://nlpr-sir.github.io/dataset/casia-iris-africa/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-iris-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Various forms of iris databases had been published that address some specific research problems and are made available to the biometrics research community such as: constraint databases; mobile iris; multispectral; synthetics; iris at a distance; contact lenses; liveness detection; Aging, etc. However, even though these databases have addressed many of the problems mentioned above, most of them contain subjects of primarily Caucasian and Asian docents with very few Africans and many times zero Africans. It is particularly challenging as this has created a research blind spot for African cohorts in these databases. Despite many of the reported investigative studies on racial bias in face biometrics, very few iris racial bias-related studies have been published, which can be due to the insufficient number of other races databases such as the Africans. Recently, face recognition algorithms have been reported to be biased toward specific demographics. Most prominently, many investigative studies have reported higher false-positive rates in African subject cohorts than in other cohorts. Due to the unavailability of the required quantity of African cohorts in the publicly available iris databases, similar studies would be difficult to replicate on iris biometrics. We presents a mainly African dataset that can be useful in addressing some of these challenges.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The iris images were captured in Nigeria, Africa. The capturing exercise was conducted across various locations in multiple sessions over three months. Due to logistical constraints, the capturing exercise was limited to the northern part of Nigeria. Each subject volunteer was asked to sign a consent form authorizing the data to be used for only research purposes. The iris sensor device for capturing the images is the IKUSBE30 iris sensor from IrisKing. The setup for the capturing exercise is shown in Fig. 1. Each volunteer was asked to hold the device, standing or sitting, based on their preference. As such, the iris images were captured at various degrees of head postures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/1.png&#34; alt=&#34;Figure 1&#34; title=&#34;Iris Capturing set-ups (indoor and outdoor)&#34;&gt;
Fig. 1: Iris Capturing set-ups (indoor and outdoor)&lt;/p&gt;
&lt;p&gt;The procedure for image capturing is in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Step 1: Straight Gaze Capture: In this step, the volunteer was asked to hold the sensor and gaze into its lenses while moving the eyes in small ranges to the left, right, up, or down for approximately 3 minutes. During this time, images were periodically captured automatically at constant intervals. The eye movement ensures that the iris position and orientation are highly diversified across the captured images. Samples of the captured images in this step are shown in Fig. 2 (left).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Step 2: Open-Close Capture:  In this step, the volunteer was asked to open and close the eyes while gazing into the device for approximately 3 minutes. The iris images were automatically captured within this period. The essence of the opening and closing of the eyes is to ensure that the images contain irises of various sizes and degrees of occlusion, from fully closed eyes to half open and fully open eyes. This procedure will ultimately improve image diversity. Exemplary samples are shown in Fig. 2 (right).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;figures/2.png&#34; alt=&#34;Figure 2&#34; title=&#34;Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&#34;&gt;
Fig. 2: Exemplary samples of captured images in Step 1 (left image) and Step 2 (right image)&lt;/p&gt;
&lt;p&gt;After the capturing sessions, the stored images were processed. The processing steps comprise automatic image selection to discard images with duplicate content, followed by manual image selection that guarantees the selected images&amp;rsquo; reliability. In the automatic selection, all the images from a single eye of one subject were organized into a matrix, with each column representing an image. The images of the subject&amp;rsquo;s eyes were then processed, and duplicates were removed using this procedure. The remaining images were then manually processed. The manual selection involves one-by-one human inspection of each image to identify damaged images and those images with no irises captured in them. Some sample images of one subject from the generated dataset are shown in Fig. 3, the labelled sampled images in Fig. 4, the summary of the database is presented in Table 1 and the database subjects&amp;rsquo; age distribution is shown in Fig. 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/3.png&#34; alt=&#34;Figure 3&#34; title=&#34;Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 3: Samples of one subject left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/4.png&#34; alt=&#34;Figure 4&#34; title=&#34;Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&#34;&gt;
Fig. 4: Labelled iris mask, inner and outer circle superimposed on respective samples in Fig. 3 for (a) left iris (upper two rows) and (b) right iris (lower two rows)&lt;/p&gt;
&lt;p&gt;Tab. 1: Summary of the generated database
&lt;img src=&#34;figures/table.png&#34; alt=&#34;Figure 5&#34; title=&#34;Summary of the generated database&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/5.png&#34; alt=&#34;Figure 6&#34; title=&#34;Age distribution of the generated database&#34;&gt;
Fig. 5: Age distribution of the generated database&lt;/p&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image folder: This contain the iris images. Each image is named with SubjectID_Eye_ImageNumber.jpg. That represents:
&lt;ul&gt;
&lt;li&gt;SubjectID: a unique number for each subject&lt;/li&gt;
&lt;li&gt;Eye is a letter that can be either “L” for left eye or “R” for right eye&lt;/li&gt;
&lt;li&gt;ImageNumber is a sequential number for images for the same subject&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iris_edge folder: This contain the corresponding 1-pixel iris edge binary images&lt;/li&gt;
&lt;li&gt;iris_edge_mask folder: This contain the corresponding iris binary masks of the images&lt;/li&gt;
&lt;li&gt;pupil_edge folder: This contain the corresponding 1-pixel pupil edge binary images&lt;/li&gt;
&lt;li&gt;pupil_edge_mask folder: This contain the corresponding pupil binary masks of the images&lt;/li&gt;
&lt;li&gt;params folder: This contain the corresponding ini files that describes radius and orientation both the iris and pupil used in the generation of iris edge, iris mask, pupil edge, pupil and mask images&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-Iris-Africa collected by the Chinese Academy of Sciences’ Institute of Automation (CASIA)” and a reference to “CASIA-Iris-Africa Image Database, 
&lt;a href=&#34;http://biometrics.idealtest.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>D-ESRGAN: A Dual-Encoder GAN with Residual CNN and Vision Transformer for Iris Image Super-Resolution</title>
      <link>https://nlpr-sir.github.io/publication/caiyong-ijcb-2022/</link>
      <pubDate>Tue, 17 Jan 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/caiyong-ijcb-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus</title>
      <link>https://nlpr-sir.github.io/publication/zhow-tcsvt-2023/</link>
      <pubDate>Mon, 16 Jan 2023 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/zhow-tcsvt-2023/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;Figure 1&#34; title=&#34; The framework of AIF-LFNet&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One paper was accepted to The Association for Transactions on Circuits and Systems for Video Technology(TCSVT 2023)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2302/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2302/</guid>
      <description>&lt;h2 id=&#34;1-aif-lfnet-all-in-focus-light-field-super-resolution-method-considering-the-depth-varying-defocus&#34;&gt;1. AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Shubo Zhou, Liang Hu, Yunlong Wang, Zhenan Sun, Kunbo Zhang, Xue-qin Jiang. &amp;ldquo;AIF-LFNet: All-in-Focus Light Field Super-Resolution Method Considering the Depth-Varying Defocus&amp;rdquo;. Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10018388&#34;&gt;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10018388&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;TCSVT-2023-pic1.png&#34; alt=&#34;TCSVT-2023&#34;&gt;&lt;/p&gt;
&lt;p&gt;As an aperture-divided computational imaging system, microlens array (MLA) -based light field (LF) imaging is playing an increasingly important role in computer vision. As the trade-off between the spatial and angular resolutions, deep learning (DL) -based image super-resolution (SR) methods have been applied to enhance the spatial resolution. However, in existing DL-based methods, the depth-varying defocus is not considered both in dataset development and algorithm design, which restricts many applications such as depth estimation and object recognition. To overcome this shortcoming, a super-resolution task that reconstructs all-in-focus high-resolution (HR) LF images from low-resolution (LR) LF images is proposed by designing a large dataset and proposing a convolutional neural network (CNN) -based SR method. The dataset is constructed by using Blender software, consisting of 150 light field images used as training data, and 15 light field images used as validation and testing data. The proposed network is designed by proposing the dilated deformable convolutional network (DCN) -based feature extraction block and the LF subaperture image (SAI) Deblur-SR block. The experimental results demonstrate that the proposed method achieves more appealing results both quantitatively and qualitatively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pose-Appearance Relational Modeling for Video Action Recognition</title>
      <link>https://nlpr-sir.github.io/publication/kunbo-itip-2022/</link>
      <pubDate>Wed, 14 Dec 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/kunbo-itip-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contextual Measures for Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/wei-tifs-2022/</link>
      <pubDate>Mon, 14 Nov 2022 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wei-tifs-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Empirical Comparative Analysis of Africans with Asians Using DCNN Facial Biometric Models</title>
      <link>https://nlpr-sir.github.io/publication/jawad-ccbr-2022/</link>
      <pubDate>Thu, 03 Nov 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/jawad-ccbr-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring Bias in Sclera Segmentation Models: A Group Evaluation Approach</title>
      <link>https://nlpr-sir.github.io/publication/matej-tifs-2022/</link>
      <pubDate>Fri, 21 Oct 2022 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/matej-tifs-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PDVN: A Patch-based Dual-view Network for Face Liveness Detection using Light Field Focal Stack</title>
      <link>https://nlpr-sir.github.io/publication/wang-ijcb-2022/</link>
      <pubDate>Thu, 13 Oct 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-ijcb-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Polar</title>
      <link>https://nlpr-sir.github.io/dataset/casia-polar/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-polar/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;CASIA-Polar is a face anti-spoofing dataset based on polarization imaging, which takes advantage of polarization information in material classification to enable robust face anti-spoofing research.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The hardware system used to collect this dataset is shown in Fig.1, consists of a Lucid Phoenix PHX050S-P polarized camera equipped with Sony’s polarization sensor and a Mindvision MVGE501GC-T RGB camera.&lt;/p&gt;
&lt;p&gt;Types of spoofing attacks include printed paper, printed glossy photographs, electronic displays, silicone masks, rubber masks, and customized silicone prosthetic heads. High-quality face samples are first captured by the MVGE501GC-T RGB camera and these high-quality samples are then printed on paper and photographs or displayed on a computer screen to produce artifacts. At the same time, both genuine and spoofing attack face samples were captured when the subjects and presentation attack were standing at or be placed at six distances, i.e. 1m, 1.5m, 2m, 3m, 4m, and 5m.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/1_1.png&#34; alt=&#34;Figure 1&#34; title=&#34;&#34;&gt;
&lt;img src=&#34;figures/1_2.png&#34; alt=&#34;Figure 2&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset includes 22,174 samples from 121 subjects. All subjects had visible light and polarized images taken. The genuine face has 14,698 samples, while the spoofing attack has about 7,476 samples.&lt;/p&gt;
&lt;p&gt;Figure 2 shows an example of the face images collected in the dataset, with the first row being the visible image and the second row is the corresponding DoLP image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/2.png&#34; alt=&#34;Figure 2&#34; title=&#34;The attacks present in CASIA-Polar&#34;&gt;
Fig.2: The attacks present in CASIA-Polar&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved.&lt;/p&gt;
&lt;p&gt;E-mail: &lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining 2D texture and 3D geometry features for Reliable iris presentation attack detection using light field focal stack</title>
      <link>https://nlpr-sir.github.io/publication/luo-iet-2022/</link>
      <pubDate>Sat, 27 Aug 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/luo-iet-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Perturbation Inactivation Based Adversarial Defense for Face Recognition</title>
      <link>https://nlpr-sir.github.io/publication/ren-tifs-2022/</link>
      <pubDate>Mon, 01 Aug 2022 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-tifs-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to International Conference on Machine Learning (ICML 2022)</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2207/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2207/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Zhengquan Luo, Yunlong Wang*, Zilei Wang, Zhenan Sun, Tieniu Tan. Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring. Thirty-ninth International Conference on Machine Learning (ICML), 2022. &lt;a href=&#34;https://icml.cc/Conferences/2022&#34;&gt;https://icml.cc/Conferences/2022&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Attributes skew hinders the current federated learning (FL) frameworks from consistent optimization directions among the clients, which inevitably leads to performance reduction and unstable convergence. The core problems lie in that: 1) Domain-specific attributes, which are non-causal and only locally valid, are indeliberately mixed into global aggregation. 2) The one-stage optimizations of entangled attributes cannot simultaneously satisfy two conflicting objectives, i.e., generalization and personalization. To cope with these, we proposed disentangled federated learning (DFL) to disentangle the domain-specific and cross-invariant attributes into two complementary branches, which are trained by the proposed alternating local-global optimization independently. Importantly, convergence analysis proves that the FL system can be stably converged even if incomplete client models participate in the global aggregation, which greatly expands the application scope of FL. Extensive experiments verify that DFL facilitates FL with higher performance, better interpretability, and faster convergence rate, compared with SOTA FL methods on both manually synthesized and realistic attributes skew datasets.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/xU_Rvbofvwveekq41SCpZA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;人工智能前沿讲习公众号：【源头活水】ICML 2022 | 共识表征提取和多样性传播的解构联邦学习框架&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ICML2022-pic1.png&#34; alt=&#34;ICML2022&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FedIris: Towards More Accurate and Privacy-Preserving Iris Recognition via Federated Template Communication</title>
      <link>https://nlpr-sir.github.io/publication/luo-cvprw-2022/</link>
      <pubDate>Mon, 20 Jun 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/luo-cvprw-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EYE LOCATION AND STATE ESTIMATION BASED ON LANDMARKS</title>
      <link>https://nlpr-sir.github.io/publication/he-cas-2022/</link>
      <pubDate>Fri, 17 Jun 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/he-cas-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring</title>
      <link>https://nlpr-sir.github.io/publication/luo-icml-2022/</link>
      <pubDate>Tue, 14 Jun 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/luo-icml-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Iris-LFLD</title>
      <link>https://nlpr-sir.github.io/dataset/casia-iris-lfld/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-iris-lfld/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The dataset for iris liveness detection based on light field (LF) imaging was collected by [1], wherein the first author is one of our collaborators. We have got the authority from the authors of [1] and released the LF focal stack data.&lt;/p&gt;
&lt;h2 id=&#34;setup-of-dataset-collection&#34;&gt;Setup of dataset collection&lt;/h2&gt;
&lt;p&gt;The dataset was captured using a 
&lt;a href=&#34;http://cripac.ia.ac.cn/CN/column/item105.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lab-produced microlens based LF camera&lt;/a&gt; and a commercial device 
&lt;a href=&#34;http://www.irisking.com/pron.php?id=523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IKUSB-E30&lt;/a&gt; under near-infrared (NIR) illumination. The setup of dataset collection was shown in Fig.1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/capture_setup.png&#34; alt=&#34;Figure 1&#34; title=&#34;Setup of dataset collection.&#34;&gt;
Fig. 1: Setup of dataset collection&lt;/p&gt;
&lt;p&gt;The types of presentation attacks include printed papers, printed glossy photos, and electronic displays. The high-quality iris samples were first captured by IKUSB-E30, and then these high-quality samples were printed on papers and photos, or displayed on the screen of iPad mini 4 to generate the artefacts. The main lens of the lab-produced LF camera was tuned to be in focus at a position of 1.6 meters. Simultaneously, both bona fide and presentation attack iris samples were captured when the subjects and PAIs were standing at or be placed at three distances, i.e. 1.5 meters, 1.6 meters, 1.7 meters.&lt;/p&gt;
&lt;h2 id=&#34;statistics-of-the-dataset&#34;&gt;Statistics of the Dataset&lt;/h2&gt;
&lt;p&gt;The dataset contains 504 samples from 14 subjects, consisting of 230 LF images of bona fide iris and 274 LF images of spoofing iris. The respective sample number of the PAIs, i.e. printed papers, printed photos, and electronic display are 18, 122, 134.&lt;/p&gt;
&lt;p&gt;An example of raw LF image containing both eyes printed on photos is shown in Fig.2. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/lf_raw_sample.png&#34; alt=&#34;Figure 2&#34; title=&#34;An example of raw LF image containing both eyes printed on photos.&#34;&gt;
Fig.2: An example of raw LF image containing both eyes printed on photos. Hexagonal microlens images can be observed from the close-up of iris in the raw LF image.&lt;/p&gt;
&lt;p&gt;The LF toolbox released by [2] was utilized to decode raw LF images into 4D LF data. The eye regions were cropped from the same location of each sub-aperture image (SAI). The spatial resolution of each SAI after cropping is $128 \times 96$, and the angular resolution is $7 \times 7$. Examples from the same subject&amp;rsquo;s right eye in the dataset are shown in Fig.3.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/dataset_samples.png&#34; alt=&#34;Figure 3&#34; title=&#34;Examples from the same subject&#39;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&#34;&gt;
Fig. 3: Examples from the same subject&amp;rsquo;s right eye in the dataset. (a) Bona fide iris sample. (b) A4 paper printed iris sample. (c) Glossy photo printed iris sample. (d) Electronically displayed iris sample.&lt;/p&gt;
&lt;p&gt;The rendered focal stack via digital refocusing has 145 slices around the best focus plane.&lt;/p&gt;
&lt;p&gt;The details of the adopted database is listed in Table 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/details.png&#34; alt=&#34;Table 1&#34; title=&#34;Details of the adopted database&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-and-contacts&#34;&gt;Copyright and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the  database are reserved.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] 宋平, 黄玲, 王云龙, 刘菲, 孙哲南. 基于计算光场成像的虹膜活体检测方法. 自动化学报, 2019, 45(9): 1701-1712. (Ping Song, Huang Ling, Wang Yunlong, Liu Fei, and Sun Zhenan. Iris liveness detection based on light field imaging. ACTA AUTOMATICA SINICA, 45(9):1701–1712, 2019.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[2] Donald G Dansereau, Oscar Pizarro, and Stefan B Williams, “Decoding, calibration and rectification for lenselet-based plenoptic cameras,” in Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1027–1034.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Towards Interpretable Defense Against Adversarial Attacks via Causal Inference</title>
      <link>https://nlpr-sir.github.io/publication/ren-mir-2022/</link>
      <pubDate>Sat, 28 May 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-mir-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to Mahcine Intelligence Research (MIR) and another one was accpeted to CVPRW 2022</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2204/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2204/</guid>
      <description>&lt;h2 id=&#34;1-towards-interpretable-defense-against-adversarial-attacks-via-causal-inference&#34;&gt;1. Towards Interpretable Defense Against Adversarial Attacks via Causal Inference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Min Ren, Yunlong Wang*, Zhaofeng He. Towards interpretable defense against adversarial attacks via causal inference. Machine Intelligence Research.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1007/s11633-022-1330-7&#34;&gt;http://doi.org/10.1007/s11633-022-1330-7&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;MIR-2022-05-pic1.jpg&#34; alt=&#34;MIR-2022-05&#34;&gt;&lt;/p&gt;
&lt;p&gt;Deep learning-based models are vulnerable to adversarial attacks. Defense against adversarial attacks is essential for sensitive and safety-critical scenarios. However, deep learning methods still lack effective and efficient defense mechanisms against adversari-al attacks. Most of the existing methods are just stopgaps for specific adversarial samples. The main obstacle is that how adversarial samples fool the deep learning models is still unclear. The underlying working mechanism of adversarial samples has not been well explored, and it is the bottleneck of adversarial attack defense. In this paper, we build a causal model to interpret the generation and performance of adversarial samples. The self-attention/transformer is adopted as a powerful tool in this causal model. Compared to existing methods, causality enables us to analyze adversarial samples more naturally and intrinsically. Based on this causal model, the working mechanism of adversarial samples is revealed, and instructive analysis is provided. Then, we propose simple and effective adversarial sample detection and recognition methods according to the revealed working mechanism. The causal insights enable us to detect and recognize adversarial samples without any extra model or training. Extensive experiments are conducted to demonstrate the effectiveness of the proposed methods. Our methods outperform the state-of-the-art defense methods under various adversarial attacks.&lt;/p&gt;
&lt;h2 id=&#34;2-fediris-towards-more-accurate-and-privacy-preserving-iris-recognition-via-federated-template-communication&#34;&gt;2. FedIris: Towards More Accurate and Privacy-preserving Iris Recognition via Federated Template Communication&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Zhengquan Luo, Yunlong Wang*, Zilei Wang, Zhenan Sun,Tieniu Tan. IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022, International Workshop on Federated Learning for Computer Vision.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/fedvision/home?authuser=0&#34;&gt;https://sites.google.com/view/fedvision/home?authuser=0&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;CVPRW-2022-05-pic1.png&#34; alt=&#34;CVPRW-2022-05&#34;&gt;&lt;/p&gt;
&lt;p&gt;As biometric data undergo rapidly growing privacy concerns, building large-scale datasets has become more difficult. Unfortunately, current iris databases are mostly in small scale, e.g., thousands of iris images from hundreds of identities. What&amp;rsquo;s worse, the heterogeneity among decentralized iris datasets hinders the current deep learning (DL) frameworks from obtaining recognition performance with robust generalization. It motivates us to leverage the merits of federated learning (FL) to solve these problems. However, traditional FL algorithms often employ model sharing for knowledge transfer, wherein the simple averaging aggregation lacks interpretability, and divergent optimization directions of clients lead to performance degradation. To overcome this interference, we propose FedIris with solid theoretical foundations, which attempts to employ the iris template as the communication carrier and formulate federated triplet (Fed-Triplet) for knowledge transfer. Furthermore, the massive heterogeneity among iris datasets may induce negative transfer and unstable optimization. The modified Wasserstein distance is embedded into the FedTriplet loss to reweight global aggregation, which drives the clients with similar data distributions to contribute more mutually. Extensive experimental results demonstrate that the proposed FedIris outperforms SOLO training, model-sharing-based FL training, and even centralized training.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基于关键点的眼睛定位和状态估计</title>
      <link>https://nlpr-sir.github.io/publication/heyong-caas-2022-new/</link>
      <pubDate>Fri, 01 Apr 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/heyong-caas-2022-new/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper was accepted to IEEE TIFS and another one was accpeted to Journal of Electronic Imaging</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2203/</link>
      <pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2203/</guid>
      <description>&lt;h2 id=&#34;towards-more-discriminative-and-robust-iris-recognition-by-learning-uncertain-factors&#34;&gt;Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;uncertainty.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jianze Wei, Huaibo Huang, Yunlong Wang, Ran He and Zhenan Sun, &amp;ldquo;Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors,&amp;rdquo; in IEEE Transactions on Information Forensics and Security, doi: 10.1109/TIFS.2022.3154240.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The uncontrollable acquisition process limits the performance of iris recognition. In the acquisition process, various inevitable factors, including eyes, devices, and environment, hinder the iris recognition system from learning a discriminative identity representation. This leads to severe performance degradation. In this paper, we explore uncertain acquisition factors and propose uncertainty embedding (UE) and uncertainty-guided curriculum learning (UGCL) to mitigate the influence of acquisition factors. UE represents an iris image using a probabilistic distribution rather than a deterministic point (binary template or feature vector) that is widely adopted in iris recognition methods. Specifically, UE learns identity and uncertainty features from the input image, and encodes them as two independent components of the distribution, mean and variance. Based on this representation, an input image can be regarded as an instantiated feature sampled from the UE, and we can also generate various virtual features through sampling. UGCL is constructed by imitating the progressive learning process of newborns. Particularly, it selects virtual features to train the model in an easy-to-hard order at different training stages according to their uncertainty. In addition, an instance-level enhancement method is developed by utilizing local and global statistics to mitigate the data uncertainty from image noise and acquisition conditions in the pixel-level space. The experimental results on six benchmark iris datasets verify the effectiveness and generalization ability of the proposed method on same-sensor and cross-sensor recognition.&lt;/p&gt;
&lt;p&gt;Github repository：
&lt;a href=&#34;https://github.com/reborn20200813/uncertainty&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/reborn20200813/uncertainty&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;multitask-deep-active-contour-based-iris-segmentation-for-off-angle-iris-images&#34;&gt;Multitask deep active contour-based iris segmentation for off-angle iris images&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Tianhao Lu, Caiyong Wang, Yunlong Wang, and Zhenan Sun &amp;ldquo;Multitask deep active contour-based iris segmentation for off-angle iris images,&amp;rdquo; Journal of Electronic Imaging 31(4), 041211 (26 February 2022).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Iris recognition has been considered as a secure and reliable biometric technology. However, iris images are prone to off-angle or are partially occluded when captured with fewer user cooperations. As a consequence, iris recognition especially iris segmentation suffers a serious performance drop. To solve this problem, we propose a multitask deep active contour model for off-angle iris image segmentation. Specifically, the proposed approach combines the coarse and fine localization results. The coarse localization detects the approximate position of the iris area and further initializes the iris contours through a series of robust preprocessing operations. Then, iris contours are represented by 40 ordered isometric sampling polar points and thus their corresponding offset vectors are regressed via a convolutional neural network for multiple times to obtain the precise inner and outer boundaries of the iris. Next, the predicted iris boundary results are regarded as a constraint to limit the segmentation range of noise-free iris mask. Besides, an efficient channel attention module is introduced in the mask prediction to make the network focus on the valid iris region. A differentiable, fast, and efficient SoftPool operation is also used in place of traditional pooling to keep more details for more accurate pixel classification. Finally, the proposed iris segmentation approach is combined with off-the-shelf iris feature extraction models including traditional OM and deep learning-based FeatNet for iris recognition. The experimental results on two NIR datasets CASIA-Iris-off-angle, CASIA-Iris-Africa, and a VIS dataset SBVPI show that the proposed approach achieves a significant performance improvement in the segmentation and recognition for both regular and off-angle iris images.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards More Discriminative and Robust Iris Recognition by Learning Uncertain Factors</title>
      <link>https://nlpr-sir.github.io/publication/wei-tifs-2022_2/</link>
      <pubDate>Mon, 28 Feb 2022 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wei-tifs-2022_2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multitask deep active contour-based iris segmentation for off-angle iris images</title>
      <link>https://nlpr-sir.github.io/publication/lu-jei-2022/</link>
      <pubDate>Thu, 17 Feb 2022 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/lu-jei-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview of Recent Published Papers</title>
      <link>https://nlpr-sir.github.io/post/recent-published-papers-2111/</link>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/recent-published-papers-2111/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Min Ren, Lingxiao He, Xingyu Liao, Wu Liu, Yunlong Wang, Tieniu Tan; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 14930-14939&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;J. Muhammad, Y. Wang, C. Wang, K. Zhang and Z. Sun, &amp;ldquo;CASIA-Face-Africa: A Large-Scale African Face Image Database,&amp;rdquo; in IEEE Transactions on Information Forensics and Security, vol. 16, pp. 3634-3646, 2021, doi: 10.1109/TIFS.2021.3080496.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Z. Yan, L. He, Y. Wang, Z. Sun and T. Tan, &amp;ldquo;Flexible Iris Matching Based on Spatial Feature Reconstruction,&amp;rdquo; in IEEE Transactions on Biometrics, Behavior, and Identity Science, doi: 10.1109/TBIOM.2021.3108559.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;J. Wei, Y. Wang, Y. Li, R. He and Z. Sun, &amp;ldquo;Cross-spectral Iris Recognition by Learning Device-specific Band,&amp;rdquo; in IEEE Transactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2021.3117291.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;J. Hu, L. Wang, Z. Luo, Y. Wang and Z. Sun, &amp;ldquo;A Large-scale Database for Less Cooperative Iris Recognition,&amp;rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-6, doi: 10.1109/IJCB52358.2021.9484357.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Y. Ru, W. Zhou, Y. Liu, J. Sun and Q. Li, &amp;ldquo;Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection,&amp;rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484408.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;J. Wei, R. He and Z. Sun, &amp;ldquo;Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples,&amp;rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484388.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;L. Wang, K. Zhang, Y. Wang and Z. Sun, &amp;ldquo;An End-to-End Autofocus Camera for Iris on the Move,&amp;rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484340.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Y. Tian, K. Zhang, L. Wang and C. Zhang, &amp;ldquo;Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method,&amp;rdquo; 2021 IEEE International Joint Conference on Biometrics (IJCB), 2021, pp. 1-8, doi: 10.1109/IJCB52358.2021.9484402.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sun Z N, He R, Wang L, Kan M N, Feng J J, Zheng F, Zheng W S, Zuo W M, Kang W X, Deng W H, Zhang J, Han H, Shan SG, Wang Y L, Ru Y W, Zhu Y H, Liu Y F and He Y. 2021. Overview of biometrics research. Journal of Image and Graphics,26(06):1254-1329.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Luo Z., Li H., Wang Y., Wang Z., Sun Z. (2021) Iris Normalization Beyond Appr-Circular Parameter Estimation. In: Feng J., Zhang J., Liu M., Fang Y. (eds) Biometric Recognition. CCBR 2021. Lecture Notes in Computer Science, vol 12878. Springer, Cham. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-86608-2_35&#34;&gt;https://doi.org/10.1007/978-3-030-86608-2_35&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Learning Instance-level Spatial-Temporal Patterns for Person Re-identification</title>
      <link>https://nlpr-sir.github.io/publication/ren-iccv-2021/</link>
      <pubDate>Fri, 15 Oct 2021 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-iccv-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-spectral Iris Recognition by Learning Device-specific Band</title>
      <link>https://nlpr-sir.github.io/publication/wei-tcsvt-2021/</link>
      <pubDate>Mon, 04 Oct 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wei-tcsvt-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris Normalization Beyond Appr-Circular Parameter Estimation</title>
      <link>https://nlpr-sir.github.io/publication/luo-ccbr-2021/</link>
      <pubDate>Wed, 08 Sep 2021 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/luo-ccbr-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Flexible Iris Matching Based on Spatial Feature Reconstruction</title>
      <link>https://nlpr-sir.github.io/publication/zihui-yan-tbiom-2021/</link>
      <pubDate>Mon, 30 Aug 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/zihui-yan-tbiom-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Large-scale Database for Less Cooperative Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/hu-ijcb-2021/</link>
      <pubDate>Sat, 07 Aug 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/hu-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization</title>
      <link>https://nlpr-sir.github.io/publication/caiyong-ijcb-2021/</link>
      <pubDate>Tue, 20 Jul 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/caiyong-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization</title>
      <link>https://nlpr-sir.github.io/publication/kunbo-itifs-2023-ne/</link>
      <pubDate>Tue, 20 Jul 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/kunbo-itifs-2023-ne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Iris-Degradation</title>
      <link>https://nlpr-sir.github.io/dataset/casia-iris-degradation/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-iris-degradation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the outbreak of the COVID-19 pandemic, iris recognition has been used increasingly as contactless and unaffected by face masks. Although less user cooperation is an urgent demand for existing systems, corresponding manually annotated databases could hardly be obtained. This work presents a large-scale database of near-infrared iris images named CASIA-Iris-Degradation Version 1.0 (DV1), which consists of 15 subsets of various degraded images, simulating less cooperative situations such as illumination, off-angle, occlusion, and nonideal eye state. A lot of open-source segmentation and recognition methods are compared comprehensively on the DV1 using multiple evaluations, and the best among them are exploited to conduct ablation studies on each subset. Experimental results show that even the best deep learning frameworks are not robust enough on the database, and further improvements are recommended for challenging factors such as half-open eyes, off-angle, and pupil dilation. Therefore, we publish the DV1 with manual annotations online to promote iris recognition.&lt;/p&gt;
&lt;h2 id=&#34;description-of-casia-iris-degradation&#34;&gt;Description of CASIA-Iris-Degradation&lt;/h2&gt;
&lt;p&gt;CASIA-Iris-Degradation contains 36,539 images from 255 Asian people. All images were collected under NIR illumination and two eyes were captured simultaneously. Details of the proposed database are shown in the table below.
&lt;img src=&#34;./Statistics.png&#34; alt=&#34;Details of the proposed database&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;setup-of-image-collection&#34;&gt;Setup of image collection&lt;/h3&gt;
&lt;p&gt;As shown in the figure below, we built a collection room using black-out cloth. The curtain on one side of the room can be opened and closed artificially to control the natural light.
Inside, there were camera (A), light sources (B, C), volunteer (D), and four directional markers (1-4).
Each volunteer was asked to sit down, put their chin on the holder (0.75 m from the camera), keep their head as still as possible, and move their eyes according to instructions.
To simulate the off-angle situation, the subject was required to look along a set of directions indicated by four markers in the visual field, while in other cases look straight ahead (i.e., the midpoint of marker 1 and 4).
&lt;img src=&#34;./newcx1camera.png&#34; alt=&#34;environment and equipment&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-proposed-database&#34;&gt;Statistics of the proposed database&lt;/h3&gt;
&lt;p&gt;To simulate real image degradation, the proposed database is divided into four categories, and each of them is separated into three to five subsets as follows:&lt;/p&gt;
&lt;h4 id=&#34;illumination&#34;&gt;Illumination&lt;/h4&gt;
&lt;p&gt;The intensity of the VW light source was adjusted to four levels: Dark (0%), Weak (25%), Medium (50%), Strong (100%) to change the pupil size.
In addition, images under natural light were also collected (with Dark level).
Note that the intensity in other categories was set to the Medium level by default.&lt;/p&gt;
&lt;h4 id=&#34;off-angle&#34;&gt;Off-angle&lt;/h4&gt;
&lt;p&gt;There are four directions: (1) Left, (2) Upper left, (3) Upper right, (4) Right.
The left and right are in the horizontal direction, while the upper left and right angles are both 45 degrees.&lt;/p&gt;
&lt;h4 id=&#34;nonideal-eye-state&#34;&gt;Nonideal eye state&lt;/h4&gt;
&lt;p&gt;Since it is difficult to keep eyes open all the time, we collected images of closed, squinted, and half-open eyes.
Although most images of the former two classes have no effective iris region and are accompanied by blur, they can be used to train eye state detectors for fatigue driving detection or other relevant scenarios.&lt;/p&gt;
&lt;h4 id=&#34;occlusion&#34;&gt;Occlusion&lt;/h4&gt;
&lt;p&gt;For occlusion, volunteers were required to wear glasses, masks and using a hand to cover the mouth and nose. During this section, the NIR light source was randomly moved slightly to generate light spots. Meanwhile, some glasses also had stains on the surface to occlude the iris.
An unexpected observation is that for some elderly volunteers, their eyelids droop naturally, resulting in severe occlusion, which should arouse more attention.&lt;/p&gt;
&lt;p&gt;More samples and annotations are presented below.
&lt;img src=&#34;./supp_images.png&#34; alt=&#34;More samples&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA-Iris-Degradation database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Iris-Degradation-V1.0 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;quot;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Face-Africa: A Large-Scale African Face Image Database</title>
      <link>https://nlpr-sir.github.io/publication/jawad-tifs-2021/</link>
      <pubDate>Wed, 16 Jun 2021 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/jawad-tifs-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>生物特征识别学科发展报告</title>
      <link>https://nlpr-sir.github.io/publication/zhenan-joiag-2021/</link>
      <pubDate>Tue, 15 Jun 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/zhenan-joiag-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An End-to-End Autofocus Camera for Iris on the Move</title>
      <link>https://nlpr-sir.github.io/publication/wang-ijcb-2021/</link>
      <pubDate>Tue, 15 Jun 2021 08:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Evaluation Benchmark for High-throughput Iris Recognition at a Distance</title>
      <link>https://nlpr-sir.github.io/dataset/blurred_iris_benchmark/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/blurred_iris_benchmark/</guid>
      <description>&lt;h2 id=&#34;1-introduction&#34;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A key problem of iris recognition at a distance is that a large portion of captured iris images is nonideal because of narrow depth of field (DoF), noncooperative user movement, incongruous exposure time and so on. Current iris recognition systems usually filter out these low-quality images using strict criteria of image quality evaluation (IQA). However, this strategy inevitably leads to a waste of device capacity and low throughput. Therefore, a better and practical solution is to make the utmost of degraded iris images for personal identification. We announce the availability of a long-range captured dataset containing 3,756 iris images of various degradation factors from 98 subjects. An evaluation benchmark is built upon the dataset for a comparative study on preprocessing and recognition of NIR iris images in high-throughput scenarios. The datasets, manual annotations and evaluation toolkit are publicly available.&lt;/p&gt;
&lt;h2 id=&#34;2-descriptions-and-statistics-of-the-database&#34;&gt;&lt;strong&gt;2. Descriptions and Statistics of the Database&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;image-collection&#34;&gt;&lt;strong&gt;Image collection&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- &lt;iframe height=498 width=510 src=&#34;collection_glasses.mp4&#34;&gt; --&gt;
&lt;p&gt;The schematic and setup of blur-varying iris image collection of this database at a distance are shown as following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./capture_setting.png&#34; alt=&#34;Schematic and setup of NIR iris image collection at a distance and in less cooperative environments.&#34; title=&#34;Image Collection&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next generation of CASIA-LR-Cam bundled with NIR illumination at a wavelength of 830 nm was employed as the capturing device. Its standoff distance is approximately 1.2 meters with a DoF of over 20 centimeters. The field of view (FoV) is approximately 20 degrees. The device was placed in an indoor environment under no extra lighting sources. During the process of image collection, the subjects were obliged to move freely in the restricted square area 1.0~1.4 meters away from the device. Specifically, they could casually step forward and backward, left and right.&lt;/p&gt;
&lt;p&gt;While moving inside the restricted area, the subjects were guided by the indication signal on the GUI screen to look at different directions for approximately 30 seconds in a single session. Two separate sessions were launched in the daytime under the same conditions, and the interval was one week. If the subject was wearing glasses, he or she needed to take them off in either of the two sessions (&lt;code&gt;play the video and see&lt;/code&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 1 With Glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_glasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Session 2 No glasses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;video src=&#34;./collection_noglasses.mp4&#34; width=&#34;800px&#34; height=&#34;600px&#34; controls=&#34;controls&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;The acquired iris image sequences were captured at 5~10 frames per second. The resolution of each frame was 3840x2748. The frames in which irides were completely invisible caused by blinking or squinting were thrown away. Then evenly spaced images are extracted from the processed sequence every 5 frames. On average, approximately 20 images of each subject were retained.&lt;/p&gt;
&lt;h3 id=&#34;statistics-of-the-dataset&#34;&gt;&lt;strong&gt;Statistics of the dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attributes&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;The database&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Camera Type&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;CASIA-LR-Cam II&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Illumination&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;NIR and natural lighting sources&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total pixel&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3840x2748&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cropped eye region&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;640x480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sessions&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Two separate sessions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Institution of subjects&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Graduate students and staff of CASIA and TAfIRT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standoff distance&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.0~1.4m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Working mode&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Step freely within a moderate square area&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Depth of field&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ca. 20cm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of subjects&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Classes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No. of Images&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3,765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Images per class&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ca. 19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pairs of Images&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39,418 intraclass and 7,406,312 interclass&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;mannual-annotations&#34;&gt;&lt;strong&gt;Mannual Annotations&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Each image in the dataset is manually annotated with binary maps of iris masks, inner and outer iris boundaries, upper and lower eyelids, and sclera masks shown as below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./annotations.png&#34; alt=&#34;Two annotated instances in the dataset.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;evaluation-toolkit&#34;&gt;&lt;strong&gt;Evaluation Toolkit&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;IrisStat_V3.0.rar&#34;&gt;IrisStat_V3.0.rar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The package of evaluation toolkit is organized as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IrisIQA
│
└───config/
│
└───out/ 
│
└───utils/
|     computeMotionblur.m
|     computeSharpness.m
|     ini2struct.m
|     Integral.m
|     progressbar.m
│     struct2ini.m
|
└───MotionBlur_Main.m
│
└───Sharpness_Main.m

 
Segmentation
│
└───config/
│
└───out/ 
│
└───utils/
|    evalSeg.m
|    Hausdorff.m
|    ini2struct.m
|    progressbar.m
|    struct2ini.m 
│
└───IrisSeg_Main.m

Recognition
│
└───config/
│
└───out/ 
│
└───utils/
|     ACC.m
|     Bitshift.m
|     colors.mat
|     compute_iriscode_sim.m
|     compute_om_sim.m
|     compute_vector_sim.m
|     draw_CMC_curve.m
|     draw_DET_curve.m
|     EER.m
|     IdentiACC.m
|     linspecer.m
|     Merge_Multi_CMC_Curve.m
|     Merge_Multi_Det_Curve.m
|     plot_styles.mat
|     progressbar.m
|     VerfiACC.m
│
└───IrisRec_main.m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The main function of iris IQA evaluating sharpness is &lt;code&gt;Sharpness_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris segmentation evaluation is &lt;code&gt;IrisSeg_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The main function of iris recognition evaluation is &lt;code&gt;IrisRec_Main.m&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;&lt;strong&gt;Database Organization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database package comprises the following components organized in multiple directories.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|
circle_params
|  │
|  └───xxxL(R)_xx.ini
|
ellipse_params
|  │
|  └───xxxL(R)_xx.ini 
|
image
|  │
|  └───xxxL(R)_xx.jpg
|
iris_edge
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
iris_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
iris_mask
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
lower_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
pupil_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
pupil_mask
|  │
|  └───xxxL(R)_xx.png
|
sclera_mask
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge
|  │
|  └───xxxL(R)_xx.png
|
up_eyelids_edge_rough
|  │
|  └───xxxL(R)_xx.png
|
vis_result
|  │
|  └───xxxL(R)_xx.png
|
vis_result_new
|  │
|  └───xxxL(R)_xx.png
|
imgList.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file naming rule is &amp;ldquo;&lt;code&gt;xxxL(R)_xx&lt;/code&gt;&amp;rdquo;, where &amp;ldquo;&lt;code&gt;xxx&lt;/code&gt;&amp;rdquo; denotes the unique identifier of the subject, &amp;ldquo;&lt;code&gt;L&lt;/code&gt;&amp;rdquo; denotes left eye and &amp;ldquo;&lt;code&gt;R&lt;/code&gt;&amp;rdquo; denotes right eye and &amp;ldquo;&lt;code&gt;xx&lt;/code&gt;&amp;rdquo; denotes the index of the image in the class, e.g., &lt;code&gt;001L_01&lt;/code&gt;. All the filenames of the iris images and belonging classes are stored in &lt;code&gt;imgList.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;&lt;strong&gt;Copyright Note and Contacts&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the dataset collected by Smart Iris Recognition (SIR) group from the Chinese Academy of Sciences, Institute of Automation (CASIA)”.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, a non-student researcher must manually sign the 
&lt;a href=&#34;license_agreement.pdf&#34;&gt;License Agreement&lt;/a&gt; and agree to observe the restrictions. The signed document should be digitized and sent through email to: &lt;a href=&#34;mailto:sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 papers were accepted by IJCB2021</title>
      <link>https://nlpr-sir.github.io/post/ijcb2021accept/</link>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/ijcb2021accept/</guid>
      <description>&lt;p&gt;Over the past few months, the Program Committee and Area Chairs worked very hard to review each of the 164 papers submitted to IJCB 2021. The reviewers were carefully selected and assigned papers for review in their areas of expertise. A double-blind review policy was adopted. The review process resulted in the selection of the following 66 (40.2%) highly qualified papers to be included in the Technical Program.&lt;/p&gt;
&lt;p&gt;42: &lt;strong&gt;A Large-scale Database for Less Cooperative Iris Recognition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;44: &lt;strong&gt;Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;87: &lt;strong&gt;Contrastive Uncertainty Learning for Iris Recognition With Insufficient Labeled Samples&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;76: &lt;strong&gt;An End-to-End Autofocus Camera for Iris on the Move&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;117: &lt;strong&gt;Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NIR Iris Challenge Evaluation in Non-cooperative Environments: Segmentation and Localization (NIR-ISL 2021)</title>
      <link>https://nlpr-sir.github.io/post/nir-isl2021/</link>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/nir-isl2021/</guid>
      <description>&lt;p&gt;For iris recognition in non-cooperative environments, iris segmentation has been regarded as the first most important challenge still open to the biometric community, affecting all downstream tasks from normalization to recognition. In recent years, deep learning technologies have gained significant popularity among various computer vision tasks and have also affected the iris biometrics, especially iris segmentation. To investigate recent developments and attract more interests of researchers in the iris segmentation method, we are planning to host the challenge competition. In this challenge, we aim to benchmark the performance of iris segmentation on NIR iris images from Asian and African people captured in non-cooperative environments. Moreover, we specially split the general iris segmentation task in the conventional iris recognition pipeline into the segmentation of noise-free iris mask and the localization of inner and outer boundaries of the iris, which are narrowly referred to as iris segmentation and iris localization. Therefore, the challenge encourages the submission of a complete solution taking the iris segmentation and iris localization into consideration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Organisers&lt;/strong&gt;: Dr. Caiyong Wang, Dr. Yunlong Wang, Dr. Kunbo Zhang, Jawad Muhammad, Tianhao Lu, Prof. Qichuan Tian, Prof. Zhaofeng He, Prof. Zhenan Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preferred contact person&lt;/strong&gt;: Dr. Caiyong Wang, &lt;em&gt;wangcaiyong at bucea.edu.cn&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Website&lt;/strong&gt;: 
&lt;a href=&#34;https://sites.google.com/view/nir-isl2021/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/view/nir-isl2021/home&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Schedule&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Registration closes: April 20, 2021
Prediction results and technical reports submission deadline: April 20, 2021
Results announcement: April 30, 2021
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;More Competition&lt;/strong&gt;:
&lt;a href=&#34;http://ijcb2021.iapr-tc4.org/competitions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://ijcb2021.iapr-tc4.org/competitions/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;ll together 30 research groups registered for NIR-ISL 2021, out of which 14 took part in the final round and submitted a total of 27 valid models for scoring.&lt;/p&gt;
&lt;p&gt;According to our ranking rules, each submitted entry was assigned one ranking score per evaluation metric and set of testing data. The final ranking was then obtained by adding all 4(evaluation metrics)x5(datasets)(=20) ranking scores (rank sum). The entry with the smallest sum was placed top in the final ranking.&lt;/p&gt;
&lt;p&gt;The top-3 winning solutions of NIR-ISL 2021 are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;1st place: Lao Yang Sprint Team (Yiwen Zhang, Tianbao Liu, and Wei Yang, from School of Biomedical Engineering, Southern Medical University)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2nd place: SUEP-Pixsur (Dongliang Wu, Yingfeng Liu, Ruiye Zhou, and Huihai Wu, from Shanghai University of Electric Power)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;3rd place: EyeCool (Hao Zhang, Junbao Wang, Jiayi Wang, and Wantong Xiong, from College of Science, Northeastern University)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Congratulations to them!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Following are the team details and results. More details can be found in the future IJCB 2021 summary paper.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;result.png&#34; alt=&#34;details&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Avoiding Spectacles Reflections on Iris Images Using A Ray-tracing Method</title>
      <link>https://nlpr-sir.github.io/publication/yu-tian-ijcb-2021/</link>
      <pubDate>Wed, 07 Apr 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/yu-tian-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bita-Net: Bi-temporal Attention Network for Facial Video Forgery Detection</title>
      <link>https://nlpr-sir.github.io/publication/yiwei-ru-ijcb-2021/</link>
      <pubDate>Wed, 07 Apr 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/yiwei-ru-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contrastive Uncertainty Learning for Iris Recognition with Insufficient Labeled Samples</title>
      <link>https://nlpr-sir.github.io/publication/wei-ijcb-2021/</link>
      <pubDate>Wed, 07 Apr 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wei-ijcb-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview of biometrics research</title>
      <link>https://nlpr-sir.github.io/publication/sun-jig-2021/</link>
      <pubDate>Thu, 11 Mar 2021 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/sun-jig-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CASIA-Face-Africa</title>
      <link>https://nlpr-sir.github.io/dataset/casia-face-africa/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-face-africa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the years, numerous face databases have been published that brought about exciting breakthrough in the facial biometric research field, most especially from the recent trend of deep learning contributions. However, as investigated by researchers, most of these databases are demographically imbalanced and often contain few number of African cohorts. Of those with relatively large number of the Africans, the databases are usually wild (downloaded from the internet or digitalised from printed photographs). Methods that adopt these skewed databases often exhibits some form of performance bias that can result to unintended consequences for real time applications. As such, there is need for more demographically inclusive datasets. CASIA-Face-Africa is developed to provide solution to this problem. It is an all-African database that is made to be used as a complementary database with the existing databases to balance the number of the African cohorts in the published datasets and improve their demographic inclusiveness.&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;DESCRIPTION&lt;/h2&gt;
&lt;p&gt;The database images were captured at various locations in Nigeria, Africa. About 1150 volunteers participated in the capturing exercise. The images of each subject were captured concurrently using 3 cameras. Two visible wavelength (VW) cameras and one near-infrared (NIR) camera. The capturing was done in various sessions over a period of 3 months. Some of the subjects have images captured in multiple sessions while majority of the subjects have their images captured in a single session.
&lt;img src=&#34;1.png&#34; alt=&#34;Figure 1&#34;&gt;
&lt;em&gt;Figure 1: The cameras arrangement set-up was made to be same in all the capturing sessions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For each capturing instance, 3 to 10 still images were captured by each camera at a fixed time interval of 1 to 3 seconds. For some subjects, an external illumination light source was used for capturing additional images of that subjects. Also, some subjects were asked to use face accessory such as eye glasses for multiple capturing. The captured images were then organized and their land mark labelled. The organized database comprises a total of 38,546 images from 1,183 subjects. Specifically, 12,063 images captured by VW camera 1 at the resolution of 1332×1080, 13,232 images are captured by VW camera 2 at the resolution of 787 × 962, and 13,251 images are captured by NIR camera at the resolution of 983 × 877. Some samples of the captured images are shown in Figure 2, Figure 3 and Figure 4.
&lt;img src=&#34;2.png&#34; alt=&#34;Figure 2&#34;&gt;
&lt;em&gt;Figure 2: Sample of a single subject images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;3.png&#34; alt=&#34;Figure 3&#34;&gt;
&lt;em&gt;Figure 3: Sample of subject Expressions&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;4.png&#34; alt=&#34;Figure 4&#34;&gt;
&lt;em&gt;Figure 4: Sample of labelled face images&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The database package comprises of the following components organised in multiple directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images folder: This contain the actual face images. Each file is named as SubjectID_ImageNumber.jpg. The SubjectID is a unique number for each subject and the ImageNumber is a sequential number for images of the same subject.&lt;/li&gt;
&lt;li&gt;Subjects folder: This contain the corresponding ini files that describe the unique subjects in the images. Each file is named with the ID of the subject as SubjectID.ini.&lt;/li&gt;
&lt;li&gt;Attributes folder: This contain the corresponding ini files that describes each individual face image. Each file is named with its corresponding face image name as SubjectID_ImageNumber.ini&lt;/li&gt;
&lt;li&gt;Protocols folder: This contain the files named with each of the defined proposed protocols. Example of the names: ID-V-All-Ep1.ini, ID-I-Split-Ep3.ini, etc. Each of the files contains list of all the images allowed to be used for that protocol which are categorised as either Training, Testing, Target or Query as described in the database paper.&lt;/li&gt;
&lt;li&gt;Codes folder: These are evaluation codes in multiple programming languages that can be used to easily adopt the database for various applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Face-Africa collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;quot;.&lt;/p&gt;
&lt;p&gt;To receive a copy of the database, you can apply for it on our 
&lt;a href=&#34;http://www.idealtest.org/#/datasetDetail/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIT website&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] Muhammad Jawad, Yunlong Wang, Caiyong Wang, Kunbo Zhang, Zhenan Sun. “CASIA-Face-Africa: A Large-scale African Face Image Database,” IEEE Transactions on Information Forensics and Security (TIFS), vol.16, pp. 3634-3646, 2021.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Kunbo Zhang Wins IJCB 2020 Best Paper Award Runner-Up</title>
      <link>https://nlpr-sir.github.io/post/ijcb2020award/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/ijcb2020award/</guid>
      <description>&lt;p&gt;Zhang published paper, 
&lt;a href=&#34;../../publication/zhang-ijcb2020&#34;&gt;All-in-Focus Iris Camera With a Great Capture Volume&lt;/a&gt;, wins the PC chairs choice best paper award runner-Up at 
&lt;a href=&#34;https://ieee-biometrics.org/ijcb2020/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the 2020 International Joint Conference on Biometrics (IJCB 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;paper.png&#34; alt=&#34;All-in-Focus Iris Camera With a Great Capture Volume&#34;&gt;&lt;/p&gt;
&lt;p&gt;The 2020 International Joint Conference on Biometrics (IJCB 2020) combines two major biometrics research conferences, the Biometrics Theory, Applications and Systems (BTAS) conference and the International Conference on Biometrics (ICB). The blending of these two conferences in 2020 is through a special agreement between the IEEE Biometrics Council and the IAPR TC-4, and should present an exciting event for the entire worldwide biometrics research community.&lt;/p&gt;
&lt;p&gt;In this work, a novel all-in-focus iris imaging system is developed. It using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth offield extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;camera.png&#34; alt=&#34;all-in-focus iris imaging system&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSBC 2020: Sclera Segmentation Benchmarking Competition in the Mobile Environment</title>
      <link>https://nlpr-sir.github.io/publication/vitek-ijcb-2020/</link>
      <pubDate>Mon, 28 Sep 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/vitek-ijcb-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feel free to contact us for academic communication: 1047450724@qq.com</title>
      <link>https://nlpr-sir.github.io/post/welcome/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/welcome/</guid>
      <description>&lt;h2 id=&#34;smart-iris-recognition-sir-group-is-affiliated-tocenter-for-research-on-intelligent-perception-and-computing-cripacinstitute-of-automation-chinese-academy-of-sciences-casia-led-by-proftieniu-tanand-profzhenan-sun-sir-group-concentrates-on-cutting-edge-research-of-ocular-biometrics-wherein-the-main-focus-is-the-whole-pipeline-of-iris-recognition-the-topics-basically-include-the-trial-manufacture-of-iris-imaging-device-the-control-of-iris-image-acquisition-iris-image-preprocessing-iris-feature-encoding-and-matching-sclera-recognition-periocular-recognition-and-multi-modal-fusion-are-also-covered-in-the-research-topics-for-developing-more-advanced-sir-systems-over-the-years-the-members-of-sir-group-are-striving-to-conquer-the-key-and-difficult-problems-in-non-cooperative-iris-recognition-at-a-distance-under-complex-scenarios-promoting-the-applications-of-sir-systems-in-surveillance-robotics-and-uavs-we-are-open-for-academic-communication-and-feel-free-to-contact-us-1047450724qqcom&#34;&gt;Smart Iris Recognition (SIR) group is affiliated to Center for Research on Intelligent Perception and Computing (CRIPAC), Institute of Automation, Chinese Academy of Sciences (CASIA), led by Prof. Tieniu Tan and Prof. Zhenan Sun. SIR group concentrates on cutting-edge research of ocular biometrics, wherein the main focus is the whole pipeline of iris recognition. The topics basically include the trial-manufacture of iris imaging device, the control of iris image acquisition, iris image preprocessing, iris feature encoding and matching. Sclera recognition, periocular recognition and multi-modal fusion are also covered in the research topics for developing more advanced SIR systems. Over the years, the members of SIR group are striving to conquer the key and difficult problems in non-cooperative iris recognition at a distance under complex scenarios, promoting the applications of SIR systems in surveillance, robotics and UAVs. We are open for academic communication and feel free to contact us: &lt;a href=&#34;mailto:1047450724@qq.com&#34;&gt;1047450724@qq.com&lt;/a&gt;&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Iris Recognition</title>
      <link>https://nlpr-sir.github.io/project/iris-recognition/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/project/iris-recognition/</guid>
      <description>&lt;div style=&#34;WIDTH:100%; HEIGHT:100%&#34;&gt;
&lt;font color=#00BFFF size = 5 &gt;&lt;strong&gt; &amp;emsp;&amp;emsp;虹膜识别是进一步提升安全性、可靠性的必由之路&lt;/strong&gt; &amp;emsp;&amp;emsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/font&gt;
&lt;font color=#00BFFF size = 5 &gt;&lt;strong&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;———谭铁牛院士&lt;/strong&gt;&lt;/font&gt;  
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;虹膜识别是最具潜力的生物识别方法之一，是识别率高、非接触性、防欺骗性好的识别方法。虹膜属于人眼的一部分，如下图所示。&lt;/font&gt;
&lt;div style=&#34;float:right; clear: both;&#34; align=&#34;center&#34;&gt;&lt;img src=&#34;iris.png&#34; width=&#34;300&#34; alt=&#34;&#34; hspace=&#34;8&#34;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;人眼的外观主要由巩膜、虹膜、瞳孔三部分组成，其中巩膜即眼球外围的白色部分，约占人眼总面积的30%，眼睛的中心为瞳孔部分，约占5%，虹膜位于巩膜和瞳孔之间，约占整个眼睛的65%，包含了最丰富的纹理信息，外观上看，虹膜由许多腺窝、褶皱、色素斑等构成，是人体中最独特的结构之一。因为瞳孔、虹膜和巩膜一般颜色不同，灰度值呈梯度变化，所以根据它们灰度不同，可以将它们明显分开。从几何形状可以看出，虹膜的内、外边界可以近似为圆形，这使它具有易检测性。临床观察发现：虹膜在人的一生当中几乎不发生变化，只有很少的虹膜纹理可能会由于年龄或者外伤导致纹理破坏。&lt;/font&gt;&lt;/div&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;作为表示个人身份的标识物，必须具备作为身份标识的重要特征。人脸、指纹等许多生物特征具有作为身份标识的特性，但是，虹膜在这些特性方面表现的更为突出，具有许多先天优势，是其他生物特征无法与之媲美的。&lt;/font&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&lt;ul type=&#39;circle&#39;&gt;&lt;li&gt;普遍性————虹膜是每个人天生都具有的。&lt;/li&gt;&lt;li&gt;唯一性————虹膜的纤维组织细节复杂而丰富，每个人错综复杂的虹膜独一无二，只与虹膜的形成过程有关。&lt;/li&gt;&lt;li&gt;稳定性————虹膜从婴儿胚胎发育的第三个月起开始发育，到第八个月虹膜的主要纹理结构已经成形。&lt;/li&gt;&lt;li&gt;非入侵检测————从一定距离即可获得虹膜数字图像，无需用户接触设备。&lt;/li&gt;&lt;li&gt;可接受程度好————虹膜识别以其认证准确度高、速度快、安全性高，被用户所接受。&lt;/li&gt;&lt;li&gt;可检测性————利用图像处理技术检测出虹膜边界，易于拟合分割和和归一化&lt;/li&gt;&lt;li&gt;防伪性高————虹膜的半径小，在可见光下中国人的虹膜图像呈现深褐色，看不到纹理信息，需要虹膜图像专业采集设备和用户的配合，所以一般情况下很难被盗取&lt;/li&gt;&lt;li&gt;防欺骗性好————虹膜的唯一性决定了不同人眼的虹膜很难被冒充模仿。&lt;/li&gt;&lt;/ul&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;生物特征识别通过捕获生物样本，然后采用数学方法把样本转化成相同大小的模板，提取有效的可区别性特征，就可以客观地和其他一个完整的虹膜身份识别系统主要由四个部分组成：虹膜图像获取、虹膜图像预处理、虹膜特征提取、模式比对。
&lt;/font&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&lt;ol&gt;&lt;li&gt;虹膜图像获取&lt;/li&gt;  
&amp;emsp;虹膜图像采集的目的是为了获取有效的虹膜图像，在传统的虹膜识别场景中，通常采用专业的成像装置在近红外光（波长700nm到900nm）照射和用户的配合下才能捕获清晰的高分辨率虹膜图像。近些年来，随着光学镜头、传感器和计算成像技术等的发展，虹膜识别的可用距离不断变大，相关装置也变得越来越轻巧实用，“远距离”、“行进中”、“移动端”和“可见光下”等少约束场景的虹膜识别对于用户使用时的约束越来越少，极大地提升了虹膜识别应用范围和用户友好性。
&lt;li&gt;虹膜图像预处理&lt;/li&gt;
虹膜图像除了必须的虹膜区域以外，也包含了诸如巩膜、睫毛、瞳孔等非虹膜区域，因此不能直接用于虹膜识别。其次，一些噪声茹照明变化、睫毛遮挡、镜面反射、瞳孔放缩等会明显增加虹膜的类内差异，降低虹膜的识别率。常规的虹膜预处理步骤包括：虹膜活体检测、虹膜图像质量评估、虹膜分割、虹膜归一化和虹膜图像增强。
&lt;li&gt;虹膜图像特征分析&lt;/li&gt;
虹膜图像特征分析主要包含两部分：特征提取和对比分类。虹膜特征提取是指从归一化的虹膜图像中提取紧凑有区分的虹膜特征，然后使用计算机可以存储和读取的格式进行编码。虹膜比对和分类(或者称为匹配)是指将提取的虹膜特征编码和事先在数据库注册过的虹膜特征编码通过某种相似性度量比如汉明距离、余弦距离进行对比，计算相似性分数，依次确定用户身份。
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Alignment Free and Distortion Robust Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/afdr-iris-recognition/afdr-iris-recognition/</link>
      <pubDate>Thu, 20 Aug 2020 07:42:58 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/afdr-iris-recognition/afdr-iris-recognition/</guid>
      <description>&lt;!-- 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/# academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>MediaPipe Iris</title>
      <link>https://nlpr-sir.github.io/post/mediapipe-iris/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/mediapipe-iris/</guid>
      <description>&lt;!--&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;10&#39;&gt;MediaPipe Iris 实时虹膜跟踪和深度估计&lt;/font&gt;&lt;/div&gt;    
&lt;div style=&#39;display:none&#39;&gt;
标题居中
&lt;/div&gt;--&gt;
 &lt;font size=&#39;5&#39;&gt;
 &amp;emsp;&amp;emsp;包括计算摄影（例如，人像模式和闪光反射）和增强现实效果（例如，虚拟化身）在内的大量实际应用程序都依赖于通过跟踪虹膜来估计眼睛位置。一旦获得了准确的虹膜跟踪，我们就可以确定从相机到用户的距离，而无需使用专用的深度传感器。反过来，这可以改善各种用例，从计算摄影到适当大小的眼镜和帽子的虚拟试戴，到根据视听者的距离采用字体大小的可用性增强。
 由于有限的计算资源，可变的光照条件和遮挡物（例如头发或人斜视）的存在，虹膜跟踪是在移动设备上解决的一项艰巨任务。通常，会使用复杂的专用硬件，从而限制了可在其中应用该解决方案的设备范围。
 &lt;/font&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://pic1.zhimg.com/v2-abc83ffe57801610f81c22845ec1a937_b.gif&#34; alt=&#34;show&#34; width=&#34;650&#34; height=&#34;230&#34; /&gt;
&lt;/div&gt;
&lt;div align=&#39;center&#39;&gt;由MediaPipe Iris实现的眼睛重新着色示例&lt;/div&gt; 
&lt;/div&gt;
 &lt;font size=&#39;5&#39;&gt;
&amp;emsp;&amp;emsp;谷歌日前发布了用于精确虹膜估计的全新机器学习模型：MediaPipe Iris。所述模型以MediaPipe Face Mesh的研究作为基础，而它无需专用硬件就能够通过单个RGB摄像头实时追踪涉及虹膜，瞳孔和眼睛轮廓的界标。利用虹膜界标，模型同时能够在不使用深度传感器的情况下以相对误差小于10％的精度确定对象和摄像头之间的度量距离。请注意，虹膜追踪不会推断人们正在注视的位置，同时不能提供任何形式的身份识别。MediaPipe是一个旨在帮助研究人员和开发者构建世界级机器学习解决方案与应用程序的开源跨平台框架，所以在MediaPipe中实现的这一系统能够支持大多数现代智能手机，PC，笔记本电脑，甚至是Web。
&lt;div&gt;
&lt;div align=center&gt;
&lt;img src=&#34;http://p3.itc.cn/q_70/images03/20200808/b4ad6d1e27c5402c9f6752ce39d9a24a.gif&#34; alt=&#34;show&#34; width=&#34;240&#34; height=&#34;270&#34;&gt;
&lt;/div&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;!-- 用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定--&gt; 
&lt;p&gt;&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;1. 用于虹膜追踪的机器学习管道&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size=&#39;5&#39;&gt;  谷歌介绍道，开发系统的第一步利用了之前针对3D Face Meshes的研究，亦即通过高保真面部界标来生成近似面部几何形状的网格。根据所述网格，研究人员分离出原始图像中的眼睛区域以用于虹膜追踪模型。然后，谷歌将问题分为两个部分：眼睛轮廓估计和虹膜位置。他们设计了一个由一元化编码器组成的多任务模型，每个组件对应一个任务，这样就能够使用特定于任务的训练数据。&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;http://p3.itc.cn/q_70/images03/20200808/462869a0c5844bc28f5c89f65479dd02.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;300&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;/div&gt; 
&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;为了将裁剪后的眼睛区域用于模型训练，团队手动注释了大约50万张图像。其中，图像涵盖了不同地理位置的各种照明条件和头部姿势，如下所示
&lt;div align=center&gt;
&lt;img src=&#34;d07b74a8b05a4721b72eb12c1a81f383.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;200&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;d9331ef1d8864e79a8288a210f869815.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;260&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;裁剪的眼睛区域形成模型的输入，而它将通过单独的组件预测界标&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;2. 虹膜深度：用单个图像进行深度估计&lt;/font&gt;&lt;/strong&gt;
&lt;font size=&#39;5&#39;&gt;  无需任何专门的硬件，这个虹膜追踪模型能够以不到10％的误差确定对象到摄像头的度量距离。相关的原理事实是，人眼的水平直径虹膜基本恒定为11.7±0.5毫米。作为说明，请想象将针孔摄像头模型投影到正方形像素的传感器。你可以使用摄像头的焦距估计从面部界标到对象的距离，而这可以通过Camera Capture API或直接从捕获图像的EXIF元数据，以及其他摄像头固有参数进行获取。给定焦距，对象到摄像头的距离与对象眼睛的物理尺寸成正比，如下图所示&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;366c2353e9654a7998c87ac32c491daf.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;500&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;利用类似的三角形，我们可以根据焦距（f）和虹膜大小来计算对象的距离（d）&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;http://p7.itc.cn/q_70/images03/20200808/0ba5001e03ca4c42a17036b97ed0326b.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;260&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;左边：在Pixel 2运行的MediaPipe Iris正在以cm为单位估计度量距离，没有采用任何深度摄像头；右边：groud-truth深度&lt;/div&gt; 
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;为了量化所述方法的精确性，研究人员收集了200多位被试的正向同步视频和深度图像，并将其与iPhone 11的深度传感器进行比较。团队使用激光测距设备，通过实验确定iPhone 11的深度传感器在2米以内的误差小于2％。对于使用虹膜大小进行深度估算的方法，平均相对误差为4.3％，标准偏差是2.4％。谷歌对有眼镜被试和正常视力被试（不计入隐形眼镜情况）测试了所述方法，并发现眼镜会将平均相对误差略微提高到4.8％（标准偏差是3.1％）。另外，实验没有测试存在任何眼睛疾病的被试。考虑到MediaPipe Iris不需要专门的硬件，所述结果表明系统能够支持一系列成本范围的设备根据单张图像获取度量深度
&lt;div align=center&gt;
&lt;img src=&#34;f49af869c2c34877a244ab791ee17b27.png&#34; alt=&#34;show&#34; width=&#34;500&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;估计误差的直方图（左边），以及实际和估计距离的比较（右边）&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;3. 发布MediaPipe Iris&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;这个虹膜和深度估计模型将作为支持PC，移动设备和Web的跨平台MediaPipe管道发布。正如谷歌在最近一篇关于MediaPipe的博文所述，团队利用WebAssembly和XNNPACK在浏览器中本地运行Iris ML管道，无需将任何数据发送到云端。
&lt;div align=center&gt;
&lt;img src=&#34;http://p7.itc.cn/q_70/images03/20200808/6974ce3508e94a1c87418733ba3a3928.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;使用MediaPipe的WASM堆栈。你可以在浏览器种运行模型&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;http://p6.itc.cn/q_70/images03/20200808/ede608441d4141629bf58a576ed495ba.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;仅使用包含EXIF数据的单张图片计算虹膜深度&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;4. 未来方向&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;谷歌计划进一步扩展MediaPipe Iris模型，实现更稳定的追踪性能以降低误差，并将其部署用于无障碍用例。谷歌在相关文档和随附的Model Card中详细说明了预期的用途，限制和模型的公平性，从而确保模型的使用符合谷歌的AI原则。请注意，任何形式的监视监控都明显超出应用范围，故不予支持。团队表示：“我们希望的是，通过向广泛的研究与开发社区提供这种虹膜感知功能，从而促使创造性用例的出现，激发负责任的新应用和新研究途径。”</description>
    </item>
    
    <item>
      <title>How does iris recognize identity successfully?</title>
      <link>https://nlpr-sir.github.io/post/how-does-iris-recognize-identity-successfully/</link>
      <pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/how-does-iris-recognize-identity-successfully/</guid>
      <description>&lt;p&gt;  8月5日晚芒果台，中科院自动化研究所智能感知与计算研究中心助理研究员王云龙老师带你探索虹膜识别的奥秘&lt;/p&gt;
&lt;iframe frameborder=&#34;0&#34; width=&#34;720px&#34; height=&#34;480px&#34; src=&#34;https://www.mgtv.com/s/9538853.html&#34; allowFullScreen=&#34;true&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Academician introduces you to iris recognition</title>
      <link>https://nlpr-sir.github.io/post/academician-introduces-you-to-iris-recognition/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/post/academician-introduces-you-to-iris-recognition/</guid>
      <description>&lt;p&gt;  7月13日播出的CCTV-1《生活圈》节目中，谭铁牛院士现身为观众介绍了虹膜识别技术。谭铁牛院士介绍到：“虹膜识别是一种相对比较新颖的生物特征识别技术，下一步虹膜识别技术会进一步朝着移动化、便捷化以及和其他的相关的生物特征识别技术，比如人脸识别技术，相融合的方向发展，具有非常广阔的发展空间。”&lt;/p&gt;
&lt;iframe frameborder=&#34;0&#34; width=&#34;720px&#34; height=&#34;480px&#34; src=&#34;20200713_17347661f3d_r29_800k.mp4&#34; allowFullScreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&amp;emsp;&amp;emsp;自动化所孙哲南研究员也携团队相关成果做客节目，具体讲解了虹膜识别技术的优势与应用。
&lt;div&gt;  
&lt;div align=&#39;center&#39;&gt;
&lt;font color=#0099ff size=6 face=&#34;黑体&#34;&gt;虹膜识别技术的优势&lt;/font&gt;
&lt;/div&gt;
&amp;emsp;&amp;emsp;虹膜识别是利用人眼表面黑色瞳孔和白色巩膜之间圆环状的区域进行身份识别的技术。虹膜识别的优势在于： 
&lt;p&gt;  第一，虹膜先天具有非常高的唯一性。虹膜中可以发现证明至少244个独立变量来决定其唯一性，而指纹和人脸大概只有十几个或者几十个这样的变量。&lt;br&gt;
  第二，虹膜终身不变。年龄的增长、化妆或者整容可以改变人的容貌，却无法改变虹膜&lt;/p&gt;
&lt;div&gt;
&lt;div align=&#39;center&#39;&gt;
&lt;font color=#0099ff size=6 face=&#34;黑体&#34;&gt;虹膜识别的应用&lt;/font&gt; 
&lt;/div&gt;  
&lt;p&gt;&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;1. 虹膜识别应用于手机&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  孙哲南研究员在节目中展示了团队研发的虹膜识别解锁手机，在手机终端装载虹膜识别模块，直接刷眼就可以解锁手机。防护镜、墨镜甚至黑暗的环境都不会成为虹膜识别的阻碍。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;2. 虹膜识别应用于电脑&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  使用虹膜解锁电脑，刷眼后一瞬间即可安全登陆，省去了总是忘记密码与密码被盗的烦恼。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;3. 虹膜识别防盗门锁&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  只需对准虹膜采集框，即可解锁开门。团队展示的虹膜锁采用近红外主动光源成像，即使在光线很暗的楼道内，虹膜锁也可以正常工作。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;4. 虹膜识别收费闸机&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  想象一下，当我们驾车通过收费闸机时，只需要刷一下眼睛，就可以自动收费抬杆，这是一种什么样的感觉呢？将来，这一系统也可以应用于高速公路ETC中，驾驶员就可以直接通过眼神识别进行缴费。&lt;br&gt;
  &lt;strong&gt;其实，虹膜识别的应用远不止这些，并且在不远的将来，它还可以在更多地方得以运用，为人们的生活提供超乎想象的便利！&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-IrisV4</title>
      <link>https://nlpr-sir.github.io/dataset/casia-irisv4/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/dataset/casia-irisv4/</guid>
      <description>&lt;h2 id=&#34;download-the-whole-database-186gbhttpbiometricsidealtestorgdownloaddbdoid4&#34;&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download the whole database (1.86GB)&lt;/a&gt;&lt;/h2&gt;
&lt;p style=&#34;text-align: center&#34;&gt;
OR&lt;br&gt;
Download the separated subsets below&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Interval (30.9MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Lamp (390MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Twins (60MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Distance(767MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Thousand (490MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Syn (171MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;With the pronounced need for reliable personal identification, iris recognition has become an important enabling technology in our society. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. Automatic iris recognition has to face unpredictable variations of iris images in real-world applications. For example, recognition of iris images of poor quality, nonlinearly deformed iris images, iris images at a distance, iris images on the move, and faked iris images all are open problems in iris recognition. A basic work to solve the problems is to design and develop a high quality iris image database including all these variations. Moreover, a novel iris image database may help identify some frontier problems in iris recognition and leads to a new generation of iris recognition technology.&lt;/p&gt;
&lt;p&gt;CASIA Iris Image Database (CASIA-Iris) developed by our research group has been released to the international biometrics community and updated from CASIA-IrisV1 to CASIA-IrisV3 since 2002. More than 3,000 users from 70 countries or regions have downloaded CASIA-Iris and much excellent work on iris recognition has been done based on these iris image databases. Although great progress of iris recognition has been achieved since 1990s, the rapid growth of iris recognition applications has clearly highlighted two challenges, i.e. usability and scalability.&lt;/p&gt;
&lt;p&gt;Usability is the largest bottleneck of current iris recognition. It is a trend to develop long-range iris image acquisition systems for friendly user authentication. However, iris images captured at a distance are more challenging than traditional close-up iris images. Lack of long-range iris image data in the public domain has hindered the research and development of next-generation iris recognition systems.&lt;/p&gt;
&lt;p&gt;Most current iris recognition methods have been typically evaluated on medium sized iris image databases with a few hundreds of subjects. However, more and more large-scale iris recognition systems are deployed in real-world applications. Many new problems are met in classification and indexing of large-scale iris image databases. So scalability is another challenging issue in iris recognition.&lt;/p&gt;
&lt;p&gt;In order to promote research on long-range and large-scale iris recognition systems,  we are pleased to release to the public domain CASIA Iris Image Database V4.0 (or CASIA-IrisV4 for short).&lt;/p&gt;
&lt;h2 id=&#34;2-brief-descriptions-and-statistics-of-the-database&#34;&gt;2. Brief Descriptions and Statistics of the Database&lt;/h2&gt;
&lt;p&gt;CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn.&lt;/p&gt;
&lt;p&gt;CASIA-IrisV4 contains a total of 54,601 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.&lt;/p&gt;
&lt;h3 id=&#34;21--casia-iris-interval&#34;&gt;2.1  CASIA-Iris-Interval&lt;/h3&gt;
&lt;p&gt;Iris images of CASIA-Iris-Interval were captured with our self-developed close-up iris camera (Fig.1). The most compelling feature of our iris camera is that we have designed a circular NIR LED array, with suitable luminous flux for iris imaging. Because of this novel design, our iris camera can capture very clear iris images (see Fig.2). CASIA-Iris-Interval is well-suited for studying the detailed texture features of iris images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.1.jpg&#34; alt=&#34;Fig.1&#34;&gt;
Fig.1 The self-developed iris camera used for collection of CASIA-Iris-Interval
&lt;img src=&#34;./V4Fig.2.jpg&#34; alt=&#34;Fig.2&#34;&gt;
Fig.2 Example iris images in CASIA-Iris-Interval&lt;/p&gt;
&lt;h3 id=&#34;22--casia-iris-lamp&#34;&gt;2.2  CASIA-Iris-Lamp&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Lamp was collected using a hand-held iris sensor produced by OKI (Fig.3). A lamp was turned on/off close to the subject to introduce more intra-class variations when we collected CASIA-Iris-Lamp. Elastic deformation of iris texture (Fig.4) due to pupil expansion and contraction under different illumination conditions is one of the most common and challenging issues in iris recognition. So CASIA-Iris-Lamp is good for studying problems of non-linear iris normalization and robust iris feature representation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.3.jpg&#34; alt=&#34;Fig.3&#34;&gt;
Fig.3 The hand-held iris camera used for collection of CASIA-Iris-Lamp
&lt;img src=&#34;./V4Fig.4.jpg&#34; alt=&#34;Fig.4&#34;&gt;
Fig.4 Example iris images in CASIA-Iris-Lamp&lt;/p&gt;
&lt;h3 id=&#34;23--casia-iris-twins&#34;&gt;2.3  CASIA-Iris-Twins&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Twins contains iris images of 100 pairs of twins, which were collected during Annual Twins Festival in Beijing using OKI&amp;rsquo;s IRISPASS-h camera (Fig.5). Although iris is usually regarded as a kind of phenotypic biometric characteristics and even twins have their unique iris patterns, it is interesting to study the dissimilarity and similarity between iris images of twins.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.5.jpg&#34; alt=&#34;Fig.5&#34;&gt;
Fig.5 Example iris images in CASIA-Iris-Twins&lt;/p&gt;
&lt;h3 id=&#34;24--casia-iris-distance&#34;&gt;2.4  CASIA-Iris-Distance&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Distance contains iris images captured using our self-developed long-range multi-modal biometric image acquisition and recognition system (LMBS, Fig.6). The advanced biometric sensor can recognize users from 3 meters away by actively searching iris, face or palmprint patterns in the visual field via an intelligent multi-camera imaging system. The LMBS is human-oriented by fusing computer vision, human computer interaction and multi-camera coordination technologies and improves greatly the usability of current biometric systems. The iris images of CASIA-Iris-Distance were captured by a high resolution camera so both dual-eye iris and face patterns are included in the image region of interest (Fig. 7). And detailed facial features such as skin pattern are also visible for multi-modal biometric information fusion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.6.jpg&#34; alt=&#34;Fig.6&#34;&gt;
Fig.6  The biometric sensor used for collection of CASIA-Iris-Distance
&lt;img src=&#34;./V4Fig.7.jpg&#34; alt=&#34;Fig.7&#34;&gt;
Fig.7  An example image in CASIA-Iris-Distance&lt;/p&gt;
&lt;h3 id=&#34;25--casia-iris-thousand&#34;&gt;2.5  CASIA-Iris-Thousand&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Fig. 8) produced by 
&lt;a href=&#34;Http://www.irisking.com&#34;&gt;IrisKing&lt;/a&gt;. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.8.jpg&#34; alt=&#34;Fig.8&#34;&gt;
Fig.8 The iris camera used for collection of CASIA-Iris-Thousand
&lt;img src=&#34;./V4Fig.9.jpg&#34; alt=&#34;Fig.9&#34;&gt;
Fig.9  An example image in CASIA-Iris-Thousand&lt;/p&gt;
&lt;h3 id=&#34;26--casia-iris-syn&#34;&gt;2.6  CASIA-Iris-Syn&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in [1] (Fig. 10). Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in [1] that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.10.jpg&#34; alt=&#34;Fig.10&#34;&gt;
Fig. 10  Flowchart of the iris texture synthesis method for generation of CASIA-Iris-Syn
&lt;img src=&#34;./V4Fig.11.jpg&#34; alt=&#34;Fig.11&#34;&gt;
Fig. 11  Example iris images in CASIA-Iris-Syn&lt;/p&gt;
&lt;h2 id=&#34;3-database-organization&#34;&gt;3. Database Organization&lt;/h2&gt;
&lt;p&gt;The file name of each image in CASIA-IrisV4 is unique to each other and denotes some useful properties associated with the image such as subset category, left/right/double, subject ID, class ID, image ID etc. The file naming rules of all six subsets are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Interval are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Interval/YYY/S1YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Lamp are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Lamp/YYY/E/S2YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Twins are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Twins\XX\YE\S3XXYENN.jpg&lt;/p&gt;
&lt;p&gt;XX: the index of family&lt;/p&gt;
&lt;p&gt;Y: the identifier to one of the twins&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Distance are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Distance/YYY/S4YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘D’ denotes dual-eye iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Thousand are stored as:&lt;/p&gt;
&lt;p&gt;$ root path$ /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Syn are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Syn/YYY/S6YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘S’ denotes it is a synthesized iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-copyright-note-and-contacts&#34;&gt;4. Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-IrisV4 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)” and a reference to “CASIA Iris Image Database, &lt;a href=&#34;http://biometrics.idealtest.org/&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included. A copy of all reports and papers that are for public or general release that use the CASIA-IrisV4 should be forwarded upon release or publication to:&lt;/p&gt;
&lt;p&gt;Professor Tieniu Tan&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;or send electronic copies to &lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Questions regarding this database can be addressed to Dr. Zhenan Sun at&lt;/p&gt;
&lt;p&gt;Dr. Zhenan Sun&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;Tel: +86 10 8261 0278&lt;/p&gt;
&lt;p&gt;Fax: +86 10 6255 1993&lt;/p&gt;
&lt;p&gt;Email: &lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;publications&#34;&gt;Publications&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tieniu Tan, Zhaofeng He, Zhenan Sun, &amp;ldquo;Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition&amp;rdquo;, Image and Vision Computing, Vol.28, No. 2, 2010, pp.223-230.&lt;/li&gt;
&lt;li&gt;T. Tan and L. Ma, “Iris Recognition: Recent Progress and Remaining Challenges”, Proc. of SPIE, Vol. 5404, pp. 183-194, 12-13 Apr 2004, Orlando, USA.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, &amp;ldquo;Ordinal Measures for Iris Recognition,&amp;rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 12, 2009, pp. 2211 - 2226.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, &amp;ldquo;Towards Accurate and Fast Iris Segmentation for Iris Biometrics&amp;rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 9, 2009, pp.1670 - 1684.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, Y. Wang and D. Zhang, “Personal Identification Based on Iris Texture Analysis”, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Vol. 25, No. 12, pp.1519-1533, 2003.&lt;/li&gt;
&lt;li&gt;Li Ma, Tieniu Tan, Yunhong Wang and Dexin Zhang, “Efficient Iris Recognition by Characterizing Key Local Variations”, IEEE Trans. on Image Processing, Vol. 13, No.6, pp. 739- 750, 2004.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, D. Zhang and Y. Wang, “Local Intensity Variation Analysis for Iris Recognition, Pattern Recognition”, Vol.37, No.6, pp. 1287-1298, 2004.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, IEEE Transactions on Systems, Man, and Cybernetics-Part Cï¼ŒVolume 35, Issue 3, 2005, pp.435 - 441.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition”, Proceedings of International Workshop on Biometric Authentication (BioAW), Lecture Notes in Computer Science, Vol.3087, 2004, pp. 270-282.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 418-425.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Robust Direction Estimation of Gradient Vector Field for Iris Recognition”, Proceedings of the 17th International Conference on Pattern Recognition, Vol.2, 2004, pp.783-786.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Cascading Statistical And Structural Classifiers For Iris Recognition”, Proceedings of IEEE International Conference on Image Processing, 2004, pp.1261-1264.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Iris Recognition Based on Non-local Comparisons”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 67-77.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, and Xianchao Qiu, &amp;ldquo;Graph Matching Iris Image Blocks with Local Binary Pattern&amp;rdquo;, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 366-372.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, “Robust and Fast Assessment of Iris Image Quality”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 464 - 471.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “An Appearance-Based Method for Iris Detection”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp.1091-1096, 2004, Korea.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Junzhou Huang, Tieniu Tan, Zhenan Sun and Li Ma, “An Iris Image Synthesis Method Based on PCA and Super-Resolution”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 4, pp. 471-474, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “A Fast and Robust Iris Localization Method Based on Texture Segmentation”, Proc. of SPIE, Vol. 5404, pp. 401-408, 2004, USA.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Li Ma, Tieniu Tan and Zhenan Sun, “An Iris Recognition Algorithm Using Local Extreme Points”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 442-449.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Tieniu Tan and Zhenan Sun, “Fast Recursive Mathematical Morphological Transforms”, Proc. of the 3rd International Conference on Image and Graphics (ICIG), pp. 422-425, 2004, Hong Kong.&lt;/li&gt;
&lt;li&gt;Junzhou Huang, Tieniu Tan, Li Ma, and Yunhong Wang, Phase Correlation Based Iris Image Registration Model, Journal of Computer Science and Technology, Vol.20, No.3, pp.419-425, May 2005.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Based on Multichannel Gabor Filtering”, Proc. of the 5th Asian Conference on Computer Vision (ACCV), Vol. I, pp.279-283, Jan 22-25, 2002, Melbourne, Australia.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Using Circular Symmetric Filters”, Proc. of IAPR International Conference on Pattern Recognitionï¼ˆICPRï¼‰, Vol. II, pp. 414-417, August 11-15, 2002, Quebec, Canada.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, T. N. Tan and Y. H. Wang, “Learning-Based Enhancement Model of Iris”, Proc. of British Machine Vision Conference (BMVC), pp. 153-162, 2003.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, and Y. H. Wang and T. N. Tan, “Iris Model Based on Local Orientation Description”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp. 954-959, 2004, Korea.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, T. N. Tan and J. L. Cui, “A New Iris Segmentation Model”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 3, pp. 554-557, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, J. L. Cui and T. N. Tan, “Noise Removal and Impainting Model for Iris Image”, Proc. of IEEE International Conference on Image Processing (ICIP), pp. 869-872, 2004, Singapore.&lt;/li&gt;
&lt;li&gt;Yuqing He, Yangsheng Wang and Tieniu Tan, “Iris Image Capture System Design For Personal Identification”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 546-552.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, &amp;ldquo;Robust and Fast Assessment of Iris Image quality&amp;rdquo;, Proc. of International Conference of Biometrics, pp. 464-471, 2006.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, &amp;ldquo;Nonlinear Iris Deformation Correction Based on Gaussian Model&amp;rdquo;, International Conference of Biometrics, pp 780-789, 2007.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Yufei Han, Zhenan Sun and Tieniu Tan, Palmprint Image Synthesis: A Preliminary Study, Proc. of IEEE International Conference on Image Processing, 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, Synthesis of Large Realistic Iris Databases Using Patch-based Sampling, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Xianchao Qiu, Zhenan Sun and Tieniu Tan, Counterfeit Iris Detection Based on Texture Analysis, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan and Zhenan Sun, “Iris Localization via Pulling and Pushing”, Proc. of the 18th IEEE International Conference on Pattern Recognition (ICPR&amp;rsquo;06), Vol.4, pp. 366-369, 2006, Hongkong.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun, Xianchao Qiu, Cheng Zhong and Wenbo Dong, Boosting Ordinal Features for Iris Recognition, Proc. of the 26th IEEE International Conference on Computer Vision and Pattern Recognition (CVPR’08) , pp. 1-8, June 23-28, Alaska, USA&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Zhenan Sun, Tieniu Tan and Xianchao Qiu, Enhanced Usability of Iris Recognition via Efficient User Interface and Iris Image Restoration, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, Robust Eyelid, Eyelash and Shadow Localization for Iris Recognition”, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California, Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Zhuoshi Wei, “Efficient Iris Spoof Detection via Boosted Local Binary Patterns”, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1080-1090, 2009.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Coarse Iris Classification by Learned Visual Dictionary&amp;rdquo;, In Proc. of The 2nd International Conference on Biometrics, pp. 770–779, Seoul, Korea, Aug. 2007.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Global Texture Analysis of Iris Images for Ethnic Classification&amp;rdquo;, In Proc. of The 1st International Conference on Biometrics, pp. 411–418, Hong Kong, China. Jan. 2006.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Self-adaptive iris image acquisition system, Proc. SPIE vol. 6944, 1-9, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, How to make iris recognition easier?, Proc. of the 19th International Conference on Pattern Recognition, pp.1-4, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Zhuoshi Wei, &amp;ldquo;Quality-based dynamic threshold for iris matching&amp;rdquo;, In Proceedings of IEEE International Conference on Image Processing, 2009.&lt;/li&gt;
&lt;li&gt;Long Zhang, Zhenan Sun, Tieniu Tan and Shungeng Hu, &amp;ldquo;Robust Biometric Key Extraction Based on Iris Cryptosystem&amp;rdquo;, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1060-1069, 2009.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Contact lens detection based on weighted LBP, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Statistics of Local Surface Curvatures for Mis-Localized Iris Detection, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Texture Removal for Adaptive Level Set based Iris Segmentation&amp;rdquo;, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Hierarchical Fusion of Face and Iris for Personal Identification&amp;rdquo;, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition</title>
      <link>https://nlpr-sir.github.io/publication/wang-icpr-2020/</link>
      <pubDate>Thu, 02 Jul 2020 21:57:47 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-icpr-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN</title>
      <link>https://nlpr-sir.github.io/publication/wang-tci-2020/</link>
      <pubDate>Thu, 02 Jul 2020 21:36:56 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tci-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics</title>
      <link>https://nlpr-sir.github.io/publication/wang-icassp-2020/</link>
      <pubDate>Thu, 09 Apr 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-icassp-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario</title>
      <link>https://nlpr-sir.github.io/publication/kunbo-cvpr-2020/</link>
      <pubDate>Wed, 18 Mar 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/kunbo-cvpr-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/wang-tifs-2020/</link>
      <pubDate>Mon, 16 Mar 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tifs-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/wang-tifs-2021/</link>
      <pubDate>Mon, 16 Mar 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tifs-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris Liveness Detection Based on Light Field Imaging</title>
      <link>https://nlpr-sir.github.io/publication/song-automatica2020/</link>
      <pubDate>Mon, 17 Feb 2020 17:23:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/song-automatica2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>All-in-Focus Iris Camera With a Great Capture Volume</title>
      <link>https://nlpr-sir.github.io/publication/zhang-ijcb2020/</link>
      <pubDate>Mon, 17 Feb 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/zhang-ijcb2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recognition Oriented Iris Image Quality Assessment in the Feature Space</title>
      <link>https://nlpr-sir.github.io/publication/wang-ijcb-2020/</link>
      <pubDate>Mon, 17 Feb 2020 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-ijcb-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application</title>
      <link>https://nlpr-sir.github.io/publication/liu-tip-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/liu-tip-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Graph Representation for Occlusion Handling in Biometrics</title>
      <link>https://nlpr-sir.github.io/publication/ren-2020-dynamic/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-2020-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation</title>
      <link>https://nlpr-sir.github.io/publication/wang-tbiom-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tbiom-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>基于计算光场成像的虹膜活体检测方法</title>
      <link>https://nlpr-sir.github.io/publication/songping-aas-2019/</link>
      <pubDate>Sun, 01 Sep 2019 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/songping-aas-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seg-Edge Bilateral Constraint Network for Iris Segmentation</title>
      <link>https://nlpr-sir.github.io/publication/hu-2019-icb/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/hu-2019-icb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Alignment Free and Distortion Robust Iris Recognition</title>
      <link>https://nlpr-sir.github.io/publication/ren-2019-alignment/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-2019-alignment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-sensor iris recognition using adversarial strategy and sensor-specific information</title>
      <link>https://nlpr-sir.github.io/publication/csin-btas-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/csin-btas-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution</title>
      <link>https://nlpr-sir.github.io/publication/wang-tip-2018/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-tip-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Multi-class Iris Classification for Liveness Detection</title>
      <link>https://nlpr-sir.github.io/publication/yan-2018-liveness/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/yan-2018-liveness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN</title>
      <link>https://nlpr-sir.github.io/publication/wang-eccv-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-eccv-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation</title>
      <link>https://nlpr-sir.github.io/publication/ren-ccbr-2017/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/ren-ccbr-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A simple and robust super resolution method for light field images</title>
      <link>https://nlpr-sir.github.io/publication/wang-icip-2016/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/wang-icip-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris Recognition</title>
      <link>https://nlpr-sir.github.io/benchmark/iris-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/benchmark/iris-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Light Field Photography</title>
      <link>https://nlpr-sir.github.io/benchmark/light-field-photography/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/benchmark/light-field-photography/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Light Field Photography</title>
      <link>https://nlpr-sir.github.io/project/light-field-photography/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/project/light-field-photography/</guid>
      <description>&lt;h2 id=&#34;light-field&#34;&gt;Light field&lt;/h2&gt;
&lt;p&gt;The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled &amp;ldquo;Thoughts on Ray Vibrations&amp;rdquo;) that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase light field was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).&lt;/p&gt;
&lt;h2 id=&#34;the-4d-light-field&#34;&gt;The 4D light field&lt;/h2&gt;
&lt;p&gt;In a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.&lt;/p&gt;
&lt;p&gt;However, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the photic field (1981), while researchers in computer graphics call it the 4D light field (Levoy 1996) or Lumigraph (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.&lt;/p&gt;
&lt;p&gt;The set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the st plane (and any objects that may lie astride or beyond it), each taken from an observer position on the uv plane. A light field parameterized this way is sometimes called a light slab.&lt;/p&gt;
&lt;h2 id=&#34;ways-to-create-light-fields&#34;&gt;Ways to create light fields&lt;/h2&gt;
&lt;p&gt;Light fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.&lt;/p&gt;
&lt;p&gt;In computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).&lt;/p&gt;
&lt;p&gt;Devices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in The Matrix), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).&lt;/p&gt;
&lt;p&gt;How many images should be in a light field? The largest known light field (of Michelangelo&amp;rsquo;s statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the st plane, then you need images taken at finely spaced positions on the uv plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.&lt;/p&gt;
&lt;p&gt;The number and arrangement of images in a light field, and the resolution of each image, are together called the &amp;ldquo;sampling&amp;rdquo; of the 4D light field. Analyses of light field sampling have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Periocular Recognition</title>
      <link>https://nlpr-sir.github.io/benchmark/periocular-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/benchmark/periocular-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Periocular Recognition</title>
      <link>https://nlpr-sir.github.io/project/periocular-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/project/periocular-recognition/</guid>
      <description>&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 7 &gt;&lt;strong&gt;To be updated.....&lt;/strong&gt;&lt;/font&gt;</description>
    </item>
    
    <item>
      <title>Sclera Recognition</title>
      <link>https://nlpr-sir.github.io/benchmark/sclera-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/benchmark/sclera-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sclera Recognition</title>
      <link>https://nlpr-sir.github.io/project/sclera-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/project/sclera-recognition/</guid>
      <description>&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 7 &gt;&lt;strong&gt;To be updated.....&lt;/strong&gt;&lt;/font&gt;</description>
    </item>
    
    <item>
      <title>The Other Internal Project</title>
      <link>https://nlpr-sir.github.io/benchmark/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/benchmark/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4D light-field sensing system for people counting</title>
      <link>https://nlpr-sir.github.io/publication/hou-spie-2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/publication/hou-spie-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>基于物联网的温室智能监控系统设计</title>
      <link>https://nlpr-sir.github.io/publication/qinlinlin-ny-2015/</link>
      <pubDate>Sun, 01 Mar 2015 16:12:05 +0800</pubDate>
      <guid>https://nlpr-sir.github.io/publication/qinlinlin-ny-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://nlpr-sir.github.io/slides/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nlpr-sir.github.io/slides/example/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
